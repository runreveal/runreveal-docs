{"/ai-chat/custom-prompts":{"title":"Custom Prompts","data":{"":"RunReveal provides a custom prompts API that follows the Model Context Protocol (MCP) specification for prompts. This allows you to create reusable prompt templates that can be dynamically customized with arguments.","overview#Overview":"Custom prompts enable you to define structured message templates that can be retrieved and customized with specific arguments. This is particularly useful for:\nStandardizing common analysis requests\nCreating reusable detection templates\nProviding consistent prompt structures across your organization\nEnabling dynamic content injection based on context","argument-syntax#Argument Syntax":"When using arguments in prompt text, they must be wrapped in double curly braces:\n{{argument_name}}","examples#Examples":"Basic argument usage:\nPlease analyze this log data: {{log_data}}\nMultiple arguments:\nAnalyze the following data from {{source}} for the period {{timeframe}}:\n{{log_data}}\nFocus on {{focus_area}} patterns.","data-types#Data Types":"","prompt#Prompt":"A prompt definition includes:\nname: Unique identifier for the prompt\ntitle: Optional human-readable name for display purposes\ndescription: Optional human-readable description\narguments: Optional list of arguments for customization","example-prompts#Example Prompts":"","security-analysis-prompt#Security Analysis Prompt":"{\n  \"name\": \"security_analysis\",\n  \"title\": \"Security Analysis\",\n  \"description\": \"Analyze logs for security threats and anomalies\",\n  \"arguments\": [\n    {\n      \"name\": \"log_data\",\n      \"description\": \"The log data to analyze\",\n      \"required\": true\n    },\n    {\n      \"name\": \"timeframe\",\n      \"description\": \"Analysis timeframe\",\n      \"required\": false\n    },\n    {\n      \"name\": \"focus_areas\",\n      \"description\": \"Specific areas to focus on\",\n      \"required\": false\n    }\n  ],\n  \"prompt_text\": \"You are a security analyst. \n    Please analyze the following log data for security threats and anomalies:\n    {{log_data}}\n    Timeframe: {{timeframe}}\n    Focus areas: {{focus_areas}}\n    Provide a comprehensive analysis including:\n    - Identified threats and their severity levels\n    - Anomalous patterns or behaviors\n    - Potential attack vectors\n    - Recommended mitigation strategies\n    - Risk assessment and impact analysis\"\n}","implementation-considerations#Implementation Considerations":"Argument Validation: Servers validate prompt arguments before processing\nAuthentication: All API requests require a valid API token\n## Security\nImplementations carefully validate all prompt inputs and outputs to prevent:\n- Injection attacks\n- Unauthorized access to resources\n- Malicious content execution\nAlways validate and sanitize arguments before using them in prompt templates.\n## Usage Examples\n### Using Arguments in Prompt Text\n```text\nPlease analyze the following logs for the pattern \"{{pattern}}\":\n{{logs}}\nProvide a detailed analysis including:\n- Pattern matches found\n- Frequency of occurrences\n- Potential security implications\n- Recommended actions\nThis documentation follows the Model Context Protocol specification for prompts and provides a comprehensive guide for using RunReveal's custom prompts API.","using-prompts-in-remote-mcp-connection#Using Prompts in Remote MCP Connection":"","claude-ai#Claude AI":"Follow the steps to set up your remote MCP server connection. Make sure that the Chat read and Chat write permissions are enabled.Once you have your connection with RunReveal established, start a new chat and open the dropdown menu from the plus icon. If you have any custom prompts configured in your RunReveal workspace, you should see an option \"Add from INTEGRATION_NAME\" (RunReveal in the example below).Selecting \"Add from INTEGRATION_NAME\" will either immediately add the prompt as a .txt file to the chat, or prompt you to add values for your configured arguments if the custom prompt has an arguments field."}},"/ai-chat":{"title":"AI Chat (Native & MCP)","data":{"":"Welcome to the RunReveal AI Chat section. This area covers both our integrated Native AI Chat system and the Model Context Protocol (MCP) integration, providing you with powerful AI-powered investigation and analysis capabilities.","native-ai-chat#Native AI Chat":"RunReveal's Native AI Chat is a purpose-built investigation agent that lets you analyze your security data conversationally, directly inside the RunReveal console. It executes complex queries, analyzes patterns, and guides you through investigations—all without your data ever leaving the platform.","key-features#Key Features":"Secure: No data leaves your workspace\nAuditable: Every action is logged and explained\nPersistent: Maintains investigation context across sessions\nDirect: Query your actual security data in real-time\nMulti-Provider Support: Use Anthropic, OpenAI, AWS Bedrock, or Google AI models","get-started-with-native-ai-chat#Get Started with Native AI Chat":"Learn how to set up and use Native AI Chat for security investigations, including:\nSetup and configuration with multiple LLM providers\nQuery optimization and best practices\nInvestigation workflows and use cases\nSecurity and audit considerations","model-context-protocol-mcp#Model Context Protocol (MCP)":"The Model Context Protocol (MCP) allows you to connect AI assistants like Claude and Cursor to external data sources and tools. This enables you to access your RunReveal data and tools from any AI assistant that supports MCP.","key-features-1#Key Features":"Remote MCP Support: Connect to RunReveal's MCP server over HTTP/HTTPS\nMultiple AI Assistants: Works with Claude, Cursor, and other MCP-compatible tools\nSecure Authentication: OAuth-based authentication with your RunReveal workspace\nComprehensive Tool Access: Query data, manage detections, and explore schemas","set-up-mcp-integration#Set Up MCP Integration":"Learn how to configure MCP with your preferred AI assistants:\nSetting up with Claude Desktop and Claude.ai\nConfiguring Cursor IDE integration\nAvailable tools and capabilities\nTroubleshooting and security considerations","custom-prompts#Custom Prompts":"RunReveal's Custom Prompts feature allows you to create reusable prompt templates that can be dynamically customized with arguments. This is particularly useful for standardizing common analysis requests and creating consistent prompt structures across your organization.","key-features-2#Key Features":"Reusable Templates: Create standardized prompts for common tasks\nDynamic Arguments: Customize prompts with specific context and data\nMCP Integration: Works seamlessly with MCP-compatible AI assistants\nSecurity Focused: Built-in validation and security measures","learn-about-custom-prompts#Learn About Custom Prompts":"Discover how to create and use custom prompts:\nArgument syntax and usage\nExample prompt templates\nIntegration with MCP connections\nSecurity best practices","choosing-between-native-ai-chat-and-mcp#Choosing Between Native AI Chat and MCP":"","native-ai-chat-1#Native AI Chat":"Best for:\nIntegrated security investigations within RunReveal\nTeams that want a unified platform experience\nOrganizations with strict data governance requirements\nUsers who prefer a dedicated security investigation interface","model-context-protocol-mcp-1#Model Context Protocol (MCP)":"Best for:\nUsers who prefer their existing AI assistant workflows\nTeams using Claude, Cursor, or other MCP-compatible tools\nOrganizations wanting to leverage AI assistants across multiple platforms\nUsers who want to combine RunReveal data with other data sources","getting-started#Getting Started":"Choose the option that best fits your workflow:\nNative AI Chat - For integrated, platform-native AI investigations\nModel Context Protocol - For external AI assistant integration\nCustom Prompts - For creating reusable prompt templates\nAll options provide secure, auditable access to your RunReveal data and powerful AI-driven analysis capabilities."}},"/ai-chat/model-context-protocol":{"title":"Model Context Protocol","data":{"":"The Model Context Protocol (MCP) allows you to connect AI assistants like Claude and Cursor to external data sources and tools. This guide shows you how to set up RunReveal's remote MCP server with both Claude and Cursor.","what-is-remote-mcp#What is Remote MCP?":"Remote MCP allows you to connect to MCP servers running on remote hosts over HTTP/HTTPS, rather than just local processes. This enables you to:\nAccess your RunReveal data and tools from any AI assistant that supports MCP\nShare MCP servers across multiple team members\nRun MCP servers in production environments\nCentralize data access and permissions","setting-up-with-claude#Setting Up with Claude":"","prerequisites#Prerequisites":"A RunReveal account with API access\nClaude Desktop or Claude.ai account","step-1-add-the-integration#Step 1: Add the Integration":"In Claude, go to Add integration (BETA)\nEnter the following details:\nIntegration Name: RunReveal\nServer URL: https://api.runreveal.com/mcp","step-2-trust-the-integration#Step 2: Trust the Integration":"Claude will show a warning that this integration has not been verified by Anthropic. Click Add to proceed.","step-3-authorize-with-runreveal#Step 3: Authorize with RunReveal":"You'll be redirected to RunReveal's authorization page\nReview the OAuth client information:\nClient Name: claudeai\nClient ID: (will be displayed)\nSelect your workspace from the dropdown\nClick Continue to authorize the connection","alternative-using-claude-code-cli#Alternative: Using Claude Code CLI":"If you're using Claude Code (the CLI tool), you can add the RunReveal MCP server directly from the command line:\nclaude mcp add -t http runreveal 'https://api.runreveal.com/mcp'\nThis command will:\nAdd the RunReveal MCP server to your Claude Code configuration\nUse the identifier \"runreveal\" for the server\nAfter running this command, the MCP server will be available in your Claude Code sessions, and you'll go through the same OAuth authorization flow when first accessing RunReveal tools.","step-4-verify-the-connection#Step 4: Verify the Connection":"Once authorized, you should see RunReveal listed in your Claude integrations with available tools:The RunReveal MCP integration provides access to several tools:\ndetections_list - List all detection rules\ndetections_get - Get details for a specific detection\nrun_query - Execute SQL queries against your data\nlist_tables - View available data tables\nget_table_schema - Get schema information for tables\ndetections_create - Create new detection rules\nAll currently available tools require Queries & Detection Read permissions, and\nthe detections_create tool requires Queries & Detection Write permissions.","setting-up-with-cursor#Setting Up with Cursor":"","prerequisites-1#Prerequisites":"A RunReveal account with API access\nCursor IDE installed","configuration#Configuration":"Click this link: Or, manually add this to your configuration:\n{\n  \"mcpServers\": {\n    \"RunReveal\": {\n      \"url\": \"https://api.runreveal.com/mcp\"\n    }\n  }\n}","authentication-flow#Authentication Flow":"Similar to Claude, Cursor will redirect you to RunReveal's OAuth authorization page where you can:\nSelect your workspace\nGrant the necessary permissions\nComplete the OAuth flow","available-tools-and-capabilities#Available Tools and Capabilities":"Once connected, both Claude and Cursor can help you with:","data-analysis#Data Analysis":"Query your log data with natural language\nExplore table schemas and relationships\nGenerate SQL queries for complex analysis","detection-management#Detection Management":"List and review existing detection rules\nCreate new detections based on your requirements\nGet detailed information about specific detections","security-operations#Security Operations":"Investigate security events and incidents\nAnalyze patterns in your data\nGenerate reports and summaries","example-usage#Example Usage":"Once set up, you can ask your AI assistant questions like:\n\"Show me all failed login attempts from the last 24 hours\"\n\"What detection rules do we have for privilege escalation?\"\n\"Create a new detection for suspicious file downloads\"\n\"What tables contain network traffic data?\"","troubleshooting#Troubleshooting":"","connection-issues#Connection Issues":"Verify your RunReveal API credentials are valid\nCheck that you have the necessary permissions in your workspace\nEnsure the MCP server URL is correct: https://api.runreveal.com/mcp","authentication-problems#Authentication Problems":"Clear your browser cookies and retry the OAuth flow\nMake sure you're selecting the correct workspace during authorization\nContact RunReveal support if you continue having issues","tool-access-issues#Tool Access Issues":"Verify your RunReveal user has appropriate permissions for the tools you're trying to use\nSome tools may require specific roles or permissions within your workspace","security-considerations#Security Considerations":"The MCP connection uses OAuth for secure authentication\nTools respect your existing RunReveal permissions and access controls\nAll data transmission is encrypted over HTTPS\nYou can revoke access at any time through your RunReveal workspace settings","next-steps#Next Steps":"With MCP set up, you can now leverage AI assistants to:\nStreamline your security operations workflows\nGet natural language insights from your data\nAutomate common detection and analysis tasks\nCollaborate more effectively with your security team"}},"/ai-chat/native-ai-chat":{"title":"Native AI Chat","data":{"":"RunReveal's Native AI Chat is a purpose-built investigation agent that lets you analyze your security data conversationally, directly inside the RunReveal console. It executes complex queries, analyzes patterns, and guides you through investigations—all without your data ever leaving the platform.","what-is-native-ai-chat#What is Native AI Chat?":"Native AI Chat is fully integrated into the RunReveal UI, giving you a secure, auditable, and persistent way to investigate logs and detections using natural language. Unlike external chat clients, Native AI Chat runs entirely within your workspace, leveraging your own API keys for model access.","why-native-ai-chat#Why Native AI Chat?":"Direct data access: Query logs, examine table schemas, and analyze security data in real-time using the same APIs that you already use.\nTransparent reasoning: Every action is explained and auditable. See exactly why a query was run or a tool was used.\nPersistent context: The chat remembers your investigation history, enabling complex, multi-day investigations.\nSecure by design: Your data never leaves the RunReveal platform. All model calls are made server-side, using your own API keys.","what-can-you-do-with-native-ai-chat#What Can You Do with Native AI Chat?":"\"Show me all failed login attempts from the last 24 hours\"\n\"Find privilege escalation events in the past week\"\n\"What IP addresses had the most failed authentication attempts?\"\n\"List all detection rules for brute force attacks\"\n\"Show me which detections fired most frequently this week\"\n\"What detection rules do we have for privilege escalation?\"\n\"What tables contain network traffic data?\"\n\"Show me the schema for the authentication logs\"\n\"Find all unique user agents in web traffic logs\"","common-use-cases#Common Use Cases":"\"I need to investigate a suspicious login from IP 192.168.1.100\"\n→ Agent analyzes login patterns, cross-references with threat intel, suggests next steps\n\"Show me our top security events this week\"\n→ Agent queries multiple log sources, creates summary with actionable insights\n\"Analyze authentication events from the past 30 days by geographic location, show me login attempts by country and city\"\n→ Agent reviews existing data, identifies trends, provides insights","supported-model-providers#Supported Model Providers":"Native AI Chat supports multiple LLM providers. Workspace admins can add API keys for any or all of the following:\nAnthropic (Claude)\nOpenAI (ChatGPT)\nGoogle AI (Gemini)\nAmazon Bedrock (AWS) - Setup Guide\nEach model runs securely and data never leaves the RunReveal platform.\nAdmins can now configure multiple providers and set a default provider for the workspace. When starting a new chat, users can select from any configured provider. Once a provider is selected for a chat session, it is fixed for that session, but you can start a new chat with a different provider at any time.","setup-guide-native-ai-chat#Setup Guide: Native AI Chat":"","prerequisites#Prerequisites":"A RunReveal account with admin or API access\nAPI key(s) for your preferred LLM provider(s): Anthropic (Claude), OpenAI (ChatGPT), Google AI (Gemini), or an IAM role if using Amazon Bedrock (AWS)\nAccess to your RunReveal workspace settings","step-1-open-ai-model-providers-settings#Step 1: Open AI Model Providers Settings":"In the RunReveal UI, go to Workspace Settings → AI Model Providers","step-2-add-your-llm-api-keys#Step 2: Add Your LLM API Keys":"Add API keys for any providers you want to enable (Anthropic, OpenAI, Google AI, or Amazon Bedrock).\nYou can add multiple providers; you do not need to remove unused keys.","step-3-set-a-default-provider-admin-only#Step 3: Set a Default Provider (Admin Only)":"In the provider settings, select which provider should be the default for your workspace.\nThis provider will be pre-selected for new chat sessions, but users can change it.","step-4-start-chatting-and-select-provider#Step 4: Start Chatting and Select Provider":"When starting a new chat, you'll see a dropdown to select from any configured provider.\nOnce you select a provider/model, it will be used for the duration of that chat session.\nTo use a different provider, simply start a new chat and select a different provider.","amazon-bedrock-setup#Amazon Bedrock Setup":"Amazon Bedrock requires AWS IAM role configuration instead of API keys. Follow these steps to set up Bedrock access:\nImportant Notes:\nConfiguration: Uses role ARN instead of API keys with external ID for security\nModel Selection: Currently limited to Claude 3.5 Sonnet (requires Bedrock console access)\nRegional: Single-region inference only - specify region during configuration","step-1-configure-bedrock-in-runreveal#Step 1: Configure Bedrock in RunReveal":"In the RunReveal UI, go to Workspace Settings → AI Model Providers\nClick Configure next to Amazon Bedrock\nA modal will appear where you can input the arn of your IAM role and an External ID that you will add to your IAM role. If you've already configured a role for RunReveal to assume for reading S3, you can re-use that same role and External ID here after you've attached the bedrock access policy below.","step-2-create-iam-policy-for-bedrock#Step 2: Create IAM Policy for Bedrock":"Attach the following policy to your IAM role:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"bedrock:InvokeModel\",\n                \"bedrock:InvokeModelWithResponseStream\",\n                \"bedrock:ListFoundationModels\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nOptional: For better security, you can restrict the policy to specific model ARNs instead of using \"Resource\": \"*\"","step-3-create-iam-role-in-aws#Step 3: Create IAM Role in AWS":"If you've configured a more general for role for RunReveal to assume, you can attach the policy to that role and skip this step.\nLog into AWS Console and navigate to IAM\nCreate a new role with the following configuration:\nTrusted entity: AWS account\nAccount ID: Use the account ID shown in the RunReveal modal\nExternal ID: Use the External ID shown in the RunReveal modal (this is required for security)","trust-relationship-configuration#Trust Relationship Configuration":"The trust relationship is a critical security component that controls which AWS account can assume your role. We recommend using an External ID to prevent unauthorized access.Example Trust Policy:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::253602268883:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"your-unique-external-id-from-runreveal\"\n                }\n            }\n        }\n    ]\n}\nKey Components:\nPrincipal: The AWS account that can assume this role (RunReveal's account)\nAction: sts:AssumeRole allows the role to be assumed\nCondition: The External ID must match exactly what's shown in the RunReveal modal\nLearn More: For detailed information about trust relationships and External IDs, see the AWS IAM documentation on trust policies and External ID best practices.","step-4-attach-policy-to-role#Step 4: Attach policy to role":"You'll need to attach the bedrock policy created in step 2 to the role.","step-5-get-role-arn#Step 5: Get Role ARN":"After creating the role, copy the Role ARN (format: arn:aws:iam::ACCOUNT-ID:role/ROLE-NAME)\nPaste the Role ARN into the RunReveal Bedrock configuration modal\nClick Save to complete the setup","step-6-test-bedrock-access#Step 6: Test Bedrock Access":"Start a new chat in RunReveal\nSelect Amazon Bedrock from the provider dropdown\nAsk a test question to verify the connection works\nSecurity Note: The External ID in the RunReveal modal is unique to your workspace and helps prevent unauthorized access. Always use the exact External ID shown in the modal when creating your IAM role. See the Trust Relationship Configuration section above for detailed setup instructions.","usage#Usage":"Once set up, you can ask:\n\"Show me all failed login attempts from the last 24 hours\"\n\"What detection rules do we have for privilege escalation?\"\n\"Create a new detection for suspicious file downloads\"\n\"What tables contain network traffic data?\"","native-ai-chat-responding-to-investigation-prompt#Native AI Chat responding to investigation prompt:":"","how-the-agent-reorients-after-a-failed-query#How the agent reorients after a failed query:":"","how-it-works-the-investigation-process#How It Works: The Investigation Process":"Native AI Chat uses a structured approach to investigations:Example: \"Investigate failed logins from suspicious IPs\"\nObserve: \"Let me check your authentication logs and IP reputation data\"\nOrient: \"I found 50 failed logins from 3 high-risk IPs in the last hour\"\nDecide: \"I should cross-reference these with your user directory and check for successful logins\"\nAct: Queries user tables, generates timeline, suggests blocking actions","getting-the-most-from-native-ai-chat#Getting the Most from Native AI Chat":"","effective-prompts#Effective Prompts":"Be specific about timeframes: \"last 24 hours\" vs \"recently\"\nInclude context: \"high-priority alerts\" vs \"all alerts\"\nAsk follow-up questions: \"What caused this spike in DNS queries?\"","investigation-tips#Investigation Tips":"Start broad, then narrow down: \"Show me authentication events\" → \"Focus on failed logins\"\nAsk for explanations: \"Why did this detection fire?\"\nRequest recommendations: \"What should I investigate next?\""}},"/bring-your-own-cloud":{"title":"Bring Your Own Cloud (BYOC) - Self-Hosted RunReveal Deployment","data":{"":"RunReveal supports a Bring Your Own Cloud (BYOC) model. This means that you can\nuse your own cloud provider to store your log data, run our product within your\nVPC, and configure our product so it works to your liking.RunReveal's Bring Your Own Cloud (BYOC) model allows you to deploy and run RunReveal infrastructure within your own cloud environment. This gives you enhanced control, security, and compliance while still leveraging the powerful features of RunReveal.","benefits-of-byoc#Benefits of BYOC":"Enhanced Security and Compliance: Keep your data within your security perimeter and meet strict regulatory requirements\nReduced Latency: Process and analyze data closer to its source for faster query response times\nCost Control: Optimize costs by leveraging your existing cloud investments and agreements\nData Sovereignty: Maintain complete ownership and control over your security data\nCustom Configuration: Tailor the deployment to your specific infrastructure needs","byoc-components#BYOC Components":"RunReveal's BYOC solution consists of several key components:","deployment#Deployment":"Learn how to deploy RunReveal infrastructure within your cloud environment, including:\nInfrastructure requirements\nNetwork configuration\nResource sizing recommendations\nDeployment options and templates","authentication#Authentication":"Configure secure authentication mechanisms for your BYOC deployment:\nIdentity provider integration\nRole-based access control configuration\nAPI key management\nAuthentication flows","runreveal-query-rrq#RunReveal Query (RRQ)":"Understand how RunReveal Query (RRQ) works in a BYOC environment:\nQuery engine architecture\nPerformance optimization\nData access patterns\nIntegration with your data sources","runreveal-scheduler-rrsch#RunReveal Scheduler (RRSCH)":"Configure the RunReveal Scheduler (RRSCH) for detection scheduling:\nCreating and managing scheduled detections\nAlert configuration\nResource management\nHigh availability options\nSchedule Types","getting-started-with-byoc#Getting Started with BYOC":"To get started with BYOC, you'll need to reach out to us and we can help you\nconfigure everything but there are a few important guides you'll need.\nDeployment\nAuthentication\nTo get started with RunReveal BYOC, please contact our team for personalized guidance and setup assistance. We'll work with you to design a deployment that meets your specific requirements and security needs.Contact us at contact@runreveal.com or through the chat on our website to schedule a consultation."}},"/bring-your-own-cloud/data-model":{"title":"Data Model","data":{"":"RunReveal's data model is simple and allows for bring your own database\ncustomers to extend the tables that RunReveal provides to create things\nlike:\nCustom views for internal or custom log sources.\nCustom schemas to perform enrichment or normalization across\nAll logs that RunReveal receives we normalize into a consistent schema. This\nschema is built to be flexible and extensible.The complete log schema is available here below. This is accessible in the\nproduct by running describe logs as a query.\n$ describe logs\nworkspaceID          String\nsourceID             String\nsourceType           LowCardinality(String)\nsourceTTL            UInt32\nreceivedAt           DateTime\nid                   String\neventTime            DateTime\neventName            String\neventID              String\nsrcIP                String\nsrcASCountryCode     String\nsrcASNumber          UInt32\nsrcASOrganization    String\nsrcCity              String\nsrcConnectionType    String\nsrcISP               String\nsrcLatitude          Float64\nsrcLongitude         Float64\nsrcUserType          String\ndstIP                String\ndstASCountryCode     String\ndstASNumber          UInt32\ndstASOrganization    String\ndstCity              String\ndstConnectionType    String\ndstISP               String\ndstLatitude          Float64\ndstLongitude         Float64\ndstUserType          String\nactor                Map(String, String)\ntags                 Map(String, String)\nresources            Array(String)\nserviceName          String\nenrichments          Array(Tuple(data Map(String, String), name String, provider String, type String, value String))\nreadOnly             Bool\nrawLog               String\nThere are a lot of important fields and nuances to this schema that is worth\ncovering. Some of the important bits areThe rawLog column contains the raw JSON (or otherwise) as a string. This is\nan incredibly useful column because it allows us to build views, materialized\nviews, or otherwise on top of the logs table for custom use cases.The other columns are populated by each of our sources as soon as the logs\nare ingested. When a log arrives at a source the following steps occur.\nThe log is parsed into a structured object.\nThe relevant fields are pulled out of the log and filled into a normalized log\nThe normalized log contains a rawLog field, which is set with the original log line that was parsed.\nThe normalized log performs automatic enrichments (srcASCountryCode, dstISP, etc).\nBy default all of our log sources will automatically do this (except for\ngeneric log sources). Additionally, however, our transforms feature can be used\nto move data around further within our normalized schema, or the rawLog fields.Within your database it's possible to build custom views that extend logs within\nRunReveal. Here's the most simple example of creating a new view called co_example_logs\nwith a new column called extra_field\nCREATE OR REPLACE VIEW runreveal.co_example_logs AS\n    SELECT\n        *,\n        JSONExtractString(rawLog, 'extra_field') as `extra_field`\n    FROM runreveal.logs\n    WHERE sourceID = '2z3ykgYgk5fats6m2S9iKrVqbFk'\n;\nOne thing to note, that's implicit in select * from logs is that\nit's critical to include the receivedAt column in tables and views.We highly recommend that all your tables include this column because\nit's one of our most important indexes on the table. This will ensure\nthat your queries remain quick. It's also the way our UI partitions\nand loads data by time. To make sure that a table or view is compatible\nwith our UI it's required to have the receivedAt column.There are other important tables to make note of in RunReveal.\ndetections - Holds all findings from sigma and query based detections\nscheduled_query_runs - Information on scheduled query. Shows runs and results.\nrunreveal_source_volumes - Counts of how much data was received.\nEach of these tables can also be extended, but many of them will have different\nprimary keys and indexes. If you're curious about the details you can describe\neach one of these and examine how they work!"}},"/bring-your-own-cloud/authentication":{"title":"Authentication","data":{"":"Standing up a BYOC environment requires a few steps. Since our product will\nbe running within your VPC, dependencies on our systems for authentication\nand authorization don't really make sense. Instead our product can be configured\nto use the same authentication and authorization methods you use in a cloud\ndeployment but within your VPC.The login process is governed by environment variables. The following need to be set\nin your environment depending on the authentication method you'd like to use.\nGOOGLE_CLIENT_ID=google\nGOOGLE_CLIENT_SECRET=google\nMICROSOFT_CLIENT_ID=microsoft\nMICROSOFT_CLIENT_SECRET=microsoft\nGITHUB_CLIENT_ID=github\nGITHUB_CLIENT_SECRET=github\nTo configure Google authentication, you'll need to create a project in the Google\nCloud Console and enable the Google OAuth API. You'll also need to create a\nclient ID and secret for your project.\nCreate a project in the Google Cloud Console.\nEnable the Google OAuth API if necessary.\nCreate a client ID and secret for your project. The authorized redirect URI\nshould be the URL of your RunReveal API, the authorized origin should be the\nURL of your RunReveal API.\nOnce you've created your project, you'll need to set the GOOGLE_CLIENT_ID and\nGOOGLE_CLIENT_SECRET environment variables when running the api process."}},"/bring-your-own-cloud/rrq":{"title":"RRQ","data":{"":"RRQ is a data pipeline tool that ingests data from various sources and processes it through a unified pipeline. This documentation covers how to configure and run RRQ.","table-of-contents#Table of Contents":"Basic Configuration\nCommon Configuration Options\nSource Types\nRunning RRQ","basic-configuration#Basic Configuration":"RRQ uses a JSON configuration file. Here's a basic example:\n{\n  \"chdsn\": \"clickhouse://user:password@localhost:9000/dbname\",\n  \"pprof\": \"localhost:6060\",\n  \"backfillProcessor\": false,\n  \"parallelism\": 4,\n  \"watchdogTimeout\": 300,\n  \"common\": {\n    \"baseurl\": \"https://your-domain.com\",\n    \"pgdsn\": \"postgres://user:password@localhost/dbname?sslmode=disable\"\n  },\n  \"sources\": [\n    {\n      \"type\": \"cloudtrail\",\n      \"sqsQueue\": \"your_cloudtrail_queue\"\n    }\n  ]\n}","common-configuration-options#Common Configuration Options":"Option\tType\tDescription\tRequired\tchdsn\tstring\tClickHouse connection string\tYes\tpprof\tstring\tAddress for pprof debugging server\tNo\tbackfillProcessor\tboolean\tEnable backfill processing\tNo\tparallelism\tinteger\tNumber of parallel workers\tNo\twatchdogTimeout\tinteger\tTimeout in seconds for watchdog\tNo\tcommon\tobject\tCommon configuration shared across sources\tYes","common-configuration-object#Common Configuration Object":"Option\tType\tDescription\tRequired\tbaseurl\tstring\tBase URL for the application\tYes\tpgdsn\tstring\tPostgreSQL connection string\tYes","running-rrq#Running RRQ":"RRQ can be run using the following command:\nrrq run --config config.json","command-line-arguments#Command Line Arguments":"Flag\tDescription\tDefault\t--config\tPath to configuration file\tconfig.json","environment-variables#Environment Variables":"Configuration values can reference environment variables by prefixing the value with $. For example:\n{\n  \"common\": {\n    \"pgdsn\": \"$POSTGRES_DSN\"\n  }\n}","next-steps#Next Steps":"See Source Types for detailed configuration of each source type\nCheck our troubleshooting guide for common issues\nRefer to the monitoring documentation for operational best practices"}},"/bring-your-own-cloud/deployment":{"title":"Deployment","data":{"":"We can host everything you'll need to use RunReveal in your account including the data pipeline.To get started with RunReveal BYOC, please contact our team for personalized guidance and setup assistance. We'll work with you to design a deployment that meets your specific requirements and security needs.Contact us at contact@runreveal.com or through the chat on our website to schedule a consultation"}},"/bring-your-own-cloud/rrsch":{"title":"RRSCH","data":{"":"RRSCH is a scheduling system that manages various types of scheduled tasks in the RunReveal platform, including queries, reports, threat feeds, notifications, and enrichments.","table-of-contents#Table of Contents":"Basic Configuration\nCommon Configuration Options\nScheduler Types\nRunning RRSCH","basic-configuration#Basic Configuration":"RRSCH uses a JSON configuration file. Here's a basic example:\n{\n  \"chdsn\": \"clickhouse://user:password@localhost:9000/dbname\",\n  \"common\": {\n    \"baseurl\": \"http://localhost:8000\",\n    \"pgdsn\": \"postgres://user:password@localhost/dbname?sslmode=verify-full\"\n  },\n  \"schedule\": {\n    \"schedulers\": [\n      {\n        \"type\": \"query\",\n        \"timer\": 500,\n        \"fetchCount\": 10,\n        \"visibilityTimeout\": 10,\n        \"notifRetryQueue\": \"\"\n      }\n    ]\n  }\n}","common-configuration-options#Common Configuration Options":"Option\tType\tDescription\tRequired\tchdsn\tstring\tClickHouse connection string\tYes\tcommon\tobject\tCommon configuration shared across schedulers\tYes","common-configuration-object#Common Configuration Object":"Option\tType\tDescription\tRequired\tbaseurl\tstring\tBase URL for the application\tYes\tpgdsn\tstring\tPostgreSQL connection string\tYes","running-rrsch#Running RRSCH":"RRSCH can be run using the following command:\nrrsch queries --config config.json","command-line-arguments#Command Line Arguments":"Flag\tDescription\tDefault\t--config\tPath to configuration file\tconfig.json","environment-variables#Environment Variables":"Configuration values can reference environment variables by prefixing the value with $. For example:\n{\n  \"common\": {\n    \"pgdsn\": \"$POSTGRES_DSN\"\n  }\n}"}},"/bring-your-own-cloud/rrsch/schedule-types":{"title":"RRSCH Scheduler Types","data":{"":"This document details all available scheduler types and their specific configuration options.","common-scheduler-configuration#Common Scheduler Configuration":"Most schedulers share these common configuration options:\nOption\tType\tDescription\tDefault\ttimer\tinteger\tTimer interval in milliseconds\t-\tfetchCount\tinteger\tNumber of items to fetch per batch\t-\tvisibilityTimeout\tinteger\tTimeout in minutes before task is considered failed\t-\tparallelism\tinteger\tNumber of parallel workers (minimum 3)\t3","available-schedulers#Available Schedulers":"","query-scheduler#Query Scheduler":"Manages scheduled queries and their execution.\n{\n  \"type\": \"query\",\n  \"timer\": 500,\n  \"fetchCount\": 10,\n  \"visibilityTimeout\": 10,\n  \"notifRetryQueue\": \"\"\n}\nAdditional Field\tType\tDescription\tRequired\tnotifRetryQueue\tstring\tQueue name for notification retries\tNo","report-scheduler#Report Scheduler":"Handles scheduled report generation.\n{\n  \"type\": \"report\",\n  \"timer\": 500,\n  \"fetchCount\": 10,\n  \"visibilityTimeout\": 10\n}","threat-feed-scheduler#Threat Feed Scheduler":"Manages threat feed updates and processing.\n{\n  \"type\": \"threatfeed\",\n  \"timer\": 30000,\n  \"fetchCount\": 2,\n  \"visibilityTimeout\": 3\n}","notification-retry-scheduler#Notification Retry Scheduler":"Handles retry logic for failed notifications.\n{\n  \"type\": \"notification-retry\",\n  \"notifRetryQueue\": \"\"\n}\nField\tType\tDescription\tRequired\tnotifRetryQueue\tstring\tQueue name for notification retries\tYes","enrichment-scheduler#Enrichment Scheduler":"Manages data enrichment tasks.\n{\n  \"type\": \"enrichment\",\n  \"timer\": 60000,\n  \"fetchCount\": 1,\n  \"visibilityTimeout\": 10\n}","complete-configuration-example#Complete Configuration Example":"Here's a complete configuration example with all scheduler types:\n{\n  \"chdsn\": \"clickhouse://user:password@localhost:9000/dbname\",\n  \"common\": {\n    \"baseurl\": \"http://localhost:8000\",\n    \"pgdsn\": \"postgres://user:password@localhost/dbname?sslmode=verify-full\"\n  },\n  \"schedule\": {\n    \"schedulers\": [\n      {\n        \"type\": \"query\",\n        \"timer\": 500,\n        \"fetchCount\": 10,\n        \"visibilityTimeout\": 10,\n        \"notifRetryQueue\": \"\"\n      },\n      {\n        \"type\": \"report\",\n        \"timer\": 500,\n        \"fetchCount\": 10,\n        \"visibilityTimeout\": 10\n      },\n      {\n        \"type\": \"threatfeed\",\n        \"timer\": 30000,\n        \"fetchCount\": 2,\n        \"visibilityTimeout\": 3\n      },\n      {\n        \"type\": \"notification-retry\",\n        \"notifRetryQueue\": \"\"\n      },\n      {\n        \"type\": \"enrichment\",\n        \"timer\": 60000,\n        \"fetchCount\": 1,\n        \"visibilityTimeout\": 10\n      }\n    ]\n  }\n}","best-practices#Best Practices":"Timer Settings\nUse shorter timers (500ms) for time-sensitive tasks like queries.\nUse longer timers (30000ms-60000ms) for background tasks like threat feeds and enrichments\nAdjust based on system load and requirements\nFetch Count\nUse higher fetch counts (10+) for quick-processing tasks\nUse lower fetch counts (1-2) for resource-intensive tasks\nConsider memory usage when setting this value\nParallelism\nMinimum of 3 workers per scheduler\nIncrease for high-volume workloads\nMonitor system resources to find optimal values."}},"/detections":{"title":"Detections - Scheduled Security Queries & Alert Management","data":{"":"Detections currently are queries that execute on a schedule and their results\nof all queries that are run are saved to our underlying database. You can view\nthe historic detection queries that have run on the RunReveal platform by\nsearching several underlying tables.\nscheduled_query_runs - The results of all scheduled query runs, their\nexecution times, the number of rows they returned, and parameter values\nthat were passed to them.\ndetections - The rows that your detection queries return. These results\ncontain the metadata of the associated detection, like risk score, mitre\nattacks, etc.","utilizing-scheduled_query_runs#Utilizing scheduled_query_runs":"If you are curious if a query of yours is executing, failing, erroring, or\nyour parameters are being passed correctly, the scheduled_query_runs table\nis exceptionally helpful.For example, here's a way to look for errors that have occurred while\nexecuting in a detection called 'ExampleQuery'.\nselect *\nfrom scheduled_query_runs\nwhere queryName='ExampleQuery' and error!='' and executionTime > now() - interval '1 day'","utilizing-detections#Utilizing detections":"The detections table is further subdivided into two views (that you can\ninteract) with like a tables.\nsignals - Detection queries that ran with the purpose of collecting data\nand have no notification channel configured.\nalerts - Detection queries that ran and alerted one of your notification\nchannels.\nThe detections table contains a row for each individual row returned by your\nquery. Fields returned by your query that match the column names of our defined\nschema are saved directly into those columns in the detection table.There is a limit of 100 rows that  acn be saved with the detection table. If\nyou need to exceed this limit for whatever reason, please contact us.","detection-data-model#Detection Data Model":"The results of your detection queries are saved to the detections table. The\ndetections table contains several fields, along with their types.\nid - String - Unique identified of the run\nscheduledRunID - String - The unique identifier of the scheduled query run\nworkspaceID - String - Your workspace ID\ndetectionID - String - The identified of the detection\ndetectionName - String - The name of the detection\nrecordsReturned - Int32 - The number of rows returned by the query\nrunTime - Int64 - The number of nanoseconds the query took to run\nquery - String - The actual query that was run for the scheduled query\nparams - Map(String, String) - The supplied parameters to the scheduled query\ncolumnNames - Array(String) - An ordered array of column names returned by the query\ncolumnTypes - Array(String) - An ordered array of the column types returned by the query\nresults - String - An array of the first 100 returned values from the query\nseverity - String - A string representing the severity of the alert\nactor - Map(String, String) - Details about the user that ran the query\nresources - Array(String) DEFAULT [] - Details about the resources returned from the query\nsrcIP - String - Details about the srcIP in the log entries\ndstIP - String - Details about the dstIPs from the log entries\nnotificationNames - Array(String) - The names of the notification channels\ncategories - Array(String) DEFAULT [] - The categories that the query belongs to\nmitreAttacks - Array(LowCardinality(String)) DEFAULT [] - The MITRE ATT&CK technique categories that the query belongs to\nThese tables can be accessed like any other table in RunRevealTo query the detection table:\nselect * from detections\nTo query the signals view:\nselect * from signals\nTo query the alerts view:\nselect * from alerts"}},"/detections/detection-as-code/deployment":{"title":"Deployment","data":{"":"A Github Action is available to use in your Github workflows. You can find the source code for it at, https://github.com/runreveal/detection-sync-action.","tokens-and-workspace#Tokens and Workspace":"To use the action you will need to store your generated API token and workspace ID in your Github account to be used as variables in the action. It is recommended to store your API token as a secret so that it remains hidden.To generate an API token navigate to your RunReveal settings page, and scroll down to Generate New Token.Give the token a name and set the role as cibot. This will allow the workflow to edit detections but make no other changes to your account.\nCopy the token value and save it for later you will need to add it into your repo settings.\nYou will also need to save your workspace ID that will be referenced in the workflows. This can also be found on your RunReveal settings page under the item labeled Your workspace's unique identifier.","workflows#Workflows":"In your Git repo include a directory called .github/workflows in the root of your repo.\nIn this folder you can include the yaml files for your Github workflows, below are examples of two that we use to verify and sync our detections.\n# Combined workflow for testing and syncing RunReveal detections\nname: RunReveal Detections CI\non:\n  # Trigger on push to main branch affecting detections/\n  push:\n    branches: [main]\n    paths:\n      - \"detections/**\"\n  # Trigger on pull requests to main affecting detections/\n  pull_request:\n    branches: [main]\n    paths:\n      - \"detections/**\"\n  # Allow manual triggering from the Actions tab\n  workflow_dispatch:\njobs:\n  # Always test detections on PRs and pushes to main\n  test-detections:\n    name: Test Detections\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: test-runreveal-detections\n        uses: runreveal/detection-sync-action@v2\n        with:\n          directory: ./detections\n          token: ${{ secrets.RUNREVEAL_TOKEN }}\n          workspace: ${{ secrets.RUNREVEAL_WORKSPACE }}\n          dry-run: true\n  # Only sync detections after test passes, and only on push to main or manual dispatch\n  sync-detections:\n    name: Sync RunReveal Detections\n    needs: [test-detections]\n    if: |\n      (github.event_name == 'push' && github.ref == 'refs/heads/main') || github.event_name == 'workflow_dispatch'\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: sync-runreveal-detections\n        uses: runreveal/detection-sync-action@v2\n        with:\n          directory: ./detections\n          token: ${{ secrets.RUNREVEAL_TOKEN }}\n          workspace: ${{ secrets.RUNREVEAL_WORKSPACE }}\nUpdate the token and workspace inputs with the names that you used when creating your secrets/variables.These workflows assume all detections are stored in a subdirectory called detections.\nAs of now, you can only sync one directory per workspace. Any detections that are managed by detection as code and are not listed in your directory will be automatically removed from your account.","github-repo-secrets#Github Repo Secrets":"Navigate to the settings page for your repo and go to Secrets and variables -> Actions.Under the secrets tab, you can create a New repository secret. Your name should be the value that you used in your workflow, in our example we used RUNREVEAL_TOKEN and RUNREVEAL_WORKSPACE.\nWe would create two new secrets with those names and paste the appropriate values that we saved from earlier.","commit-detections#Commit Detections":"You are now ready to start uploading your detections. Make a commit to the repo containing all of your detections under the directory listed in your workflow, in our example that would be ./detections/.\nAn important note about the flow of detection as code.If you exported all of your workspace's detections using the runreveal detections export cli command or from the UI, then uploading those detections will give an error as there will be a naming collision.In order to allow Github actions to work as expected you will need to run the command runreveal detections sync -d ./detections -o, the -o flag will override all detections that have a matching name regardless where that detection is being managed from."}},"/detections/detection-as-code/export-detections":{"title":"Export Detections","data":{"":"To get started or to adopt detection as code after already adding detections to\nyour account the RunReveal CLI has the commands necessary to bootstrap a code\nrepo from nothing, so you canOnce you're logged into the CLI from the getting started\nsteps, you can utilize the export functionality to download all detection state\ninto a directory of your choosing.\n:) runreveal detections export -d ~/detection-as-code\n:) ls -l ~/detection-as-code \ntotal 0\ndrwxr--r--   4 evan  staff   128 Jun 18 09:42 1password\ndrwxr--r--   8 evan  staff   256 Jun 18 09:42 cf-audit\ndrwxr--r--  16 evan  staff   512 Jun 18 09:42 cloudtrail\ndrwxr--r--  16 evan  staff   512 Jun 18 09:42 gcp\ndrwxr--r--  20 evan  staff   640 Jun 18 09:42 google-workspace\ndrwxr--r--   8 evan  staff   256 Jun 18 09:42 gsuite\ndrwxr--r--  18 evan  staff   576 Jun 18 09:42 notion\ndrwxr--r--  18 evan  staff   576 Jun 18 09:42 okta\ndrwxr--r--   6 evan  staff   192 Jun 18 09:42 tailscale-audit\ndrwxr--r--  60 evan  staff  1920 Jun 18 09:42 uncategorized\nThese detections are categorized by sourceType and will be\nsorted into directories by the categories tag if they\nmatch the name of any sourceType within RunReveal.After exporting each detection will be output into two files. The detection\nquery and the detection configuration and metadata.Here's an example containing both the yaml and sql for our Okta detection of\na user reporting a suspicious action.\n:) cat okta-user-suspicious-report.sql \nselect * from okta_logs where\n  eventType='user.account.report_suspicious_activity_by_enduser'\n  and receivedAt > {from:DateTime} and receivedAt < {to:DateTime}%\n:) cat okta-user-suspicious-report.yaml\nname: okta-user-suspicious-report\ndisplayName: Okta User Reported Suspicious Activity\ndescription: Suspicious activity reported by end user\nfile: okta-user-suspicious-report.sql\ncategories:\n- signal\n- okta\nsourceTypes:\n- okta\nschedule: \"*/15 * * * *\"\nnotificationNames: []\nmitreAttacks:\n- initial-access\nseverity: \"\"\nriskScore: 0","first-time-you-export#First time you export":"The first time you export existing detections that were made in the web ui\nyou'll first need to upload them and force overwrite them from the CLI before\nyour CI/CD system can use them.This step will force overwrite all detections that you have in the repo so\nthey will be managed via detection as code going forward.\n$ runreveal detections sync -o -d ~/detection-as-code"}},"/detections/detection-as-code/uploading-detections":{"title":"Uploading Detections","data":{"":"When it's time to upload detections, RunReveal's CLI is built specifically to\nmake this process simple and not error prone. The upload takes place through\nour API.","test-your-upload#Test your upload":"When you want to test to ensure that each detection is properly formatted and\nsee which detections have been updated, deleted, or created, the dry run flag\nwill show you exactly the information you want.\n$ runreveal detections sync -d ~/detection-as-code --dry-run                                                            \nprocessing '/Users/evan/detection-as-code/1password/1password-unusual-client.yaml'                                           \nprocessing '/Users/evan/detection-as-code/cf-audit/anomalous-api-key-usage.yaml'                                             \nprocessing '/Users/evan/detection-as-code/cf-audit/new-cloudflare-api-key.yaml'                                              \nprocessing '/Users/evan/detection-as-code/cf-audit/user-added-removed-cloudflare.yaml\n...\n{                                                                                                                            \n     \"detections\": {                                                                                                         \n         \"added\": [                                                                                                                                                                                                                                       \n             \"1password-unusual-client\",                                                                                                                                                                                                             \n             \"anomalous-api-key-usage\",                                                                                                                                                                                                                 \n             \"new-cloudflare-api-key\",                                                                                                                                                                                                                        \n         ],\n         \"deleted\": [],                                                                                                                                                                                                                                   \n         \"updated\": [                                                                                                                                                                                                                                     \n             \"user-added-removed-cloudflare\"                                                                                         \n         ]                                                                                                                   \n     },                                                                                                                      \n     \"dryRun\": true                                                                                                          \n }","uploading#Uploading":"The only difference between uploading and performing a dryrun is the --dry-run flag.\nRemove the --dry-run flag and your detections will be uploaded.\n$ runreveal detections sync -d ~/detection-as-code\nIf you created a detection in the UI, and then attempt to upload your detection\nfrom the CLI you will see an error message the first time you attempt to\noverwrite your UI detection.It will look a little something like this:\n$ runreveal detections sync -d ~/detection-as-code --dry-run                                                            \nprocessing '/Users/evan/detection-as-code/1password/1password-unusual-client.yaml'                                           \nprocessing '/Users/evan/detection-as-code/cf-audit/anomalous-api-key-usage.yaml'                                             \nprocessing '/Users/evan/detection-as-code/cf-audit/new-cloudflare-api-key.yaml'                                              \nprocessing '/Users/evan/detection-as-code/cf-audit/user-added-removed-cloudflare.yaml\nERROR  This detection name is already in use, please choose a different name. If you're trying to upload your detections using detection as code, try the -o flag to overwrite existing detections created in the UI.\nIf you see this error, then adding the -o flag will overwrite the existing\ndetection and convert the detection from being web managed to being cli\nmanaged.","force-uploading#Force Uploading":"If you receive an error because the detections are managed with the web interface\nyou'll need to run the following command to convert your detections to managed\nby the CLI\n$ runreveal detections sync -o -d ~/detection-as-code"}},"/detections/sigma-streaming":{"title":"Sigma Streaming Detections - Real-Time Security Rule Processing","data":{"":"RunReveal also offers Sigma streaming detections that can be checked while the event is being processed in the pipeline.","how-it-works#How it works":"A Sigma detection consists of a yaml file that lists metadata about the detection and the detection rule that will be matched against events.\nA user will upload a Sigma detection yaml file with at least a title and a detection.\nSigma yaml can also contain additional context for the detection that will be saved when the detection matches.\nWhen an event is ingested we check if the Sigma detection is setup to run for this source.\nIf it is, we run the event's raw log and any defined enrichments through our Sigma processor to look for a match.\nIf the Sigma rule matches, we check if the rule supplies any notification channels to alert to and then the event is inserted into your detections table in Clickhouse.","sigma-rules#Sigma Rules":"RunReveal supports standard Sigma rule properties along with additional fields that help provide more context to the detection. At its most basic level a Sigma detection\nrequires a title which is a unique descriptor to tell your detections apart, and a detection block which contains the rules that are checked.","sources#Sources":"If a Sigma rule only contains the required fields, your detection will be uploaded but won't run on any events unless a source is also defined in the yaml.Sigma allows you to define a source using the logsource property.\nlogsource:\n  category: cloud\n  product: aws\n  service: cloudtrail\nRunReveal uses this field and checks all of the child properties to match them to a RunReveal source type.\nIf one of these properties matches a source type, e.g. the service field matches our cloudtrail source type, then all events ingested with the cloudtrail source type will be checked.We also offer two extra fields to specify which sources this rule applies to sourcenames and sourcetypes.sourcenames is a string array where you can list the unique names of your sources that you want this rule to evaluate on. This is useful if you have multiple sources with the same type (like a cloudtrail source for different environments) but only want a detection to run for your 'prod-cloudtrail' source.sourcetypes is also a string array where you can list multiple source types that the detection should match against. This can be useful if you want the same detection rule to match on different source types.","sigma-provided-fields#Sigma provided fields":"Sigma rules have a few fields that RunReveal uses to provide some extra context to the detection.description is used to provide a text description on what the detection does.level is used to give a detection a severity, RunReveal uses low, medium, high, and critical levels.tags are stored with a matched detection in the RunReveal category field. They can be used to help group detections.","extra-runreveal-fields#Extra RunReveal fields":"RunReveal also provides additional fields for the Sigma rule to provide additional context or control the detection.\nSigma Field\tField Description\tdisabled\tIf set to true the detection will be listed in your account but will not be checked with incoming events.\tnotes\tAdditional notes about your detection to give additional context.\triskscore\tA score that can be assigned to a detection that can be used when performing signal chaining.\tmitreattacks\tA list of mitre attack framework techniques that this detection checks. This can be used when identifying attack patterns.\tnotificationnames\tAn array of notification channel names in your RunReveal workspace, if set and the detection triggers, an alert will be sent to the notifications listed.","example-sigma-rule#Example Sigma Rule":"Below is an example Sigma rule that will match when an okta sign in event fails.\ntitle: Detect Okta failures\ndescription: |-\n  This detection will produce a record in the `signals` table when an okta signin attempt fails.\nlogsource:\n  product: okta\ndetection:\n  filter:\n    eventType: user.session.start\n    outcome.result: FAILURE\n  condition: filter","yaml-schema#YAML schema":"To learn more about writing detections using the Sigma schema check out the Sigma docs.To learn more about the open source library RunReveal uses to parse Sigma rules, check out the SigmaLite repo.\nThe following can be copied into a blank file to start creating your own Sigma detections.\n# Required: A unique title of the detection to identify it.\ntitle: The detection title\n# Required: The detection that will be matched against the incoming events.\n# In this scenario the detection will check the event for the text `runreveal.com`\ndetection:\n  keywords:\n    - runreveal.com\n  condition: keywords\n# Optional: Set to true to turn off this detection.\ndisabled: false\n# Optional: A description of the detection.\ndescription: A description of the detection and what it is doing.\n# Optional: An array of strings to help group queries\ntags:\n  - tag1\n  - tag2\n# Optional: A severity string to identify the importance of the detection results.\n# low, medium, high, critical\nlevel: low\n# Optional: An integer score to give the detection.\nriskscore: 40\n# Optional: Additional notes that can be attached to the detection.\nnotes: |-\n  Extra info about the detection.\n# Optional: An array of mitre attack framework classifications. \n# This is useful when identifying attack patterns.\nmitreattacks:\n  - evasion\n  - initial-access\n# Optional: An array of notification channel names that should receive an alert if this detection triggers\nnotificationnames:\n  - email\n## logsource, sourcetypes, and sourcenames are all optional, but in order for the detection to be associated with specific events \n## one of them needs to be filled in with a source that your workspace uses.\n# If the source type of the event matches on of the logsource fields, this detection will be checked.\nlogsource:\n  category: cloud\n  product: aws\n  service: cloudtrail\n# An array of source types, if the source type of any event matches one in this list the detection will be checked. \nsourcetypes:\n  - okta\n  - cloudtrail\n# An array of source names, if the source name the event was generated from matches an item in this list the detection will be checked.\nsourcenames:\n  - runreveal-okta"}},"/detections/writing-a-detection":{"title":"Writing SQL Detections - Query Parameters and Time Windowing","data":{"":"RunReveal detections are queries that run on a schedule. Currently, SQL is the only supported\nlanguage for detections, althrough pql is planned to be supported soon (and isn't difficult\nto translate to valid clickhouse SQL).","parameters#Parameters":"When using the explore page and creating a detection you'll notice that the generated query\nhas {from:DateTime} and {to:DateTime} in it. These are parameters. Parameters are a clickhouse\nspecific syntax that you can read more about on their docs:https://clickhouse.com/docs/en/sql-reference/syntax#defining-and-using-query-parameters","always-parameters#Always Parameters":"RunReveal passes the following parameters to each of your detections every time they run to help with windowing your data.\nfrom - The start-time of the current data window.\nto - The end-time of the current data window.\nBy including these parameters in your detection, you can support effective windowing, without needing to worry\nabout lost data, delayed data, etc. It's best to include in your where clause a check that the receivedAt time\nis greater than or equal to the from time, and less than the to time. By using the receivedAt time\nfrom the RunReveal log schema, you automatically handle situations where your services generate logs that\narrive on a delay (for example, cloudtrail).For example:\nSELECT \n    *\nFROM aws_cloudtrail_logs\nWHERE\n    eventName='Decrypt' AND\n    receivedAt >= {from:DateTime} AND\n    receivedAt < {to:DateTime}","defining-your-own-parameters#Defining your own parameters":"Parameters can also be defined as well, for useful omissions, inclusions, etc. And you'll notice several of RunReveal's\npre-created detections contain parameters that can be used like this. For example, to create a detection and exclude your\nlist of office IP addresses, you can create a SQL detection like this:\nSELECT * from runreveal_logs\nWHERE eventName = 'thing-that-should-only-be-done-in-office'\nAND srcIP not in ({officeIPs:Array(String)})\nWith the officeIPs parameter defined like so to include each of your office IPs.\n1.2.3.4, 2.3.4.5, 4.3.2.1"}},"/enrichments":{"title":"Log Enrichments - Augment Security Data with Supplemental Information","data":{"":"Enrichments allow you to augment your log events with supplemental data based on pattern matching.","how-it-works#How it works":"An enrichment in Runreveal consists primarily of two parts:\nAn enrichment configuration, which contains metadata about your enrichment like Name and Description, which log sources to\nenrich, as well as information about which log event field you wish to match with.\nA set of enrichment Rules, which describe the data you wish to add to your log events, and the conditions when you\nwould like that data added.","example#Example":"Let's say you are wanting to enrich log events from your cloudflare audit log source, to map user emails to user id's\nfrom some other external system you have.First, let us take a look at an example log event from that source, a log in event from bob@example.com:\n{\n  \"action\": {\n    \"result\": true,\n    \"type\": \"login\"\n  },\n  \"actor\": {\n    \"email\": \"bob@example.com\",\n    \"id\": \"c38246172e13e3e341f7ef3319d2c913\",\n    \"ip\": \"24.225.52.121\",\n    \"type\": \"user\"\n  },\n  \"id\": \"a683351a-5e63-7272-72b6-6bf7f712fdf2\",\n  \"interface\": \"\",\n  \"metadata\": {},\n  \"newValue\": \"\",\n  \"oldValue\": \"\",\n  \"owner\": {\n    \"id\": \"c38246172e13e3e341f7ef3319d2c913\"\n  },\n  \"resource\": {\n    \"id\": \"c38246172e13e3e341f7ef3319d2c913\",\n    \"type\": \"account\"\n  },\n  \"when\": \"2024-08-20T23:47:18Z\",\n  \"newValueJson\": {},\n  \"oldValueJson\": {}\n}\nTo create our enrichment configuration, we need to determine which log event field we want to match with to look up our\nsupplemental data.  We tell the enrichment configuration how to find this field using\ngjson path notation.  In this case, we could create\nan enrichment configuration with path actor.email configured to enrich the cloudflare audit log source.\nNext, we will want to define a set of rules which will map cloudflare user email adresses to user id's.  If bob@example.com here\nhad user id 12345, then we could make a rule:\nexact, bob@example.com, {\"user_id\": \"12345\"}\nThis would tell Runreveal to match any log event where the value of actor.email is exactly bob@example.com.  If the event\ndoes match, then {\"user_id\": \"12345\"} will be added to the enrichments field of the log event before it is written to its destination.Now, we can save and enable our enrichment by clicking the Create Enrichment button:\nTo verify that our enrichment is working, we can search for our newly enriched events on the Explore page:\nFrom the results, we can see our resulting log event ends up in the logs table as:\n{\n  ...\n  \"enrichments\": [\n    {\n      \"data\": {\n        \"user_id\": \"12345\"\n      },\n      \"name\": \"actor.email\",\n      \"provider\": \"cloudflare-email-to-external-id\",\n      \"type\": \"custom\",\n      \"value\": \"bob@example.com\"\n    }\n  ],\n  \"rawLog\": {\n    \"action\": {\n      \"result\": true,\n      \"type\": \"login\"\n    },\n    \"actor\": {\n      \"email\": \"bob@example.com\",\n      \"id\": \"c38246172e13e3e341f7ef3319d2c913\",\n      \"ip\": \"24.225.52.121\",\n      \"type\": \"user\"\n    },\n    \"id\": \"a683351a-5e63-7272-72b6-6bf7f712fdf2\",\n    \"interface\": \"\",\n    \"metadata\": {},\n    \"newValue\": \"\",\n    \"oldValue\": \"\",\n    \"owner\": {\n      \"id\": \"c38246172e13e3e341f7ef3319d2c913\"\n    },\n    \"resource\": {\n      \"id\": \"c38246172e13e3e341f7ef3319d2c913\",\n      \"type\": \"account\"\n    },\n    \"when\": \"2024-08-20T23:47:18Z\",\n    \"newValueJson\": {},\n    \"oldValueJson\": {}\n  },\n  ...\n}"}},"/enrichments/using-the-api":{"title":"Using The API","data":{"":"You can create and edit your enrichments using the runreveal API if you need more programmatic control.  This is\nespecially useful if you have a large number of rules, or rules that change often.","authentication-and-authorization#Authentication and Authorization":"Authentication with the API is done using API tokens which you can generate in the UI.  These tokens are scoped to specficic\nroles which are authorized to perform different sets of actions.  For working with enrichments, you will want to create a token\nwith either the admin role (which has full read/write/delete access) or the analyst role (which has read/edit permissions).Once you have generated a token, you can use it by setting the Authorization header:Authorization: Basic <TOKEN>","making-requests#Making Requests":"Requests can be made by appending the desired endpoint to the api base url (https://api.runreveal.com) and setting the appropriate\nAuthorization header.  For POST endpoints, you should also set the appropriate Content-type header, application/json for all\nenrichment endpoints.\nTip: All endpoints require workspace id as a query parameter, you can find your workspace id on the workspace settings page\nExamples:\nCreating an enrichment:\n$ TOKEN=\"<your token>\"\n$ WKSPCID=\"<your workspace id>\"\n$ CREATE_JSON=\"{\n  \\\"name\\\": \\\"example\\\",\n  \\\"workspaceID\\\": \\\"${WKSPCID}\\\",\n  \\\"description\\\": \\\"example enrichment\\\",\n  \\\"sources\\\": [\\\"my-source\\\"],\n  \\\"path\\\": \\\"normalized.actor.email\\\",\n  \\\"isActive\\\": true,\n  \\\"rules\\\": [\n    {\n      \\\"type\\\": \\\"exact\\\",\n      \\\"pattern\\\": \\\"bob@example.com\\\",\n      \\\"data\\\": {\n        \\\"userid\\\": \\\"12345\\\"\n      }\n    }\n  ]\n}\"\n$ curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/enrichments/create?workspaceid=${WKSPCID}\" --data-raw \"${CREATE_JSON}\"\nimport requests\nimport json\nTOKEN = \"your-token\"\nWORKSPACE_ID = \"your-workspace-id\"\nCREATE_JSON = {\n    \"name\": \"example\",\n    \"workspaceID\": WORKSPACE_ID,\n    \"description\": \"example enrichment\",\n    \"sources\": [\"test\"],\n    \"path\": \"normalized.actor.email\",\n    \"isActive\": True,\n    \"rules\": [\n        {\"type\": \"exact\", \"pattern\": \"bob@example.com\", \"data\": {\"userid\": \"12345\"}}\n    ],\n}\nURL = \"https://api.runreveal.com/enrichments/create\"\nheaders = {\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Basic \" + TOKEN,\n}\nresponse = requests.post(\n    URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    data=json.dumps(CREATE_JSON),\n    headers=headers,\n)\nprint(response.json())\nUpdating an Enrichment:\n$ TOKEN=\"<your token>\"\n$ WKSPCID=\"<your workspace id>\"\n$ UPDATE_JSON=\"{\n  \\\"id\\\": \\\"2oFro0ffPTle8mRgigX44o41f4U\\\",\n  \\\"name\\\": \\\"example\\\",\n  \\\"workspaceID\\\": \\\"${WKSPCID}\\\",\n  \\\"description\\\": \\\"example enrichment\\\",\n  \\\"sources\\\": [\\\"my-source\\\"],\n  \\\"path\\\": \\\"normalized.actor.email\\\",\n  \\\"isActive\\\": true,\n  \\\"rules\\\": [\n    {\n      \\\"type\\\": \\\"exact\\\",\n      \\\"pattern\\\": \\\"bob@example.com\\\",\n      \\\"data\\\": {\n        \\\"userid\\\": \\\"12345\\\"\n      },\n    },\n    {\n      \\\"type\\\": \\\"exact\\\",\n      \\\"pattern\\\": \\\"alice@example.com\\\",\n      \\\"data\\\": {\n        \\\"userid\\\": \\\"67890\\\"\n      }\n    }\n  ]\n}\"\n$ curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/enrichments/update?workspaceid=${WKSPCID}\" --data-raw \"${UPDATE_JSON}\"\nimport requests\nimport json\nTOKEN = \"your-token\"\nWORKSPACE_ID = \"your-workspace-id\"\nCREATE_JSON = {\n    \"id\": \"2oFro0ffPTle8mRgigX44o41f4U\",\n    \"name\": \"example\",\n    \"workspaceID\": WORKSPACE_ID,\n    \"description\": \"example enrichment\",\n    \"sources\": [\"test\"],\n    \"path\": \"normalized.actor.email\",\n    \"isActive\": True,\n    \"rules\": [\n        {\"type\": \"exact\", \"pattern\": \"bob@example.com\", \"data\": {\"userid\": \"12345\"}}\n        {\"type\": \"exact\", \"pattern\": \"alice@example.com\", \"data\": {\"userid\": \"67890\"}}\n    ],\n}\nURL = \"https://api.runreveal.com/enrichments/update\"\nheaders = {\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Basic \" + TOKEN,\n}\nresponse = requests.post(\n    URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    data=json.dumps(CREATE_JSON),\n    headers=headers,\n)\nprint(response.json())","resources#Resources":"","enrichment#Enrichment":"Parameters:\nName\tType\tDescription\tid\tstring\tAutogenerated ID, returned from read endpoints\tname\tstring\tName for the enrichment\tworkspaceID\tstring\tYour workspace ID\tdescription\tstring\tSmall description of the enrichment\tsources\tlist<string>\tList of Sources that the enrichment should apply to\tpath\tstring\tA gjson-compatible path specifying which log field your enrichment rules should match against.\tisActive\tbool\tWhether the enrichment is currently active\trules\tlist<enrichment_rule>\tList of rules to apply to each log event\t\nExample:\n{\n  \"id\": \"2oFgWhP298CnRowC3hEhD2mdRed\",\n  \"name\": \"My Enrichment\",\n  \"workspaceID\": \"2oFgWhJ2FOS8PRnv8dTG7XPSeIp\",\n  \"description\": \"Example enrichment\",\n  \"sources\": [\"mysourcename\"],\n  \"path\": \"normalized.actor.email\",\n  \"isActive\": true,\n  \"rules\": [\n    {\n      \"id\": \"2oFgWjb5mWnOOcUWbVkLjxnXgRb\",\n      \"type\": \"exact\",\n      \"pattern\": \"test@example.com\",\n      \"data\": {\n        \"userid\": \"1234567\"\n      }\n    }\n  ]\n}","enrichment-rule#Enrichment Rule":"Parameters:\nName\tType\tDescription\tid\tstring\tAutogenerated ID, returned from read endpoints\ttype\tstring\tMatching type for rule, must be one of \"exact\", \"cidr\", or \"regex\"\tpattern\tstring\tPattern to match against\tdata\tmap<string, string>\tData to attach to matching log events\t\nExamples:CIDR matching\n{\n  \"id\": string,\n  \"type\": string,\n  \"pattern\": string,\n  \"data\": map<string, string>,\n}","endpoints#Endpoints":"","post-enrichmentscreate#POST /enrichments/create":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\t\nBody: EnrichmentRequired fields:\nname\nworkspaceID\ndescription\nsources\npath\nisActive\nrules\nNote: id is not required on creation, it will be generated server side\nTip: You can get the valid source names available for use in the sources field by querying the /sources/list?workspaceid={workspaceid} endpoint","post-enrichmentsupdate#POST /enrichments/update":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\t\nBody: EnrichmentRequired fields:\nid\nname\nworkspaceID\ndescription\nsources\npath\nisActive\nrules\nNote: id is required for updates","get-enrichmentsget#GET /enrichments/get":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\tid\tThe enrichment id to get\tYes","get-enrichmentslist#GET /enrichments/list":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\t\nNote: For performance reasons, the list endpoint does not return individual enrichment rules.  To see those, use the get endpoint for a specific configuration.","get-enrichmentsdelete#GET /enrichments/delete":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\tid\tThe id of the enrichment to delete\tYes","post-enrichmentsappendrules#POST /enrichments/appendrules":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\tid\tThe id of the enrichment to append to\tYes\t\nBody: List<EnrichmentRule>Required Fields:\ntype\npattern\ndata\nNote: id is not required on append, ids will be generated server side"}},"/enrichments/loading-rules-from-csv":{"title":"Loading Rules From CSV","data":{"":"If you are creating an enrichment with many rules, you may find it more convenient to load your rule definitions from a CSV instead of\nmanually creating each rule in the UI.  To do so, you should prepare a CSV with:\nExactly 3 columns\nComma delimited\nDouble quoted values.  Escaping of double quotes within values can be done with double double quotes (\"\")\nwhere each row is an enrichment rule of the form:\nmatch_type,pattern,data\nWhere:\nmatch_type is one of exact, regex, or cidr\npattern is the pattern to match for this rule\ndata is a JSON object with the data to be added to log events matching this enrichment rule.  data is furthermore restricted to string\nkeys and values, with no nested objects.\nSome valid rule examples:\n\"exact\",\"alice@example.com\",\"{\"\"user_id\"\":\"\"12345\"\"}\"\n\"regex\",\"\\.org^\",\"{\"\"email_tld\"\":\"\"org\"\"}\"\n\"cidr\",\"10.0.0.0/16\",\"{\"\"location\"\":\"\"san francisco\"\"}\"\nOn the create/edit enrichment page, in the Rules section, you will find a Load from file button.\nOn successfully loading your rules, you will see the table below populated.  If you are happy with the results, you can click Create Enrichment (or Update Enrichment if this is an update) to enable your new rules."}},"/filtering":{"title":"Log Filtering - Filter Security Data with Regular Expressions","data":{"":"RunReveal supports log filtering as a first class feature of the product. The current supported\nmethods are:\nRegular Expression Filtering","regex-filtering#Regex Filtering":"To filter with a regular expression, click to add a new filter.\nThe pattern is the regular expression that we will match on. By default, all filter rules will\ndrop any log that matches the pattern. For example, consider the following log:\n{\"user\":\"evan\", \"event\":\"login\", \"ip\":\"1.1.1.1\"}\nSpecifiying this pattern will cause the log to be dropped.\nuser","flags#Flags":"We support the following regular expression flags. DO NOT add these flags to your pattern.\nInstead add filter flags using the Filter Flags checkbox toggles.\ni - Case Insensitivity - Ignore case when matching."}},"/how-to-guides":{"title":"How-To Guides - Step-by-Step Instructions for RunReveal","data":{"":"Welcome to the RunReveal How-To Guides section. Here you'll find step-by-step instructions for common tasks and use cases to help you get the most out of RunReveal.","available-guides#Available Guides":"","collecting-nginx-logs#Collecting Nginx Logs":"Learn how to use Reveald, our efficient log collection daemon, to collect and process Nginx logs for visualization in RunReveal.\nInstallation and configuration of Reveald\nSetting up a structured webhook source in RunReveal\nConfiguring Nginx to forward logs to Reveald\nViewing and analyzing your Nginx logs in RunReveal","kubernetes-logs#Kubernetes Logs":"Configure log collection from Kubernetes clusters using Reveald as a daemonset.\nSetting up Reveald as a Kubernetes daemonset\nConfiguring Reveald to collect pod logs\nForwarding logs to RunReveal\nBest practices for Kubernetes log collection","quick-start-detections-signals-and-alerts#Quick Start: Detections, Signals, and Alerts":"Learn the fundamentals of RunReveal's detection system with hands-on examples.\nCreating webhook sources for test data\nBuilding SQL-based detection rules\nUnderstanding signals vs alerts\nSetting up notification workflows","need-more-help#Need More Help?":"If you need assistance with implementation or have questions about collecting logs from other sources, please reach out through:\nOur Discord community\nEmail us at contact@runreveal.com\nUse the chat bubble on our website"}},"/how-to-guides/collect-nginx-logs":{"title":"Collect Nginx Logs","data":{"":"Reveald is an easy-to-use, performant,\nefficient, and open source log and event collection daemon for endpoint\ncollection.We can use reveald to collect nginx logs to be visualized in RunReveal.  In this\nguide, we'll install, configure and run reveald on the machine running nginx (or\na host that is visible on the network from the host running nginx).First, download a reveald release for the operating system and architecture of\nthe host you'll be running the daemon on from the releases page\nhere.\n(https://github.com/runreveal/reveald/releases)\nExtract the package to a directory of your choosing on the host.Next, login to RunReveal and create a structured\nwebhook source.  This will give\nus a webhookURL needed to configure a destination in reveald to send our data to\nrunreveal.  You can create a source directly using this\nlinkHere is a sample configuration for the syslog listener for which we'll be\nsending our logs.  Replace {{YOUR-RUNREVEAL-WEBHOOKURL}} with the webhook\nreceived from the UI in the step above.\n{\n  \"sources\": {\n    \"nginx-logs\": {\n        \"type\": \"nginx_syslog\",\n        \"addr\": \"127.0.0.1:5514\",\n    },\n  },\n  \"destinations\": {\n    \"rr-dest\": {\n      \"type\": \"runreveal\",\n      \"webhookURL\": \"{{YOUR-RUNREVEAL-WEBHOOKURL}}\",\n    },\n  },\n}\nWe can run reveald pointing at the configuration like so (assuming you download\nit to the same directory as the binary).\n./reveald run --config=./config.json\nNow, add this access_log line to the server block for the sites that you\nwish to receive access logs from.  If you're not running reveald on the same\nhost as nginx, replace the localhost address 127.0.0.1 with the host IP you've\nconfigured reveald to listen on.\nserver {\n    access_log syslog:server=127.0.0.1:5514 combined;\n    # ... other config ...\n}\nTest your nginx config and reload the nginx process:\nsudo nginx -s reload\nYou should now be getting logs in your RunReveal workspace!  Let's run some\nqueries on RunReveal once you've gotten some traffic to the site you've\nconfigured.\nThat's it!  If you encounter any problems setting this up, please feel free to\nreach out on our discord, via the chat bubble on the website, or via\ncontact@runreveal.com."}},"/how-to-guides/detections-signals-alerts-quick-start":{"title":"Quick Start: Detections, Signals, and Alerts","data":{"":"This guide walks you through RunReveal's detection system using hands-on examples. We will go over adding a webhook to send test logs to via curl to test detections and see how Detections, Signals, and Alerts work together.Detections, Signals, and Alerts are all classified as detection results and are part of the Detections table. Signals are detection results without notifications (for analysis/tuning). Alerts are detection results that trigger notifications.\n\tDetections\tSignals\tAlerts\tWhat it is\tAutomated rules that analyze logs\tDetection results without notifications\tDetection results with notifications\tPurpose\tContinuous monitoring and analysis\tPattern recognition and tuning\tImmediate awareness and response\tFiltering\tdetections table\tsignals table\talerts table\tNotifications\tNone\tNone\tEmail, Slack, PagerDuty, etc.\tUse Case\tSecurity policy enforcement\tThreat hunting and analysis\tIncident response workflows","getting-started#Getting Started":"This guide walks you through the complete workflow for setting up detections, signals, and alerts in RunReveal:\nCreate Webhook Source - Set up a data source to receive test events (Either a Structured Webhook or Generic Webhook Source in UI)\nSend Test Data - Generate sample security events for testing\nCreate Detection Rule - Build a SQL-based detection that analyzes your data\nTrigger and Review - Manually run the detection and check results\nCompare Signal vs Alert - Convert to alerts and see the difference","step-1-create-webhook-source#Step 1: Create Webhook Source":"First, create a webhook source in RunReveal to receive test data:\nIn RunReveal UI:\nNavigate to Sources in the left sidebar\nClick Connect a new source\nSelect Webhook from the source types\nConfigure the webhook:\nName: test-webhook-source\nDescription: Test webhook for detection guide\nClick Create or Save\nCopy the webhook URL - it will look like: https://api.runreveal.com/sources/hook/YOUR_WEBHOOK_ID","step-2-send-test-data#Step 2: Send Test Data":"Now send a test security event using your webhook URL:\nTest Event:\ncurl -X POST https://api.runreveal.com/sources/hook/30pw8Fynw5W7PzjbnRyxnfMsID2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"timestamp\": \"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\n    \"source\": \"test-system\",\n    \"eventName\": \"suspicious_login\",\n    \"severity\": \"high\",\n    \"user\": \"test-user\",\n    \"srcIP\": \"192.168.1.100\",\n    \"action\": \"login_attempt\",\n    \"result\": \"success\",\n    \"user_agent\": \"curl-test\",\n    \"metadata\": {\n      \"login_method\": \"ssh\",\n      \"session_id\": \"test-123\"\n    }\n  }'\nVerify ingestion to logs table:\nSELECT receivedAt, \n       sourceType,\n       JSONExtractString(rawLog, 'eventName') as eventName,\n       JSONExtractString(rawLog, 'user') as user,\n       JSONExtractString(rawLog, 'srcIP') as srcIP,\n       rawLog\nFROM logs \nWHERE sourceType = 'structured-webhook'\n  AND JSONExtractString(rawLog, 'eventName') = 'suspicious_login'\nORDER BY receivedAt DESC \nLIMIT 5;","step-3-create-a-test-detection-rule#Step 3: Create a Test Detection Rule":"Create a SQL detection that will trigger a signal from your test logs:\nIn RunReveal UI:\nGo to Detections > Create Detection\nName: test-suspicious-login\nType: SQL\nQuery:\nSELECT \n  JSONExtractString(rawLog, 'eventName') as eventName,\n  JSONExtractString(rawLog, 'user') as user,\n  JSONExtractString(rawLog, 'srcIP') as srcIP,\n  JSONExtractString(rawLog, 'user_agent') as user_agent,\n  JSONExtractString(rawLog, 'severity') as severity,\n  receivedAt\nFROM logs \nWHERE sourceType = 'structured-webhook'\n  AND JSONExtractString(rawLog, 'eventName') = 'suspicious_login'\n  AND JSONExtractString(rawLog, 'srcIP') = '192.168.1.100'\n  AND receivedAt BETWEEN {from:DateTime} AND {to:DateTime}\nSchedule: */5 * * * * (every 5 minutes)\nSeverity: Medium\nCategories: [\"authentication\", \"test\"]\nSave (Signal will be generated as a notification channel was not configured)","step-4-test-detection-using-cli-optional#Step 4: Test Detection using CLI (optional)":"Test your detection to verify it works:\nInstall and configure CLI:\nInstall RunReveal CLI:\nbrew install runreveal/tap/runreveal\nConnect to your workspace:\nrunreveal init\nFollow the prompts to authenticate and select your workspaceFor complete CLI reference, see CLI Documentation.\nFor SQL detections (queries workspace data):\nCreate SQL detection configuration file:Create file: test-suspicious-login.yaml\nname: test-suspicious-login\ndisplayName: Test Suspicious Login Detection\ndescription: Detects suspicious login attempts from specific IPs\ntype: sql\nfile: test-suspicious-login.sql\ncategories:\n  - authentication\n  - test\nsourceTypes:\n  - structured-webhook\nschedule: \"*/5 * * * *\"\nseverity: medium\nriskScore: 50\nCreate SQL query file:Create file: test-suspicious-login.sql\nSELECT \n  JSONExtractString(rawLog, 'eventName') as eventName,\n  JSONExtractString(rawLog, 'user') as user,\n  JSONExtractString(rawLog, 'srcIP') as srcIP,\n  JSONExtractString(rawLog, 'user_agent') as user_agent,\n  JSONExtractString(rawLog, 'severity') as severity,\n  receivedAt\nFROM logs \nWHERE sourceType = 'structured-webhook'\n  AND JSONExtractString(rawLog, 'eventName') = 'suspicious_login'\n  AND JSONExtractString(rawLog, 'srcIP') = '192.168.1.100'\n  AND receivedAt >= now() - INTERVAL 2 HOUR\nLIMIT 10\nTest the SQL detection:\n# Test against actual data in your workspace\nrunreveal detections test --file test-suspicious-login.yaml --from \"now-1h\" --to \"now\"\nNote: SQL detections must query real workspace data. Make sure you've sent test events via webhook (Step 2) before running this command.\nFor Sigma detections (tests with local samples):\nCreate Sigma detection file:Create file: test-suspicious-login-sigma.yaml\ntitle: Test Suspicious Login\nid: 12345678-1234-1234-1234-123456789abc\nstatus: test\ndescription: Detects suspicious login attempts\nauthor: security-team\ndate: 2024/01/01\ntags:\n  - authentication\n  - test\nlogsource:\n  product: custom\n  service: webhook\ndetection:\n  selection:\n    eventName: suspicious_login\n    srcIP: 192.168.1.100\n  condition: selection\nlevel: medium\nriskscore: 50\nCreate sample data file:Create file: sample-events.ndjsonImportant: NDJSON format requires one JSON object per line with NO commas between lines and NO array brackets.\n{\"eventName\": \"suspicious_login\", \"user\": \"test-user\", \"srcIP\": \"192.168.1.100\", \"severity\": \"high\", \"timestamp\": \"2024-01-01T12:00:00Z\"}\n{\"eventName\": \"suspicious_login\", \"user\": \"test-user-2\", \"srcIP\": \"192.168.1.100\", \"severity\": \"high\", \"timestamp\": \"2024-01-01T12:01:00Z\"}\n{\"eventName\": \"normal_login\", \"user\": \"admin\", \"srcIP\": \"10.0.0.1\", \"severity\": \"low\", \"timestamp\": \"2024-01-01T12:02:00Z\"}\nTest the Sigma detection:\n# Test against local sample file\nrunreveal detections run --file test-suspicious-login-sigma.yaml --input sample-events.ndjson\n✓ Shows which events in the sample file would trigger the detection\nExpected output for successful detection test:\nline 1 matches detection Test Suspicious Login\nline 2 matches detection Test Suspicious Login\nline 3 does not match detection Test Suspicious Login\nUnderstanding Detection Data Flow\nSQL Detections: Query the raw logs table directly, using JSONExtractString(rawLog, 'field') to extract values\nSigma Detections (CLI testing): Expect normalized/extracted fields as they would appear after processing\nWhen creating sample data for Sigma CLI testing, provide the fields as if they've already been extracted from rawLog.","step-5-trigger-and-review-detection#Step 5: Trigger and Review Detection":"Manual Execution:\nNavigate to Detections\nFind your test-suspicious-login detection\nClick Run Detection to execute it manually\nWait a few seconds for execution to complete which will generate a signal\nCheck signals view:\nSELECT detectionName,\n       severity,\n       recordsReturned,\n       categories,\n       receivedAt\nFROM signals\nWHERE detectionName = 'test-suspicious-login'\nORDER BY receivedAt DESC\nLIMIT 5;","step-6-compare-signal-vs-alert#Step 6: Compare Signal vs Alert":"Now convert your detection to an alert:\nAdd notification channel:\nFirst, add a Notification Channel (email, Slack, etc.) in RunReveal UI\nEdit detection:\nEdit test-suspicious-login detection\nAttach the notification channel to the detection\nSave changes\nSend another test event:\ncurl -X POST https://api.runreveal.com/sources/hook/30pw8Fynw5W7PzjbnRyxnfMsID2 \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"timestamp\": \"'$(date -u +%Y-%m-%dT%H:%M:%SZ)'\",\n    \"source\": \"test-system\", \n    \"eventName\": \"suspicious_login\",\n    \"severity\": \"high\",\n    \"user\": \"test-user-2\",\n    \"srcIP\": \"192.168.1.100\",\n    \"action\": \"login_attempt\",\n    \"result\": \"success\"\n  }'\nWait 5 minutes, then compare the signal results:\n-- View signals for the detection\nSELECT detectionName,\n       severity,\n       recordsReturned,\n       categories,\n       receivedAt,\n       results as raw_results\nFROM signals \nWHERE detectionName = 'test-suspicious-login'\nORDER BY receivedAt DESC\nLIMIT 5;\nTo the alerts results:\n-- View alerts for the detection\nSELECT detectionName,\n       severity,\n       recordsReturned, \n       id,\n       receivedAt,\n       -- Extract user from results (simplified)\n       results as raw_results\nFROM alerts\nWHERE detectionName = 'test-suspicious-login'\nORDER BY receivedAt DESC\nLIMIT 5;\nCompare in RunReveal UI:\nNavigate to the Alerts page in RunReveal UI\nUse the dropdown filter to switch between All, Alerts, and Signals\nCompare the results:\nSignals: Shows detection results without notifications\nAlerts: Shows detection results that triggered notifications\nAll: Shows both signals and alerts together","suggested-workflow#Suggested Workflow":"Start with Signals → Validate detection logic without noise\nTune the rule → Adjust query logic and thresholds\nConvert to Alert → Add notifications when ready for production\nMonitor performance → Track execution times and match rates","next-steps#Next Steps":"Now that you have detections, signals, and alerts set up, explore the detailed configuration guides:\nDetections - Create and manage security detection rules\nSigma Streaming - Use Sigma rules for standardized threat detection\nDetection as Code - Manage detections through code and version control\nSources - Set up data collection from your systems\nPipelines - Configure data processing workflows\nNotifications Getting Started - Set up alerting and notification channels\nAI Chat - Use AI-powered analysis for threat hunting and investigation\nEnrichments - Add context and metadata to your security events"}},"/":{"title":"RunReveal - Modern SIEM Platform for Security Detection & Analysis","data":{"what-is-runreveal#What is RunReveal?":"RunReveal is a data detection platform designed to make detection simple. We've\nrethought the existing SIEM category from the ground up so we can instantly\ntake any company's security data and logs and turn them into insights, provide\nthe tools to build your own detailed detection, integrations, etc.","how-does-runreveal-work#How does RunReveal Work?":"There are quite a few components that make RunReveal tick.\nSources - RunReveal orchestrates and manages the collection of logs\nfrom the security tools and business tools that you use via APIs, Webhooks,\ndirect integrations, and object storage.\nData-storage and log search - We provide tools to store your logs,\nsearch your logs, schedule detections, and easily investigate what's going\non in your environment.\nDestinations - Object storage and managed services for backing up and\nusing your data in other third party services. We collect the data from different\ncloud services and save your data to object storage for a rainy day.\nAlerting - We integrate with the tools you want to send alerts to and\nby default sends all of your alerts to your email.\nReports - Insights we forward to you on a daily basis about what's\nhappening in your environment. These are queries we've written.","the-secret-sauce#The secret sauce":"What's the secret sauce that RunReveal has that no other company's have?\nWe work with companies of any size, whether 1 person or 100,000. All\ncompanies deserve to have the tools they need to detect compromises.\nWe're built on an open-source database so you can own your own data\ninstead of renting it while using the most advanced database technology.\nWe are making every part of our platform customizable, but keeping the\nexperience dead simple. By running on an open-source database, customers\ncan customize log formats, visualization tools, enrichment capabilities,\netc. But out of the box we provide a batteries fully included experience.\nEfficiency. RunReveal is able to provide a faster and more performant\nexperience than any other platform, and this enables us to provide\nsimple transparent pricing that beats the competition.","integrations-with-runreveal#Integrations with RunReveal":"RunReveal provides APIs, a CLI, and direct integrations with the platform.\nDetection as code - We support detection as code workflows via github\nfunctions or API.\nDashboarding and Visualization tools - RunReveal has a direct integration\nwith Grafana, can be used with Jupyter notebooks, and using our API we can\nsupport any specialty integrations you need.\nSOAR - RunReveal supports SOAR platforms to automate the response,\ninvestigation, or whatever parts of the detection & response process."}},"/glossary":{"title":"RunReveal Glossary - Security Terms and Platform Definitions","data":{"":"A glossary of terms used throughout the RunReveal platform, documentation, and terms related to security logs and detections."}},"/integrations/jupyter":{"title":"Jupyter","data":{"":"RunReveal supports Jupyter through an open-source python package we maintain. To install the plugin.\nhttps://github.com/runreveal/runrevealpy","getting-started#Getting started":"To install and get started using jupyter, you'll need to grab an API key from the RunReveal dashboard. Click \"Generate new token\" and take note of your API token and your Workspace ID from the same settings page.\nNext, install the python package.\npip3 install runreveal\nThe python package will render your token from two separate environment variables\nRUNREVEAL_AUTH_TOKEN - The API credential used to authenticate with the API.\nRUNREVEAL_WORKSPACE - The workspace identifier that you're querying the logs from.","your-first-notebook#Your first notebook.":"Within your jupyter notebook, try to import the RunReveal class from the runreveal package you just installed. You should be able to query your logs and see results.\nfrom runreveal import RunReveal\nrr = RunReveal(\"show me my cloudflare audit logs from the past month where srcIP is not empty string\", True).create_dataframe()\nrr.head(5)\nThe method create_dataframe() will return a pandas dataframe containing all of the data returned by the RunReveal instance.\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"}},"/notifications":{"title":"Notification Channels - Alert Management for Security Events","data":{"":"RunReveal provides a comprehensive notification system that allows you to be alerted when your detection queries return results. This section covers all the notification options available to ensure your team stays informed about important security events.\nNew to notifications? Check out our Getting Started guide for step-by-step instructions on setting up your first notification channels and adding them to your detections.","available-notification-channels#Available Notification Channels":"","email-notifications#Email Notifications":"Configure email notifications to receive alerts directly in your inbox. Learn how to set up and customize email notifications for your detection queries.","slack-integration#Slack Integration":"Connect RunReveal to your Slack workspace to receive real-time alerts in your security channels. Includes configuration steps and customization options.","pagerduty-integration#PagerDuty Integration":"Set up PagerDuty integration for critical alerts that require immediate attention from your on-call team.","jira-integration#Jira Integration":"Automatically create Jira issues from your RunReveal detections to streamline your security workflow.","webhooks#Webhooks":"Use webhooks to integrate RunReveal with custom systems or third-party services not covered by our direct integrations.","tines-integration#Tines Integration":"Connect RunReveal to Tines for advanced security automation workflows.","advanced-features#Advanced Features":"","notification-templates#Notification Templates":"Create and customize notification templates to control how your alerts appear across different channels.","history-api#History API":"Access historical notification data programmatically using the Notification History API.","best-practices#Best Practices":"Configure multiple notification channels for critical detections to ensure alerts are not missed\nUse templates to standardize alert formats across your organization\nSet appropriate severity levels to prevent alert fatigue\nRegularly review your notification settings to ensure they align with your security operations"}},"/notifications/email":{"title":"Email","data":{"":"Delivering notifications to email is straightforward to setup. All you need is an email address that you want to deliver notifications to.Set this up in the notifications page of the RunReveal dashboard."}},"/integrations":{"title":"Integrations - Connect RunReveal with Visualization & Analysis Tools","data":{"":"RunReveal integrates with popular data visualization and analysis tools to help you get the most out of your security data. This section covers the available integrations and how to set them up.","available-integrations#Available Integrations":"","grafana-integration#Grafana Integration":"Connect RunReveal to Grafana to create powerful dashboards for visualizing your security data.\nInstalling the RunReveal Grafana plugin\nConfiguring the RunReveal data source\nUsing pre-built dashboards for threat monitoring\nCreating custom dashboards with RunReveal data\nSelf-hosting options for the Grafana plugin","jupyter-notebooks#Jupyter Notebooks":"Use RunReveal with Jupyter Notebooks for advanced data analysis and exploration.\nInstalling the RunReveal Python package\nAuthenticating with your RunReveal account\nCreating your first notebook with RunReveal data\nConverting RunReveal queries to pandas DataFrames\nAdvanced analysis examples","benefits-of-using-integrations#Benefits of Using Integrations":"Unified Visualization: Combine security data with operational metrics in a single dashboard\nAdvanced Analysis: Perform complex data analysis using familiar tools\nCustom Reporting: Create tailored reports for different stakeholders\nOperational Efficiency: Streamline your security operations by integrating with your existing workflow","coming-soon#Coming Soon":"We're constantly working on new integrations to enhance your RunReveal experience. If you have suggestions for integrations you'd like to see, please contact our support team.For any issues with existing integrations, check our GitHub repositories for the latest updates and troubleshooting information."}},"/notifications/getting-started":{"title":"Getting Started with Notifications","data":{"":"Overview: This guide walks you through setting up notifications in RunReveal, from creating notification channels to testing them and adding them to your detection rules so that you are notified when alerts are triggered.","what-youll-learn#What You'll Learn":"In this guide, you'll learn how to:\nCreate notification channels in the UI\nTest your notifications\nView alert history\nAdd notifications to detection rules:\nManaged Detections - Out-of-the-box detections with default detection rules enabled. These are read-only and must be cloned first to add notifications.\nCustom Detections - Detections that you create or cloned detections.\nSigma Streaming Detections - Add notification channels to your sigma streaming in YAML syntax.\nDetection as Code - Notifications for detection as code rules are not managed through the UI. See Detection as Code for information on managing notifications in your detection code.","prerequisites#Prerequisites":"Before you begin, make sure you have:\nAccess to the RunReveal dashboard\nAdmin permissions (for creating notification channels)\nThe necessary credentials for your chosen notification platform (Slack, PagerDuty, etc.)","steps#Steps:":"","1-create-a-notification-channel#1. Create a Notification Channel":"Navigate to Notification Channels\nGo to the RunReveal dashboard\nClick on Notification Channels in the left sidebar\nClick Create Notification Channel\nChoose Your Notification Type\nEmail: Simple email delivery\nSlack: Send to Slack channels\nPagerDuty: Incident management integration\nJira: Issue tracking integration\nWebhooks: Custom HTTP endpoints\nTines: Security automation platform\nConfigure the Notification Type\nGive your channel a descriptive name (e.g., \"DevOps Security Alerts - Slack\")\nInput the required information (webhook URL, email address, integration keys, etc.)\nSave the Notification Channel\nClick Create Notification to save your notification channel\nThe channel will appear in your notifications list","2-test-your-notification-channel#2. Test Your Notification Channel":"Send Test Notification\nClick Send Test to trigger a test notification\nThis will send a sample alert to your notification channel to verify your configuration\nVerify Delivery\nCheck your notification platform to confirm receipt of test notification","3-add-notifications-to-detections#3. Add Notifications to Detections":"When detection rules trigger alerts in RunReveal, they automatically send notifications to all configured notification channels associated with that detection. Each detection type has a different method for adding notification channels:","undefined#Managed Detections":"Managed detections are read-only. You must clone them before adding notifications. This applies to both managed SQL detections and managed sigma detections.\nClone the Detection\nGo to Detections → Detection Queries\nFind the managed detection you want to customize\nClick Clone & Edit to create a copy of the detection\nAdd Notification Channels\nIn the cloned detection, scroll to Notification Channels section\nClick to view a dropdown of available channels\nSelect your newly created notification channel\nSave and Deploy\nClick Save Detection to store your changes\nThe detection will now send notifications to your channel when alerts are triggered\nDeactivate Original Detection\nToggle the \"Active\" switch to off on the original managed detection to avoid duplicate alerts","undefined#Custom SQL Detections":"Create or Edit Custom Detection\nGo to Detections → choose an existing Custom Detection or click Create new Detection\nIf creating from scratch, input the detection query, description, interval schedule, and detection metadata\nConfigure Notifications\nIn the detection editor, find the Notifications section\nClick Add Notification Channel\nSelect from your available notification channels\nCustom detections can always be edited to add/remove notification channels\nSet Notification Parameters\nChoose when to send notifications:\nOn every alert: Send for each detection match\nOn severity change: Only when severity increases\nDaily digest: Summarize alerts once per day\nConfigure custom message templates if needed\nTest and Deploy\nTest your detection to ensure notifications work\nDeploy the detection to start monitoring","undefined#Sigma Streaming Detections":"Sigma detections use the notificationnames field in the YAML configuration to specify which notification channels should receive alerts.\nCreate or Edit Sigma Detection\nGo to Detections → Sigma Streaming\nCreate a new sigma detection or edit an existing one\nOpen your sigma detection YAML file\nAdd Notification Channels to YAML\nAdd the notificationnames field to your sigma detection\nList the exact names of your notification channels as an array\nEnsure the channel names match exactly (case-sensitive)\nExample Sigma Detection with Notifications\ntitle: Detect Okta Authentication Failures\ndescription: Detects failed Okta sign-in attempts\nlevel: high\nlogsource:\n  product: okta\ndetection:\n  filter:\n    eventType: user.session.start\n    outcome.result: FAILURE\n  condition: filter\n# Add notification channels here\nnotificationnames:\n  - \"Security Alerts - Slack\"\n  - \"Critical Alerts - Email\"\nUpload and Deploy\nSave your sigma detection YAML file\nUpload the file to RunReveal via the Sigma Streaming interface\nThe detection will now send notifications to the specified channels when triggered\nVerify Configuration\nCheck that your notification channel names are spelled correctly\nTest the sigma detection to ensure notifications are sent\nReview the alert history to confirm delivery\nThe notificationnames field in sigma detections must exactly match the names of your notification channels (including case sensitivity). If the channel name doesn't match, no notifications will be sent.","4-view-alert-history#4. View Alert History":"Note: Alert history shows notifications from real detection alerts, not test notifications sent from the notification channel test function.\nAccess History\nIn the Notification Channels page in the UI, click Alert History\nOr utilize the History API for programmatic access\nReview Notifications\nView all sent notifications with timestamps\nSee delivery status (success/failed)\nCheck notification content and recipients\nTroubleshoot Issues\nFailed notifications will show error details\nUse this information to fix configuration issues","next-steps#Next Steps":"Now that you have notifications set up, explore the detailed configuration guides:\nEmail Notifications - Simple email delivery setup\nSlack Integration - Send alerts to Slack channels\nPagerDuty Integration - Incident management integration\nJira Integration - Issue tracking integration\nWebhooks - Custom HTTP endpoint integration\nTines Integration - Security automation platform\nNotification Templates - Custom message formatting\nHistory API - Programmatic access to notification history","troubleshooting#Troubleshooting":"","notifications-not-sending#Notifications Not Sending":"Check channel configuration\nTest Notification Channel\nVerify platform credentials\nReview alert history for error messages","duplicate-notifications#Duplicate Notifications":"Check rule notification settings\nEnsure no overlapping rules\nReview notification frequency settings","sigma-rule-notifications-not-working#Sigma Rule Notifications Not Working":"Verify notificationnames field spelling matches channel names exactly\nCheck that notification channels are active and tested\nEnsure sigma rule is enabled and properly configured\nReview sigma rule logs for any processing errors"}},"/notifications/history-api":{"title":"Using The API","data":{"":"Authentication with the API is done using API tokens which you can generate in the UI.  These tokens are scoped to specficic\nroles which are authorized to perform different sets of actions.  For working with enrichments, you will want to create a token\nwith either the admin role (which has full read/write/delete access) or the analyst role (which has read/edit permissions).Once you have generated a token, you can use it by setting the Authorization header:Authorization: Basic <TOKEN>","making-requests#Making Requests":"Requests can be made by appending the desired endpoint to the api base url (https://api.runreveal.com) and setting the appropriate\nAuthorization header.  For POST endpoints, you should also set the appropriate Content-type header, application/json for all\nenrichment endpoints.\nTip: All endpoints require workspace id as a query parameter, you can find your workspace id on the workspace settings page\nExamples:\nListing notification history for your workspace:\n$ TOKEN=\"<your token>\"\n$ WKSPCID=\"<your workspace id>\"\n$ curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/notification-history/list?workspaceid=${WKSPCID}\"\nimport requests\nimport json\nTOKEN = \"your-token\"\nWORKSPACE_ID = \"your-workspace-id\"\nURL = \"https://api.runreveal.com/notification-history/list\"\nheaders = {\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Basic \" + TOKEN,\n}\nresponse = requests.get(\n    URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    headers=headers,\n)\nprint(response.json())\nListing notification history for a specific alert:\n$ TOKEN=\"<your token>\"\n$ WKSPCID=\"<your workspace id>\"\n$ ALERTID=\"<your alert id>\"\n$ curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/notification-history/list/${ALERTID}?workspaceid=${WKSPCID}\"\nimport requests\nimport json\nTOKEN = \"your-token\"\nWORKSPACE_ID = \"your-workspace-id\"\nALERT_ID = \"your-alert-id\"\nURL = f\"https://api.runreveal.com/notification-history/list/{ALERT_ID}\"\nheaders = {\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Basic \" + TOKEN,\n}\nresponse = requests.get(\n    URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    headers=headers,\n)\nprint(response.json())\nGoing from notification history entry to alert details\n$ TOKEN=\"<your token>\"\n$ WKSPCID=\"<your workspace id>\"\n$ RESULT=$(curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/notification-history/list?workspaceid=${WKSPCID}\")\n$ ALERTID=$(echo $RESULT | jq -r '.result.[0].alertID')\n$ curl -H \"Authorization: Basic ${TOKEN}\" -H \"Content-type: application/json\" \"https://api.runreveal.com/logs/query/v3?workspaceid=${WKSPCID}\" --data-raw \"{\\\"query\\\": \\\"SELECT * FROM detections WHERE scheduledRunID == '${ALERTID}'\\\"}\"\nimport requests\nimport json\nTOKEN = \"YOUR TOKEN\"\nWORKSPACE_ID = \"YOUR WORKSPACE ID\"\nURL = \"https://api.runreveal.com/notification-history/list\"\nLOGSQUERY_URL = \"https://api.runreveal.com/logs/query/v3\"\nheaders = {\n    \"Content-type\": \"application/json\",\n    \"Authorization\": \"Basic \" + TOKEN,\n}\nresponse = requests.get(\n    URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    headers=headers,\n)\nalert_id = response.json()[\"result\"][0][\"alertID\"]\nquery_json = {\n    \"query\": f\"SELECT * FROM detections WHERE scheduledRunID='{alert_id}'\",\n}\ndetection_info = requests.post(\n    LOGSQUERY_URL,\n    params={\"workspaceid\": WORKSPACE_ID},\n    headers=headers,\n    data=json.dumps(query_json),\n)\nprint(detection_info.json())","resources#Resources":"","notification-history-entry#Notification History Entry":"Parameters:\nName\tType\tDescription\tid\tstring\tAutogenerated ID\tworkspaceID\tstring\tYour workspace ID\talertID\tstring\tID of alert that fired this notification\tdetectionDisplayName\tstring\tName of the detection that fired this notification\tchannel\tstring\tNotification channel, e.g. Jira, Linear, Slack, etc\tstatus\tstring\tStatus of notification (\"sent\", \"successful\", \"error\", or \"retrying\")\tmeta\tmap<string, string>\tMetadata about a notification, including created linear or jira issues\tcreatedAt\ttimestamp\tWhen the notification was created\t\nExamples:\n{\n    \"id\": \"2vuLtZJeKD5rzNUfONbZRo8ysDV\",\n    \"workspaceID\": \"2vuJirrWU4zlZTxs4pfYPbmsLOI\",\n    \"alertID\": \"2vuLstBUci790Qimp7tdqEIzNuF\",\n    \"detectionDisplayName\": \"My Detection\",\n    \"channel\": \"linear\",\n    \"status\": \"success\",\n    \"meta\": {\n        \"linearIssue\": \"NOT-41\",\n        \"linearLink\": \"https://linear.app/notification-testing/issue/NOT-41/runreveal-detection-alert\"\n    },\n    \"createdAt\": \"2025-04-18T15:30:27.529124Z\"\n}\n{\n    \"id\": \"2vuLtQhMcGMTG7zqr99icIINmzf\",\n    \"workspaceID\": \"2vuJirrWU4zlZTxs4pfYPbmsLOI\",\n    \"alertID\": \"2vuLstBUci790Qimp7tdqEIzNuF\",\n    \"detectionDisplayName\": \"My Detection\",\n    \"channel\": \"jira\",\n    \"status\": \"success\",\n    \"meta\": {\n        \"jiraIssue\": \"SMP-66\",\n        \"jiraLink\": \"https://company.atlassian.net/browse/SMP-66\"\n    },\n    \"createdAt\": \"2025-04-18T15:30:26.320842Z\"\n}","endpoints#Endpoints":"","get-notification-historylist#GET /notification-history/list":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\tlimit\tMaximum results to return\tNo, default 100\t\nResponse:\n{\n    \"success\": bool,\n    \"result\": List<Notification History Entry>\n}","get-notification-historylistalertid#GET /notification-history/list/{alertID}":"Query Parameters:\nName\tDescription\tRequired\tworkspaceid\tYour workspace ID\tYes\t\nResponse:\n{\n    \"success\": bool,\n    \"result\": List<Notification History Entry>\n}"}},"/notifications/jira":{"title":"Jira","data":{"":"Setting up a Jira notification channel requires a few pieces of information.","setup-settings#Setup Settings":"","jira-url#Jira URL":"The Jira URL is the base URL that your atlassian organization is located. For us we would enter https://runreveal.atlassian.net","jira-project-key#Jira Project Key":"The key for the project that you want new issues added. This key was set when creating your project. It should be the prefix assigned to issues.","jira-issue-type#Jira Issue Type":"The name of the issue type that you want created when sending alerts to Jira.","labels#Labels":"A comma separated list of labels that you want to add to the issues. This field is run through the template renderer so can include properties from the data being passed.","email-address#Email Address":"Enter the email address of the user that the issues will be created under. This user will need permission to create issues in that project with that issue type.","api-token#API Token":"Navigate to the API token creation page, log in with the same user that you entered in the email address field. Click on the Create API token button to generate a new token."}},"/notifications/pagerduty":{"title":"PagerDuty","data":{"":"The PagerDuty notification channel allows you to create a PagerDuty incident when a detection is triggered.","setup#Setup":"","create-integration#Create Integration":"In your PagerDuty account, open the service that you want new incidents to be created under and create a new integration.Select the Events API V2 integration and click add.Once the integration is created you should be presented with an integration key. Copy this key as it is needed when creating the notification channel in RunReveal.","runreveal-setup#RunReveal Setup":"In your RunReveal workspace, create a new PagerDuty notification channel.Give your notification channel a name, and copy the integration key that was generated in your PagerDuty account.The RunReveal notification allows you to override the title and severity of the incident that is created.\nBy default the title and severity will be the name and severity of the detection that is triggered.\nThe title field can be updated using a template format, e.g. {{query.schedule.name}}.You can also add custom details to your incident to provide additional context for your alert. Both the key and value fields are run through the template engine when the alert is created.\nE.g. to add the risk score as an additional detail you would set the key to risk-score and the value to {{query.schedule.riskScore}}.Once added you can then add this notification channel to your detections using the notification slug."}},"/notifications/slack":{"title":"Slack","data":{"":"To get started setting up your alerts you'll need to create a slack webhook app, and provide us with the webhook url to start receiving your alerts. Don't worry, this will only take a minute or two.","create-slack-application#Create Slack Application":"Follow the steps on slack's website to create a new webhook application while logged into your slack account.\nClick https://api.slack.com/apps/new\nClick \"Create New App\" and \"From Scratch\"\nFill in a name like \"RunReveal\" and select the desired slack workspace\nUnder \"Add features and functionality\" select \"Incoming Webhooks\".\nIf necessary, select \"Activate Incoming Webhooks\"\nClick \"Add Webhook to workspaces\" and select the channel you want.\nCopy the webhook!","provide-us-with-your-webhook-url#Provide us with your webhook URL":"Using the CLI or the web UI, provide us with your slack webhook URL."}},"/notifications/templates":{"title":"Templates","data":{"":"Templates allow custom text to be sent to a notification channel. RunReveal templates are rendered using handlebar expressions. Handlebar expressions allow properties from the query or trigger to be inserted into the notification text dynamically.\nFor a deeper understanding of handlebars and a more detailed explanation of the syntax check out their docs.","template-ui#Template UI":"To access templates navigate to the notifications page and scroll to the bottom. From there you will see a list of templates that are saved with your account. There should be one default template for each notification channel. You can add/edit a template or duplicate an existing one to use as a starting point.","editing-templates#Editing Templates":"The edit screen is split into two sections. On the right you have the fields that are needed to create a template. A template name is used to identify the template and must be unique from all of your templates. The other two fields, Message Title and Message Body, are the actual template fields that will be rendered when sending your notification. You can preview the rendered text using the test data loaded on the left panel.The left panel is used to give you access to the fields that can be added to your template. The drop down shows all saved queries and triggers associated with your account. Selecting one and clicking load will either try to load the data from most recent execution or fill in with test data if needed. The tree view shows the properties that can be added into your template. Clicking on a property will add the path to that field to your clipboard and can be pasted into a handlebar expression. RunReveal offers partial templates that can be used to display certain aspects of your data in an easy to use format. For example we offer the ability to render the results of the scheduled query as an HTML table. The second section of data shows the loaded example data that will be replaced in your template. Scheduled query data will be under the query object and trigger data will be under the trigger object. Templates are used for scheduled queries and triggers, this means that the template will be reused and must be written in a way to handle both cases. The default templates give an example of how this is done. The data passed to the template will either have a query object with the results from the scheduled query or a trigger object with the results of the trigger that fired. The other object will be blank when rendering the template. This can be used to create conditional sections that display data depending on what is being notified.\n{{#if query}}\nThis is a query\nResults: {{query.queryResponse.resultCount}}\n{{/if}}\nThis example template will only render the text inside the if block, if the template is being rendered from a scheduled query execution. Otherwise this block will not render.","syntax#Syntax":"Handlebar expressions must be surrounded with {{ and }} any text between these are processed and replaced. expressions starting with a # are blocks and must have a matching / expression.  A partial is an expression that starts with a > and is a template that is injected into the current template. RunReveal offers a few built in partials to help display certain aspects of the data.","default-templates#Default Templates":"RunReveal provides a default template for each notification type. These templates provide basic information about the data being notified. They can be duplicated and used as a starting point for your own template. Look at the docs for each notification channel for more information about its default template and the data that is sent to it."}},"/integrations/grafana":{"title":"Grafana","data":{"":"RunReveal supports a grafana plugin in both Grafana Cloud, and self-hosted grafana deployments. The code supporting our grafana plugin is hosted here:\nhttps://github.com/runreveal/runreveal-datasource\nThe grafana plugin allows anyone to create powerful dashboards in seconds and centrally host them with operational and other dashboards.","1-installing-the-plugin#1. Installing the plugin":"Go to the RunReveal Grafana Plugin and install it to your grafana instance by clicking \"Install plugin\" under the Installation tab.","2-add-runreveal-datasource#2. Add RunReveal Datasource":"We still need to configure the RunReveal to authenticate with our API. Under Home > Connections > Data sources, search for RunReveal.\nNext you'll need an API token.\nGenerate in the RunReveal UI. You can generate an API token under the the \"Settings\" panel. Click \"Generate API Token\" next to the API Tokens.\nPaste the generated token into \"Session Token\" input field, along with the workspace ID from the settings page into the Workspace ID field, and press \"Save & Test\".","3-live-threat-monitoring-dashboards#3. Live threat monitoring dashboards":"From the data sources screen, select the \"Dashboards\" tab. On the Dashboards tab, click \"Import\"\nYou'll then see \"RunReveal Default Dashboard\" within your dashboards\nThis dashboard is maintained by us to help you quickly get up and running. If you'd like to see a revision history or see the detailed JSON specification of the dashboard that was just installed, this link has more information.","4-custom-dashboards#4. Custom dashboards":"Within a grafana dashboard you can make a new panel and select the runreveal_source datasource. Once you do that, you can make dashboards and panels using the same interface that the runreveal logs search interface works with.\nRemember, Grafana works really well with timeseries data, and the RunReveal search interface supports the macros $__fromTime $__toTime and $__timeInterval \n\t\t$__fromTime\tReplaced by the starting time of the range of the panel casted to DateTime\t$__toTime\tReplaced by the ending time of the range of the panel casted to DateTime\t$__timeInterval(columnName)\tReplaced by a function calculating the interval based on window size, useful when grouping","self-hosting#Self hosting":"The RunReveal plugin can be installed on self-hosted grafana as well. Clone the repo and follow the directions here to set up the RunReveal grafana plugin on self-hosted grafana.\nhttps://grafana.com/docs/grafana/latest/administration/plugin-management/#install-plugin-on-local-grafana"}},"/notifications/tines":{"title":"Tines","data":{"":"We support Tines as a destination so our customers can trigger\ndifferent SOAR workflows.Create a Tines webhook to receive a notification event from RunReveal. Note the webhook URL and the secret.\nProvide the RunReveal Tines notification with the Webhook URL, followed by the secret in the following format. You can read the Tines documentation about this here\nhttps://tenant.tines.com/webhook/<path>/<secret>\nIn RunReveal, click \"Test Notifcation\" to be sure that in Tines the secret was not rejected."}},"/notifications/webhooks":{"title":"Webhooks","data":{"":"RunReveal will send HTTP POST request to a URL of your choosing along with the data that is associated with the detection.Set this up using the Notification page in the RunReveal dashboard.","schema#Schema":"The following shows the schema of the data that is sent in the webhook.\nProperty\tData Type\tDescription\ttitle\tstring\tThe template title that was rendered.\tmessage\tstring\tThe template body that was rendered.\tquery\tscheduled query response\tThe field will be null if the notification was created from a trigger. Otherwise it will contain the data of the scheduled query.","webhook-signing#Webhook Signing":"Webhooks have a webhook secret that we use to sign the request body, along with a timestamp header we pass along with the notification.The webhook integrity hash is calculated by concatenating the timestamp header and the HTTP request post body together, and calculating a SHAHMAC256 using the secret key\nwe provide to you. Make sure when calculating the hash that the hex string is treated as a byte array.\nThe HTTP Headers that we send along with all Webhook Notifications are described as:\nRunReveal-Timestamp -- The timestamp in unix seconds that the webhook was generated.\nRunReveal-Integrity -- The integrity hash that we calculated using our shared key.\nHere is an example function that we've used, in Golang, to successfully verify the webhooks.\nfunc SHAHMAC256(key, data []byte) (string, error) {\n\tmac := hmac.New(sha256.New, key)\n\t_, err := mac.Write(data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\thashBytes := mac.Sum(nil)\n\tbase64Hash := base64.StdEncoding.EncodeToString(hashBytes)\n\treturn base64Hash, nil\n}\nfunc TestVerifySha256(t *testing.T) {\n\tvar integrity = \"3w7SmNuf5UHk9v0WV6IIACBx/dZazysTcU8fxowNQVI=\"\n\tvar bodyPlusTime = `1720198139{\"link\":\"https://app.runreveal.com/dash/detections/\",\"message\":\"Yay! Your notification channel was tested successfully.\",\"rawEvent\":null,\"test\":{\"status\":true},\"title\":\"RunReveal Notification Test\"}`\n\tvar key = \"\"\n\tkeyBytes, err := hex.DecodeString(key)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tbase64Integrity, err := base64.StdEncoding.DecodeString(integrity)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tresultHash, err := SHAHMAC256(keyBytes, []byte(bodyPlusTime))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tbase64ResultHash, err := base64.StdEncoding.DecodeString(resultHash)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\titWorked := hmac.Equal(base64Integrity, base64ResultHash)\n\tif !itWorked {\n\t\tt.Fatal(\"Integrity check failed\")\n\t}\n}\nNote, these signatures are calculated even for test notifications, so you can develop the code needed\nto verify webhook signatures in a straightforward way."}},"/pipelines/getting-started":{"title":"Getting Started","data":{"":"With pipelines, you can have more fine-grained control over the processing of your log events.  Topics allow you to\nselect a subset of your events and send them to a custom processing pipeline.  By mixing and rearranging\npipeline steps, you can control how your event is transformed before being sent to it's destination.","topics#Topics":"Topics apply a filter to select a subset of your events.  Any matching event is sent to the pipeline which is\nattached to your topic.","managing-topics#Managing Topics":"To manage your topics, you can head over to the Pipelines Page.\nWhen events come in from a source, they are evaluated against each topic in the topics list from top to bottom.  If\nthe event doesn't match any of your custom topics, then it is sent to the default runreveal pipeline.  You can\nrearrange the order they are evaluated in by dragging them up or down the list.","creating-a-topic#Creating a Topic":"Clicking on the Create Topic button will bring you to the topic creation wizard which will walk you through setting\nup your new topic.\nEach topic needs a name, and a precondition.  The precondition is how you select which subset of events will be\nrouted to your topic.  Use the Precondition Type drop-down to set up your selection criteria.\nFor example, the precondition above would match any events coming from webhook sources.Next you configure where your matching events will go:\nUse an existing pipeline: This option allows you to reuse your pipelines across multiple topics.  For example,\nif you have multiple sources that you want to have a shared pipeline, you could make a topic for each source you want to\nmatch and just reuse the same pipeline with this option.\nCreate a new pipeline: Start with a fresh pipeline, and build it up from scratch in the next step.\nCopy from an existing pipeline: This option allows you to build a new pipeline based on an existing one.  For example,\nif you want to add processing on top of the default processing, you could use this option to copy the default pipeline and\nmodify it in the next step.\nNext you are brought to the pipeline editor, where you can configure how your events are processed.  For specifics, see the\nPipelines section.  Clicking complete will finish the wizard and set up your new resources.","pipelines#Pipelines":"Pipelines detail how an event is processed before being sent to it's final destination.  Pipelines are made up of steps\nwhich are evaluated in order from top to bottom.  Each step consists of a function to apply as well as a precondition to\nselect which events the step is applied to.Pipeline step types:\nTransform:  Apply a transformation to matching events, see the\ntransform documentation for more details.\nEnrich: Apply an enrichment to matching events, see the enrichment documentation\nfor more details.\nFilter: Apply source filters to matching events, see the filtering documentation\nfor more details.\nDetect: Apply a streaming detection to matching events, see the streaming detection documentation\nfor more details.\nDrop: Drops matching events, preventing them from being output to your destinations.\nFrom the routing tab, you can create new pipelines as well as view/edit existing ones.","creating-a-pipeline#Creating a pipeline":"Clicking Add Pipeline or editing an existing pipeline will bring you to the pipeline editor.\nThe left column is your pipeline, and the right column shows you which steps are available to be added to your pipeline.\nYou can add a step by dragging from the right column into the left one.  You can also drag individual steps in your\npipeline up or down to change the order they are evaluated in.\nNote: When editing a pipeline that is shared between multiple topics, the UI will prompt you to first unlock your pipeline.\nChanges made to a shared pipeline will effect all matching topics, do so with caution."}},"/reference":{"title":"Reference Documentation - APIs, CLI, and Technical Resources","data":{"":"Welcome to the RunReveal Reference section. This area contains technical reference documentation for developers and advanced users working with RunReveal's APIs, CLI, and other technical components.","api-reference#API Reference":"","logs-api#Logs API":"Comprehensive documentation for the RunReveal Logs API, which allows you to programmatically access and query your log data.\nAuthentication and authorization\nEndpoint details and query parameters\nResponse formats and pagination\nError handling and rate limits\nCode examples in multiple languages","tools-and-utilities#Tools and Utilities":"","using-the-cli#Using the CLI":"Learn how to use the RunReveal Command Line Interface (CLI) for automation and integration with your existing workflows.\nInstallation and configuration\nAvailable commands and options\nAuthentication setup\nQuery execution and data export\nScripting and automation examples","access-control#Access Control":"","role-based-access-control#Role-Based Access Control":"Detailed information about RunReveal's role-based access control system.\nAvailable roles and permissions\nManaging user access\nCustom role creation\nBest practices for secure access management\nAudit logging and compliance","ai-powered-features#AI-Powered Features":"For documentation on RunReveal's AI-powered features, including Native AI Chat and Model Context Protocol (MCP) integration, please visit the AI Chat (Native & MCP) section.","need-more-information#Need More Information?":"If you can't find what you're looking for in our reference documentation, please contact our support team through:\nOur Discord community\nEmail at contact@runreveal.com\nThe chat bubble on our website"}},"/reference/role-based-access-control":{"title":"Role-Based Access Control - User Permissions and Security","data":{"":"RunReveal supports Role Based Access Control for subjects accessing resources in your workspace.We have configured a set of default roles to simplify the management of permissions assignment to users through those roles.Each resource has 3 permissions defined: Read, Edit and Delete.  Read allows read-only access to the class of resources including listing the instances of that resource.  Edit allows for creation and updating that resources attributes and metadata.  Deletion is required to remove a resource record from the database.The following roles have been defined for workspaces: admin, analyst, operator and cibot.  Below you will see their permissions enumerated.\nPermission\tDescription\tAdmin\tAnalyst\tOperator\tCIBot\tRead Workspaces\tRead and List Source Configs\t✅\t✅\t✅\t\tEdit Workspaces\tCreate and Update Workspace configuration\t✅\t\t\t\tDelete Workspaces\tDelete Workspaces\t✅\t\t\t\tRead Sources\tRead and List Source Configs\t✅\t✅\t✅\t\tEdit Sources\tCreate and Update Source Configs\t✅\t\t\t\tDelete Sources\tDelete Source Configs\t✅\t\t\t\tRead Destinations\tRead and List Destination Configs\t✅\t\t✅\t\tEdit Destinations\tCreate and Update Destination Configs\t✅\t\t\t\tDelete Destinations\tDelete Destination Configs\t✅\t\t\t\tRead Queries\tRead and List Named Queries and Detections\t✅\t✅\t✅\t✅\tEdit Queries\tCreate and Edit Named Queries and Detections\t✅\t✅\t✅\t✅\tDelete Queries\tDelete Named Queries and Detections\t✅\t\t✅\t✅\tRead Reports\tRead and List Reports\t✅\t✅\t✅\t\tEdit Reports\tCreate and Update Report Configs\t✅\t\t✅\t\tDelete Reports\tDelete Report Configs\t✅\t\t\t\tRead Analytics\tRead and List Analytics Views\t✅\t✅\t✅\t\tEdit Analytics\tCreate and Update Analytics Configs\t✅\t\t✅\t\tDelete Analytics\tDelete Analytics Configs\t✅\t\t\t\tRead Notifications\tRead and List Notification Channels\t✅\t✅\t✅\t\tEdit Notifications\tCreate and Update Notification Channels\t✅\t\t\t\tDelete Notifications\tDelete Notification Channels\t✅\t\t\t\tRead Topics\tRead and List Topics for Pipelines\t✅\t✅\t✅\t\tEdit Topics\tCreate, Edit and Delete Pipelines Topics\t✅\t\t✅"}},"/reference/logs-api":{"title":"Logs API - Programmatic Access to RunReveal Data","data":{"authentication#Authentication":"Authentication is done via basic auth with your API token.  Get your token by clicking \"New API Token\" on the page: https://app.runreveal.com/dash/workspace/api-tokens.e.g. with cURL, set the authorization header like so:$ curl -H 'Authorization: Basic <token>' https://api.runreveal.com/...","logs-query-endpoint#Logs Query Endpoint":"https://api.runreveal.com/logs/query/v2\nThe first endpoint is the query endpoint where you can submit queries of any format. Currently the supported formats are: pql, sql, or natural language. Set the useAI and usePQL flags to query using natural language and PQL, respectively. The arguments are mutually exclusive, and setting both is invalid but if useAI is set, it will take precedence. The query will be converted into the corresponding SQL before being submitted to the database backend.This endpoint will kick off the query and return up to limit results. The query will continue running in the background.Here's a full example using cURL:\ncurl 'https://api.runreveal.com/logs/query/v2?workspaceid=<wkspID>' \\\n  -H 'authorization: Basic <token>' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"query\":\"SELECT receivedAt, workspaceID, sourceType, id, eventName FROM runreveal.logs LIMIT 2;\",\"useAI\":false,\"usePQL\":false,\"source\":\"ui-query\",\"parameters\":{\"a\":\"b\"},\"limit\":1000}'\n{\n  \"success\": true,\n  \"result\": {\n    \"queryID\": \"2e2GQkLQiZNUyDIZXnvGHssbRIH\",\n    \"canceled\": false,\n    \"query\": \"SELECT receivedAt, workspaceID, sourceType, id, eventName FROM runreveal.logs LIMIT 2;\",\n    \"aiText\": \"\",\n    \"pqlText\": \"\",\n    \"parameters\": {\n      \"a\": \"b\"\n    },\n    \"error\": \"\",\n    \"resultCount\": 2,\n    \"totalResults\": 2,\n    \"startedAt\": \"2024-03-22T08:14:05.958701794Z\",\n    \"runTime\": \"38.836527ms\",\n    \"runTimeMs\": 38,\n    \"limit\": 0,\n    \"offset\": 0,\n    \"columns\": [\n      \"receivedAt\",\n      \"workspaceID\",\n      \"sourceType\",\n      \"id\",\n      \"eventName\"\n    ],\n    \"columnTypes\": [\n      \"DateTime\",\n      \"String\",\n      \"LowCardinality(String)\",\n      \"String\",\n      \"String\"\n    ],\n    \"rows\": [\n      [\n        \"2024-02-08T23:18:45Z\",\n        \"<wkspID>\",\n        \"gcplogs\",\n        \"2c6a8tdYFRWa7oqVudOrGTIP3B7\",\n        \"google.identity.oauth2.GetTokenInfo\"\n      ],\n      [\n        \"2024-02-08T23:18:55Z\",\n        \"<wkspID>\",\n        \"gcplogs\",\n        \"2c6aA74bB5VCJVesecPRD9cv2ul\",\n        \"google.identity.oauth2.GetTokenInfo\"\n      ]\n    ]\n  }\n}","results-endpoint#Results Endpoint":"If the query has completed, you can query it using the results endpoint. You can pass in a limit and offset parameter to page through the results. The queryID is used to load the results, and is returned upon querying using the above endpoint.If the query has not yet completed, totalResults will be -1.\ncurl 'https://api.runreveal.com/logs/query/v2/results?workspaceid=<wkspID>' \\\n  -H 'authorization: Basic <token>' \\\n  -H 'content-type: application/json' \\\n  --data-raw '{\"queryID\":\"2e2GQkLQiZNUyDIZXnvGHssbRIH\",\"limit\":50,\"offset\":0}'\n{\n  \"success\": true,\n  \"result\": {\n    \"queryID\": \"2e2GQkLQiZNUyDIZXnvGHssbRIH\",\n    \"canceled\": false,\n    \"query\": \"SELECT receivedAt, workspaceID, sourceType, id, eventName FROM runreveal.logs LIMIT 2;\",\n    \"aiText\": \"\",\n    \"pqlText\": \"\",\n    \"parameters\": {\n      \"a\": \"b\"\n    },\n    \"error\": \"\",\n    \"resultCount\": 2,\n    \"totalResults\": 2,\n    \"startedAt\": \"2024-03-22T08:14:05.958702Z\",\n    \"runTime\": \"38.836527ms\",\n    \"runTimeMs\": 38,\n    \"limit\": 50,\n    \"offset\": 0,\n    \"columns\": [\n      \"receivedAt\",\n      \"workspaceID\",\n      \"sourceType\",\n      \"id\",\n      \"eventName\"\n    ],\n    \"columnTypes\": [\n      \"DateTime\",\n      \"String\",\n      \"LowCardinality(String)\",\n      \"String\",\n      \"String\"\n    ],\n    \"rows\": [\n      [\n        \"2024-02-08T23:18:45Z\",\n        \"2KUOdhvRReF5RZfQX8ILneT4fSd\",\n        \"gcplogs\",\n        \"2c6a8tdYFRWa7oqVudOrGTIP3B7\",\n        \"google.identity.oauth2.GetTokenInfo\"\n      ],\n      [\n        \"2024-02-08T23:18:55Z\",\n        \"2KUOdhvRReF5RZfQX8ILneT4fSd\",\n        \"gcplogs\",\n        \"2c6aA74bB5VCJVesecPRD9cv2ul\",\n        \"google.identity.oauth2.GetTokenInfo\"\n      ]\n    ]\n  }\n}","cancellation-endpoint#Cancellation Endpoint":"This cancels an inflight query given by the queryID.\ncurl 'https://api.runreveal.com/logs/query/v2/cancel?workspaceid=<wkspID>&queryID=<queryID>' \\\n  -H 'authorization: Basic <token>'\n{\n  \"success\": true\n}"}},"/release-notes":{"title":"Release Notes - RunReveal Platform Updates and New Features","data":{"":"Welcome to the RunReveal Release Notes section. Here you'll find detailed information about all the new features, improvements, and bug fixes introduced with each release of the RunReveal platform.","recent-highlights#Recent Highlights":"","v2025429#v2025.4.29":"New Event Routing UI: Create and manage pipelines with ease using our new intuitive interface\nImproved Raw Data Rendering: Enhanced readability of raw results on the explore page","v2025428#v2025.4.28":"Advanced Detection Templates: New detection templates with preconfigured queries for common security scenarios\nEnhanced Query Builder: Improvements to the SQL query builder with better autocomplete functionality","v2025427#v2025.4.27":"Enrichment API Enhancements: Expanded capabilities for the Enrichment API\nNew Source Integrations: Additional integrations with popular security and business tools","versioning#Versioning":"RunReveal uses a versioning scheme in the format vYYYY.MM.PP where:\nYYYY represents the year\nMM represents the month\nPP represents the patch count within the month.","historical-release-notes#Historical Release Notes":"For a comprehensive list of all changes made throughout 2024, you can view our 2024 Release Notes.","stay-updated#Stay Updated":"To receive notifications about new releases and upcoming features:\nJoin our Discord community\nFollow us on Twitter\nSubscribe to our newsletter via the RunReveal dashboard"}},"/reference/using-the-cli":{"title":"RunReveal CLI - Command Line Interface for Automation","data":{"install-the-cli#Install the CLI":"Install homebrew for macOS, then enable our homebrew tap and install the CLI:\nbrew tap runreveal/runreveal\nbrew install runreveal","login-to-runreveal#Login to RunReveal":"Once RunReveal has enabled your domain on the platform, login with:\nrunreveal init\nThe init command is used to both create an account or log in to an existing account. If it is your first time running init, you'll be prompted to enter the name of your workspace.\nrunreveal init\nEnter your workspace name: Example Inc.\nYou can validate that you're logged in by running the following command\nrunreveal config account\n👍 You're logged in!\nUser ID: 2KUOdUOFyuTbPD7amU3WidyfOzf\nUser Email: evan@runreveal.com\nNow you're ready to search your logs like a pro and manage your RunReveal account from the terminal!","environment-variables#Environment Variables":"RunReveal CLI uses environment variables to set up specific aspects of the CLI configuration. These are helpful\nif you don't want to run runreveal init, configure your active workspace, proxy the CLI through a local proxy, or\nany number of use cases.","runreveal_token#RUNREVEAL_TOKEN":"The RUNREVEAL_TOKEN environment variable is used to authenticate your CLI session. It supplants the need to run runreveal init\nand it will override the token stored in your keychain or the RunReveal configuration file.","runreveal_workspace#RUNREVEAL_WORKSPACE":"This environment variable is used to set the active workspace for your CLI session. It will override the workspace ID\nif one is set in the RunReveal configuration file or keychain. This ID must be a workspace ID, not the name of your\nworkspace.","runreveal_baseurl#RUNREVEAL_BASEURL":"For customers who have deployed the RunReveal API on-prem or in a private cloud, you can set the RUNREVEAL_BASEURL\nenvironment variable to point the CLI to your RunReveal API instance.\nexport RUNREVEAL_BASEURL=https://api.runreveal.com","runreveal_debug#RUNREVEAL_DEBUG":"The RUNREVEAL_DEBUG environment variable is used to enable debug logging for the CLI. This is helpful if you're\ntroubleshooting an issue or want to see more information about the requests and responses from the RunReveal API.This will output the HTTP requests that are being sent to the RunReveal API.\nexport RUNREVEAL_DEBUG=1","runreveal_header_file#RUNREVEAL_HEADER_FILE":"This is a file that contains additional headers that should be sent along with all HTTP requests that the CLI makes.\nThis is helpful if you're using a proxy or need to send additional headers for authentication.\nexport RUNREVEAL_HEADER_FILE=/path/to/headers.json\nThe structure of the header file is a JSON object in this format:\n{\n    \"Headers\": {\n        \"Custom-Header\": \"value\",\n        \"Another-Header\": \"value\"\n    }\n}"}},"/release-notes/2024-release-notes":{"title":"RunReveal Release Notes","data":{"week-of-october-28-release-notes#Week of October 28, Release Notes":"PR: #945 - Nov-01-2024 - backend - bugfix - add missing unique indexes for enrichments:\nPR: #944 - Nov-01-2024 - backend - feature - Allow analyst role to read/edit enrichments\nPR: #942 - Oct-30-2024 - backend - bugfix - sourceType: fix sourceType for OTLP logs\nPR: #941 - Oct-30-2024 - backend - bugfix - Fix nil pointer in authentik log source\nPR: #940 - Oct-30-2024 - backend - bugfix - otlp: use raw strings and bytes, json everything else\nPR: #939 - Oct-30-2024 - backend - bugfix - otlp: fix value serialization\nPR: #938 - Oct-30-2024 - backend - bugfix - OTLP format: fix handling of non-string log record Body values\nPR: #934 - Oct-30-2024 - backend - bugfix - Update queries to match needed CF changes\nPR: #932 - Oct-30-2024 - backend - feature - Add OTLP JSON Log Source\nPR: #931 - Oct-29-2024 - backend - bugfix - Add authentik to sourceloader\nPR: #930 - Oct-29-2024 - backend - feature - Authentik source\nPR: #929 - Oct-29-2024 - backend - feature - Override google oauth creds\nPR: #926 - Oct-29-2024 - backend - bugfix - aws/assumerole: more plumbing\nPR: #924 - Oct-29-2024 - backend - bugfix - aws/s3: fix region specification for external s3\nPR: #923 - Oct-29-2024 - backend - bugfix - Don't specify notification retry url\nPR: #921 - Oct-28-2024 - backend - bugfix - fix missing headers initialization\nPR: #920 - Oct-28-2024 - backend - feature - clickhouse: support separate read/write paths\nPR: #919 - Oct-28-2024 - backend - feature - Added r2 jurisdiction for s3 access url.\nPR: #728 - Nov-01-2024 - frontend - feature - Add volume widget by source name\nPR: #726 - Oct-31-2024 - frontend - bugfix - Fix bugs with volume widget\nPR: #725 - Oct-31-2024 - frontend - bugfix - Don't flash detection and overwrite them with empty detection results\nPR: #724 - Oct-31-2024 - frontend - bugfix - Use primary key when loading detections, even when using ID\nPR: #723 - Oct-30-2024 - frontend - feature - Add authentik to explore page\nPR: #722 - Oct-31-2024 - frontend - feature - Allow ability to change rows per page\nPR: #720 - Oct-30-2024 - frontend - feature - otlp: add frontend source\nPR: #719 - Oct-30-2024 - frontend - bugfix - Fix oddities on the explore page with table list, and visible columns causing bugs when switching tables\nPR: #716 - Oct-29-2024 - frontend - feature - Add authentik source to front end\nPR: #715 - Oct-29-2024 - frontend - feature - r2 source image\nPR: #713 - Oct-29-2024 - frontend - feature - Make selecting fields rewrite the query being run on explore\nPR: #712 - Oct-29-2024 - frontend - bugfix - Fix front end bugs related to aggressive clickhouse queries.\nPR: #711 - Oct-29-2024 - frontend - bugfix - Fix sources page queries that are too intense on loading.\nPR: #710 - Oct-28-2024 - frontend - feature - Added fedramp to R2 settings\nPR: #709 - Oct-28-2024 - frontend - bugfix - Fix bucket name we'll read from","week-of-october-21-release-notes#Week of October 21, Release Notes":"PR: #916 - Oct-27-2024 - backend - feature - Objectstorage: plumb errors back to user and source_errors table.\nPR: #914 - Oct-26-2024 - backend - feature - Source Verification for R2 ingest type\nPR: #911 - Oct-27-2024 - backend - feature - clickhouse: support headers in BYO clickhouse destination\nPR: #910 - Oct-26-2024 - backend - feature - Add Support for Cloudflare R2 Blob Storage\nPR: #909 - Oct-25-2024 - backend - bugfix - Fix polling sources not saving next_poll_time\nPR: #908 - Oct-24-2024 - backend - bugfix - Refactor source name link in notification template\nPR: #907 - Oct-24-2024 - backend - bugfix - Add detection type to signal/alert views\nPR: #905 - Oct-23-2024 - backend - bugfix - Add missing gcs scope\nPR: #903 - Oct-23-2024 - backend - feature - Add GCP Identity Federation\nPR: #708 - Oct-26-2024 - frontend - bugfix - Add r2 missing icon\nPR: #707 - Oct-26-2024 - frontend - feature - R2 generic blob source\nPR: #706 - Oct-26-2024 - frontend - feature - Add R2 for CF HTTP logs\nPR: #705 - Oct-26-2024 - frontend - feature - Add R2 source from Cloudflare HTTP and DNS Gateway\nPR: #704 - Oct-24-2024 - frontend - bugfix - Fix UI colors in dark mode\nPR: #703 - Oct-24-2024 - frontend - bugfix - Fix small UI bugs on home page\nPR: #701 - Oct-23-2024 - frontend - feature - Add federated settings for GCP source\nPR: #700 - Oct-21-2024 - frontend - bugfix - Fix race condition in new alert history page. New managed enrichment","week-of-october-14-release-notes#Week of October 14, Release Notes":"PR: #902 - Oct-16-2024 - backend - bugfix - Fix rare data race in enrichment query path\nPR: #901 - Oct-14-2024 - backend - feature - Add pagerduty notification channel\nPR: #697 - Oct-20-2024 - frontend - feature - Redo design of alert history page\nPR: #695 - Oct-18-2024 - frontend - bugfix - Remove broken doc links\nPR: #694 - Oct-18-2024 - frontend - bugfix - Fix crowdstrike source desc.\nPR: #693 - Oct-18-2024 - frontend - bugfix - Fix alignment issues with splitview\nPR: #692 - Oct-18-2024 - frontend - feature - Resizable splits for results view and alert view detections\nPR: #690 - Oct-14-2024 - frontend - feature - Add pagerduty notification","week-of-october-7-release-notes#Week of October 7, Release Notes":"PR: #898 - Oct-10-2024 - backend - bugfix - Fix some events slipping through detections\nPR: #895 - Oct-10-2024 - backend - feature - Add normalized to sigma detections\nPR: #894 - Oct-08-2024 - backend - bugfix - visuals: restoration already run\nPR: #691 - Oct-11-2024 - frontend - dependencies - Bump dompurify from 2.4.7 to 2.5.7\nPR: #689 - Oct-11-2024 - frontend - bugfix - Fix 404 link to managed-enrichment\nPR: #687 - Oct-10-2024 - frontend - bugfix - Replace missing workspace create button\nPR: #686 - Oct-09-2024 - frontend - bugfix - Detection menu hidden on bottom rows\nPR: #685 - Oct-08-2024 - frontend - bugfix - Fix Demo/Signup buttons that don't link anywhere\nPR: #684 - Oct-09-2024 - frontend - feature - Make cli toggles show color\nPR: #683 - Oct-08-2024 - frontend - feature - Added edit as sql button to explore page\nPR: #682 - Oct-07-2024 - frontend - bugfix - fix error stacktraces being thrown in console","week-of-september-30-release-notes#Week of September 30, Release Notes":"PR: #892 - Oct-04-2024 - backend - feature - Change s3 sources to use multi ingest\nPR: #891 - Oct-01-2024 - backend - bugfix - Log Crowdstrike rawLog from S3\nPR: #888 - Sep-30-2024 - backend - bugfix - Don't log non-security related logs in GCP by default.\nPR: #885 - Sep-30-2024 - backend - feature - Add a separate domain managed enrichment for crypto domains\nPR: #680 - Oct-05-2024 - frontend - feature - Add comparison to alternative page template\nPR: #679 - Oct-04-2024 - frontend - feature - dropdown: improve slightly\nPR: #678 - Oct-04-2024 - frontend - bugfix - fix spelling\nPR: #677 - Oct-04-2024 - frontend - feature - Changes for multi ingest on all object storage sources\nPR: #676 - Oct-04-2024 - frontend - bugfix - Minor ux tweaks to the alert history.\nPR: #675 - Oct-04-2024 - frontend - feature - Better user-experience for dropdown filter buttons.\nPR: #673 - Oct-01-2024 - frontend - bugfix - Fix dark mode on source create\nPR: #672 - Oct-01-2024 - frontend - bugfix - Fix dark mode not working on source card\nPR: #671 - Oct-01-2024 - frontend - bugfix - fix data volume page (was querying logs? or rows? instead of bytes)\nPR: #670 - Oct-01-2024 - frontend - bugfix - Update data volume query to fix CH error\nPR: #668 - Oct-01-2024 - frontend - bugfix - Expand clickable sidebar to fill whole item","week-of-september-23-release-notes#Week of September 23, Release Notes":"PR: #887 - Sep-26-2024 - backend - performance - remove rules from enrichment list calls\nPR: #886 - Sep-26-2024 - backend - performance - Add 'Get' routes for enrichment configs\nPR: #883 - Sep-25-2024 - backend - bugfix - detections: faster save w/ validation (fixes RUN-501)\nPR: #882 - Sep-25-2024 - backend - bugfix - Skip non-IP/CIDR threat feed rules:\nPR: #881 - Sep-24-2024 - backend - bugfix - Skip GCP pub/sub notifications unless its OBJECT_FINALIZE\nPR: #880 - Sep-23-2024 - backend - bugfix - Reduce memory usage for enrichments\nPR: #878 - Sep-23-2024 - backend - bugfix - Exact -> CIDR for matching managed enrichments.\nPR: #867 - Sep-24-2024 - backend - performance - Change how polling scheduler gets next source\nPR: #666 - Sep-26-2024 - frontend - performance - Use Get instead of List for enrichment edit/create\nPR: #665 - Sep-26-2024 - frontend - feature - Updated empty source state and add source card\nPR: #663 - Sep-25-2024 - frontend - bugfix - Okta logs marketing page.\nPR: #662 - Sep-25-2024 - frontend - bugfix - detections: don't pass to/from parameters to create\nPR: #660 - Sep-25-2024 - frontend - feature - Managed enrichments\nPR: #659 - Sep-25-2024 - frontend - feature - Alerts: improved filters for alerts page\nPR: #652 - Sep-23-2024 - frontend - dependencies - Bump webpack from 5.91.0 to 5.94.0\nPR: #650 - Sep-23-2024 - frontend - dependencies - Bump micromatch from 4.0.5 to 4.0.8\nPR: #649 - Sep-23-2024 - frontend - feature - Alan/detections sorting filters","week-of-september-16-release-notes#Week of September 16, Release Notes":"PR: #876 - Sep-22-2024 - backend - bugfix - close rows in case of leak\nPR: #875 - Sep-19-2024 - backend - bugfix - Update polling s3 sources to get correct session\nPR: #874 - Sep-18-2024 - backend - feature - Allow wildcard source type for managed enrichments\nPR: #873 - Sep-18-2024 - backend - feature - Managed Enrichment for IP based enrichments\nPR: #872 - Sep-18-2024 - backend - bugfix - S3 ack if no events were sent\nPR: #871 - Sep-18-2024 - backend - bugfix - Fix s3\nPR: #870 - Sep-17-2024 - backend - bugfix - Fix issues with getting s3 region\nPR: #869 - Sep-16-2024 - backend - bugfix - Re-enable managed enrichments\nPR: #868 - Sep-16-2024 - backend - bugfix - Fix SSO to work with CLI\nPR: #866 - Sep-16-2024 - backend - feature - Added detection as code support for sigma detections\nPR: #861 - Sep-17-2024 - backend - feature - Set cloudtrail to use multi ingest\nPR: #648 - Sep-20-2024 - frontend - feature - detections: improve list view\nPR: #647 - Sep-19-2024 - frontend - feature - detections: add notification channels to detection list\nPR: #646 - Sep-19-2024 - frontend - feature - New sources empty state\nPR: #645 - Sep-19-2024 - frontend - bugfix - Alerts + Detections: refactor and fix \"Run Detection\" button.\nPR: #644 - Sep-19-2024 - frontend - dependencies - Update dropdowns with new icons and styling.\nPR: #641 - Sep-18-2024 - frontend - feature - New homepage.\nPR: #640 - Sep-18-2024 - frontend - feature - clipboard/archive icons replaced with history icon\nPR: #638 - Sep-20-2024 - frontend - bugfix - Fix format query with map columns\nPR: #637 - Sep-18-2024 - frontend - feature - Require name when saving detection\nPR: #636 - Sep-18-2024 - frontend - bugfix - Rename some links in sidebar\nPR: #635 - Sep-17-2024 - frontend - feature - Added proper collapsing sidebar\nPR: #634 - Sep-17-2024 - frontend - bugfix, feature - Clean up notifications copy:\nPR: #633 - Sep-16-2024 - frontend - feature - Support minimizing the sidebar\nPR: #632 - Sep-16-2024 - frontend - bugfix - Add SSO to CLI login\nPR: #630 - Sep-16-2024 - frontend - feature - Update streaming detections\nPR: #628 - Sep-17-2024 - frontend - feature - Added external s3 modal for ingest types update cloudtrail to support multi-ingest","week-of-september-9-release-notes#Week of September 9, Release Notes":"PR: #863 - Sep-12-2024 - backend - bugfix - Revert \"Managed Enrichments (#846)\"\nPR: #862 - Sep-12-2024 - backend - bugfix - Remove visibility timeout on sqs\nPR: #859 - Sep-11-2024 - backend - performance - Add watchdog timeout in config\nPR: #858 - Sep-11-2024 - backend - bugfix - Only saving webhook settings for some sources\nPR: #857 - Sep-10-2024 - backend - bugfix - Sigma should use Provider when looking at enrichments not the name\nPR: #856 - Sep-11-2024 - backend - bugfix, performance - Change how azure sources poll for events\nPR: #855 - Sep-10-2024 - backend - bugfix - Use PREWHERE when loading scheduled query runs results.\nPR: #854 - Sep-09-2024 - backend - feature - Drop duplicate sqs messages, controllable with const\nPR: #846 - Sep-11-2024 - backend - feature - Managed Enrichments\nPR: #629 - Sep-11-2024 - frontend - bugfix - Fix some of the source items aren't strings\nPR: #627 - Sep-11-2024 - frontend - bugfix - Display webhook settings and new webhook popup after creating\nPR: #626 - Sep-10-2024 - frontend - bugfix - Simplify query pattern on detection results page\nPR: #625 - Sep-10-2024 - frontend - bugfix - Dedupe IPs and actors to not display duplicates in list\nPR: #624 - Sep-10-2024 - frontend - bugfix - Use localStorage for alert history time picker.","week-of-september-2-release-notes#Week of September 2, Release Notes":"PR: #853 - Sep-06-2024 - backend - performance - remove the watchdog timer from detection destination\nPR: #852 - Sep-05-2024 - backend - bugfix - Update sigma package\nPR: #850 - Sep-05-2024 - backend - bugfix - Missing comma in clickhouse view.\nPR: #849 - Sep-05-2024 - backend - bugfix - Add textPayload to GCP log parsing.\nPR: #848 - Sep-05-2024 - backend - performance - Change ack helper to not use wait group and not need a go func\nPR: #845 - Sep-04-2024 - backend - bugfix - Fix crowdstrike fdr date parsing issues\nPR: #844 - Sep-04-2024 - backend - bugfix - Change how nsg flow records are read\nPR: #842 - Sep-03-2024 - backend - bugfix - Not telling k8s to continue to next bytes\nPR: #840 - Sep-03-2024 - backend - performance - bump batch sizes\nPR: #839 - Sep-03-2024 - backend - bugfix - Fix dnsfilter parsing issues\nPR: #838 - Sep-03-2024 - backend - bugfix - Fix bugs with object storage readers and how they are acked\nPR: #832 - Sep-02-2024 - backend - performance - Refactor Multi-Ingest Sources Slightly for Memory Efficiency\nPR: #827 - Sep-05-2024 - backend - bugfix - Populate important okta risk debugContext fields.\nPR: #622 - Sep-04-2024 - frontend - feature - Expose sigma links to users","week-of-august-26-release-notes#Week of August 26, Release Notes":"PR: #831 - Sep-01-2024 - backend - bugfix - Fix streaming clickhouse detection insert\nPR: #830 - Aug-29-2024 - backend - feature - AppInfo view and fix AppInfo log format.\nPR: #829 - Aug-29-2024 - backend - feature - Support parsing appinfo and userinfo crowdstrike fdr logs.\nPR: #828 - Aug-28-2024 - backend - bugfix - Add no sso error message\nPR: #826 - Aug-27-2024 - backend - bugfix - Error completely when receiving malformed logs.\nPR: #825 - Aug-27-2024 - backend - bugfix - Update okta source to parse an array of RawMessage\nPR: #824 - Aug-27-2024 - backend - feature - sso: migrate to list of approved domains\nPR: #823 - Aug-27-2024 - backend - bugfix - Fix login bug for different cases.\nPR: #820 - Aug-26-2024 - backend - bugfix - fix initialization of notificationListener\nPR: #791 - Aug-26-2024 - backend - feature - Augment table schema requests with primary keys\nPR: #621 - Sep-01-2024 - frontend - bugfix - Fix sigma 404 error\nPR: #620 - Aug-31-2024 - frontend - bugfix - detections: don't export predefined parameters\nPR: #619 - Aug-31-2024 - frontend - bugfix - Add parameters to detection view\nPR: #618 - Aug-30-2024 - frontend - bugfix - Add padding to sources list page.\nPR: #617 - Aug-29-2024 - frontend - feature - Add appinfo logs to front end.\nPR: #616 - Aug-29-2024 - frontend - bugfix - Support map columns in the where clause builder (again)\nPR: #615 - Aug-29-2024 - frontend - bugfix - Fix filter pattern not showing on filter page,.\nPR: #614 - Aug-29-2024 - frontend - bugfix - Prevent filters page from crashing with new data model\nPR: #613 - Aug-29-2024 - frontend - bugfix - Use correct cloudflare source type\nPR: #612 - Aug-29-2024 - frontend - bugfix - Show entire description and notes fields without textarea\nPR: #611 - Aug-29-2024 - frontend - feature - Support map columns in the where clause builder:\nPR: #610 - Aug-28-2024 - frontend - bugfix - sidebar: don't full reload with anchor tags, use react's Link\nPR: #609 - Aug-27-2024 - frontend - bugfix - Shading under detection frequency graph\nPR: #608 - Aug-28-2024 - frontend - bugfix - detections: remove flashDetection from localstoage after loaded Fixes RUN-550\nPR: #607 - Aug-27-2024 - frontend - feature - Toast login errors\nPR: #602 - Aug-30-2024 - frontend - feature - Add sigma support to frontend\nPR: #600 - Aug-26-2024 - frontend - bugfix - Update meeting links to demo page","week-of-august-19-release-notes#Week of August 19, Release Notes":"PR: #815 - Aug-23-2024 - backend - bugfix - Fix issue with missing azure logs\nPR: #811 - Aug-22-2024 - backend - bugfix - fix ids\nPR: #810 - Aug-22-2024 - backend - bugfix - Fix crowdstrike and notions normalized event IDs to use provider IDs\nPR: #807 - Aug-21-2024 - backend - feature - Support CIDR matching enrichment rules\nPR: #806 - Aug-20-2024 - backend - bugfix - Check for nil azure content length\nPR: #805 - Aug-20-2024 - backend - bugfix - ack only if not nil\nPR: #803 - Aug-20-2024 - backend - feature - Duplicate Event Source Middleware\nPR: #798 - Aug-19-2024 - backend - feature - Add enrichment support to the backend\nPR: #605 - Aug-23-2024 - frontend - feature - Allow sorting of sources\nPR: #604 - Aug-22-2024 - frontend - bugfix - Don't clobber the webhook signing key. Don't alert on success.\nPR: #603 - Aug-23-2024 - frontend - feature - Frontend support for CIDR matching rules\nPR: #601 - Aug-20-2024 - frontend - bugfix - Make the detection results page not crash.\nPR: #599 - Aug-21-2024 - frontend - feature - Frontend support for enrichments\nPR: #597 - Aug-19-2024 - frontend - bugfix - Demo Form Update\nPR: #596 - Aug-19-2024 - frontend - feature - Demo page","week-of-august-12-release-notes#Week of August 12, Release Notes":"PR: #802 - Aug-15-2024 - backend - feature - block login via oauth if sso configured\nPR: #801 - Aug-15-2024 - backend - bugfix - Fix baseurl to use mktBase\nPR: #799 - Aug-14-2024 - backend - feature - SSOReady for SAML Sign On\nPR: #595 - Aug-16-2024 - frontend - feature - Add source add card to source list\nPR: #594 - Aug-15-2024 - frontend - bugfix - sso: fix ⏎ navigating back instead of submitting\nPR: #593 - Aug-15-2024 - frontend - feature - New source card.\nPR: #592 - Aug-14-2024 - frontend - feature - SSO Support UI\nPR: #591 - Aug-13-2024 - frontend - bugfix - Make back button on notifications work\nPR: #590 - Aug-13-2024 - frontend - feature - Notification, creation, edit, and templates UI redo","week-of-august-5-release-notes#Week of August 5, Release Notes":"PR: #797 - Aug-10-2024 - backend - bugfix - Fix multi ingest issues\nPR: #796 - Aug-07-2024 - backend - bugfix - Use correct azure ack function\nPR: #795 - Aug-05-2024 - backend - bugfix - Use docker compose v2 syntax\nPR: #789 - Aug-05-2024 - backend - bugfix - google_workspace source: limit events per poll\nPR: #783 - Aug-06-2024 - backend - feature - Allow sources to ingest from multiple locations\nPR: #580 - Aug-06-2024 - frontend - feature - Allow sources to ingest from multiple locations","week-of-july-31-release-notes#Week of July 31, Release Notes":"PR: #795 - Aug-05-2024 - backend - bugfix - Use docker compose v2 syntax\nPR: #789 - Aug-05-2024 - backend - bugfix - google_workspace source: limit events per poll\nPR: #787 - Aug-01-2024 - backend - feature - Detection Parameters\nPR: #783 - Aug-06-2024 - backend - feature - Allow sources to ingest from multiple locations\nPR: #773 - Aug-02-2024 - backend - bugfix - Add filter prometheus metric\nPR: #588 - Aug-02-2024 - frontend - bugfix - Hotfix, detection page.\nPR: #587 - Aug-02-2024 - frontend - feature - Slight page redesign\nPR: #586 - Aug-02-2024 - frontend - feature - Reorganize settings to have a few dropdowns.\nPR: #585 - Aug-01-2024 - frontend - bugfix - TiB -> TB. Show same unit we bill for.\nPR: #584 - Aug-01-2024 - frontend - feature - Allow formatting detection in detection edit and create screen.\nPR: #583 - Aug-01-2024 - frontend - feature - Parameters page, fully working\nPR: #582 - Aug-01-2024 - frontend - bugfix - Fix dark mode font being impossible to see for Mediums\nPR: #581 - Aug-01-2024 - frontend - feature - Can sort alert history by signal vs alert.\nPR: #580 - Aug-06-2024 - frontend - feature - Allow sources to ingest from multiple locations","week-of-july-22-release-notes#Week of July 22, Release Notes":"PR: #781 - Jul-23-2024 - backend - bugfix - Remove lookup of amzID from cloudtrail source\nPR: #780 - Jul-22-2024 - backend - bugfix - Notification result link update\nPR: #779 - Jul-22-2024 - backend - bugfix - Ignore files/folders starting with .\nPR: #778 - Jul-22-2024 - backend - feature - Generic Azure Blob Source\nPR: #578 - Jul-25-2024 - frontend - feature - Table schema visual improvements\nPR: #577 - Jul-23-2024 - frontend - feature - Allow format of sql query\nPR: #576 - Jul-22-2024 - frontend - feature - Add generic azure blob source","week-of-july-15-release-notes#Week of July 15, Release Notes":"PR: #777 - Jul-18-2024 - backend - bugfix - Fix broken resources columns\nPR: #775 - Jul-18-2024 - backend - feature - K8s Audit Log source\nPR: #772 - Jul-17-2024 - backend - bugfix - Fix s3 destination update bug\nPR: #771 - Jul-17-2024 - backend - bugfix - Rename json field for mitreAttacks\nPR: #770 - Jul-16-2024 - backend - bugfix - Up okta limits\nPR: #769 - Jul-15-2024 - backend - bugfix - Added parallelism to palo source\nPR: #768 - Jul-16-2024 - backend - bugfix - Apple Silicon issues, ensure --no-keychain works\nPR: #767 - Jul-17-2024 - backend - bugfix - Pano perms.\nPR: #575 - Jul-18-2024 - frontend - feature - Add kubernetes audit logs source.\nPR: #574 - Jul-18-2024 - frontend - bugfix - Fix render loop on detection view\nPR: #573 - Jul-17-2024 - frontend - bugfix - Only show one version of api token.\nPR: #572 - Jul-17-2024 - frontend - bugfix - Fix some UI issues with detection view\nPR: #571 - Jul-15-2024 - frontend - bugfix - Palo alto table name bugfix\nPR: #569 - Jul-17-2024 - frontend - bugfix - Fix bugs on alert list","week-of-july-8-release-notes#Week of July 8, Release Notes":"PR: #766 - Jul-11-2024 - backend - feature - Palo Panorama Source and Materialized View\nPR: #765 - Jul-12-2024 - backend - bugfix - Fix rawLog field for runreveal audit events\nPR: #764 - Jul-12-2024 - backend - feature - Add workspace name to daily report email\nPR: #763 - Jul-10-2024 - backend - bugfix - Lookup backfill queue urls\nPR: #762 - Jul-10-2024 - backend - bugfix - allow role based session for backfill\nPR: #761 - Jul-10-2024 - backend - feature - Added Gitlab S3 streaming source\nPR: #756 - Jul-09-2024 - backend - feature - Added hosted zone log source\nPR: #570 - Jul-13-2024 - frontend - bugfix - Fix missing support and broken sla link.\nPR: #568 - Jul-11-2024 - frontend - feature - Palo Alto panorama source\nPR: #567 - Jul-10-2024 - frontend - bugfix - Add role arn and external id to backfill\nPR: #566 - Jul-10-2024 - frontend - bugfix - Tweak padding and bg color on new text renderer\nPR: #565 - Jul-10-2024 - frontend - feature - Added json viewer for vertical data\nPR: #564 - Jul-10-2024 - frontend - bugfix - Alert history side by side view\nPR: #563 - Jul-10-2024 - frontend - feature - Added gitlab source\nPR: #561 - Jul-09-2024 - frontend - feature - Added hosted zone log source\nPR: #560 - Jul-10-2024 - frontend - feature - Webhook signing","week-of-july-1-release-notes#Week of July 1, Release Notes":"PR: #760 - Jul-06-2024 - backend - bugfix - rrsch: write out logs in JSON w/ metadata\nPR: #755 - Jul-05-2024 - backend - feature - Expose Destination Errors to Customers\nPR: #754 - Jul-04-2024 - backend - bugfix - Max pages in cloudflare audit logs, undocumented api\nPR: #753 - Jul-05-2024 - backend - feature - Sign webhooks if they have a key.\nPR: #751 - Jul-05-2024 - backend - bugfix - Don't sync detections if no change to files\nPR: #749 - Jul-01-2024 - backend - feature - Create azure activity logs view.\nPR: #747 - Jul-01-2024 - backend - bugfix - Fix admin query with date picker\nPR: #746 - Jul-01-2024 - backend - bugfix - Fix admin page query\nPR: #745 - Jul-01-2024 - backend - feature - Allow runreveal cli to accept any format of token.\nPR: #744 - Jul-01-2024 - backend - bugfix, feature - Add Test Verification for Crowdstrike FDR\nPR: #558 - Jul-03-2024 - frontend - bugfix - Fix split view for explore / detection page\nPR: #557 - Jul-02-2024 - frontend - feature - Show schemas on tab view alongside tabs.\nPR: #555 - Jul-02-2024 - frontend - feature - Add line wrapping in sql box\nPR: #554 - Jul-02-2024 - frontend - feature - Added autocomplete columns/tables for sql queries\nPR: #553 - Jul-01-2024 - frontend - feature - Add azure activity view\nPR: #552 - Jul-01-2024 - frontend - bugfix - Fix volume count with date picker\nPR: #551 - Jul-01-2024 - frontend - bugfix - Remove isset check when getting size\nPR: #550 - Jul-01-2024 - frontend - feature - Add time picker to admin page\nPR: #549 - Jul-01-2024 - frontend - bugfix - Fix daisy issues\nPR: #548 - Jul-01-2024 - frontend - bugfix - Fix color on explore page new btn","week-of-june-24-release-notes#Week of June 24, Release Notes":"PR: #743 - Jun-27-2024 - backend - bugfix - Handle destination kcrypt appropriately.\nPR: #742 - Jun-27-2024 - backend - bugfix - Destinations management\nPR: #741 - Jun-26-2024 - backend - feature - Added nsg flow logs source\nPR: #740 - Jun-25-2024 - backend - bugfix - Alan was right ack messages in send or they don't ack\nPR: #739 - Jun-27-2024 - backend - bugfix - Remove extra newline from destination files.\nPR: #738 - Jun-24-2024 - backend - bugfix - Destinations secrets default of \nPR: #737 - Jun-25-2024 - backend - feature - Resend notifications if failed\nPR: #736 - Jun-24-2024 - backend - feature - S3 Destinations and ClickHouse update.\nPR: #547 - Jun-28-2024 - frontend - feature - Add support for custom tables in explore.\nPR: #546 - Jun-28-2024 - frontend - feature - Link to explore page for certain internal links\nPR: #545 - Jun-28-2024 - frontend - bugfix - Add crowdstrike fdr docs. Fix \"These docs\" missing space.\nPR: #544 - Jun-28-2024 - frontend - dependencies - Update daisyui\nPR: #543 - Jun-27-2024 - frontend - bugfix - Appropriately display bucket verification status\nPR: #542 - Jun-27-2024 - frontend - bugfix - Put menu above explore tabs (z-index)\nPR: #541 - Jun-27-2024 - frontend - bugfix - Destinations / Detections UI\nPR: #540 - Jun-26-2024 - frontend - feature - Added new nsg flow source\nPR: #539 - Jun-25-2024 - frontend - bugfix - Fix loading of notification list in detection create\nPR: #538 - Jun-25-2024 - frontend - bugfix - Destination repopulate externalID and minor UI bug\nPR: #537 - Jun-24-2024 - frontend - bugfix - Handle byodb clickhouses.\nPR: #536 - Jun-24-2024 - frontend - feature - S3 Destination Support\nPR: #524 - Jun-24-2024 - frontend - bugfix - Fixed explore page to run query when state changes","week-of-june-17-release-notes#Week of June 17, Release Notes":"PR: #735 - Jun-17-2024 - backend - feature - Use the userIdentity Arn on assume role.\nPR: #716 - Jun-17-2024 - backend - feature - Added test detection cli command\nPR: #535 - Jun-19-2024 - frontend - bugfix - Fix destination routing error\nPR: #534 - Jun-18-2024 - frontend - bugfix - Detection as code cleanup","week-of-june-10-release-notes#Week of June 10, Release Notes":"PR: #727 - Jun-11-2024 - backend - bugfix - Additional dropbox logging of hwm\nPR: #726 - Jun-10-2024 - backend - bugfix - Return enabled state of destinations.\nPR: #725 - Jun-10-2024 - backend - bugfix - Destinations tweak.\nPR: #723 - Jun-10-2024 - backend - bugfix - Destination Settings column should have default \nPR: #722 - Jun-10-2024 - backend - bugfix - Destinations Bugfix\nPR: #720 - Jun-10-2024 - backend - dependencies - sources: s/kawa/reveald/\nPR: #533 - Jun-16-2024 - frontend - feature - Support modifying time in data volume page.\nPR: #532 - Jun-10-2024 - frontend - bugfix - Don't allow configuration of default destination\nPR: #531 - Jun-10-2024 - frontend - feature - Destinations UI for ClickHouse Destinations and Framework for others","week-of-june-3-release-notes#Week of June 3, Release Notes":"PR: #719 - Jun-07-2024 - backend - feature - Dropbox polling source\nPR: #529 - Jun-08-2024 - frontend - bugfix - crowdstrike misspelling leads to unrendered image\nPR: #528 - Jun-07-2024 - frontend - bugfix, feature - Minor improvements to detections page.\nPR: #526 - Jun-07-2024 - frontend - feature - Add dropbox source UI\nPR: #525 - Jun-03-2024 - frontend - feature - Added filter input for explore saved queries","week-of-may-28-release-notes#Week of May 28, Release Notes":"PR: #708 - backend - bugfix - Missing semi-colons from sql migration (aidmaster & managed)\nPR: #707 - backend - bugfix - Rename id field in view\nPR: #706 - backend - feature - Add crowdstrike data table\nPR: #705 - backend - feature - Add crowdstrike_aidmaster_logs\nPR: #703 - backend - feature - Crowdstrike Falcon Data Replicator source\nPR: #702 - backend - bugfix - keeper typo\nPR: #701 - backend - bugfix - Fix keeper source mishandling empty HTTP request.\nPR: #700 - backend - feature - Support keeper source.\nPR: #698 - backend - bugfix - Fix where auth0 bearer token is set\nPR: #697 - backend - feature - Add auth0 bearer token support\nPR: #696 - backend - feature - Auth0 source and view.\nPR: #684 - backend - feature - First cut of detection creation wizard\nPR: #520 - frontend - feature - Add crowdstrike fdr tables to front end\nPR: #519 - frontend - feature - Falcon Data Replicator source for frontend.\nPR: #517 - frontend - feature - Update explore tab title with table that is selected\nPR: #516 - frontend - feature - Add keeper to front end.\nPR: #513 - frontend - feature - Add support for bearer tokens + auth0_logs table\nPR: #512 - frontend - feature - Add auth0 source front-end.\nPR: #509 - frontend - feature - Added detection export buttons","week-of-may-17-release-notes#Week of May 17, Release Notes":"PR: #692 - backend - feature - aad logs table\nPR: #691 - backend - performance - cloudtrail: add parallelism to source\nPR: #690 - backend - performance - sqs: more messages!\nPR: #688 - backend - performance - tweaks to avoid idling on network i/o\nPR: #687 - backend - performance - rrq: bump batch sizes and parallelism\nPR: #686 - backend - feature - Added export detection command\nPR: #685 - backend - bugfix - crm: don't run in test\nPR: #683 - backend - dependencies - clickhouse: upgrade dependency.  Next upgrade bumps otel and breaks\nPR: #681 - backend - bugfix - Alan/spring cleaning\nPR: #680 - backend - feature - Add teleport audit log source -- parquet\nPR: #679 - backend - bugfix - triggers: remove deprecated wasm code\nPR: #678 - backend - bugfix - only alert non-system health checks\nPR: #677 - backend - feature - polish links / template\nPR: #676 - backend - bugfix - more info slack template\nPR: #675 - backend - feature - Alan/work\nPR: #673 - backend - dependencies - Bump github.com/jub0bs/fcors from 0.5.0 to 0.9.0\nPR: #672 - backend - feature - Add re-invite functionality for workspace members.\nPR: #511 - frontend - feature - Add entra table to source list\nPR: #510 - frontend - bugfix - Remove week numbers from time picker\nPR: #508 - frontend - bugfix - Run sentry's setup wizard for next js\nPR: #507 - frontend - bugfix - Fix timepicker issue\nPR: #506 - frontend - bugfix - Revert \"Created new date range picker (#505)\"\nPR: #505 - frontend - feature - Created new date range picker\nPR: #503 - frontend - bugfix - Fix typo in teleport docs\nPR: #502 - frontend - feature - Add teleport front end source.\nPR: #501 - frontend - bugfix - Fix aggrivating secondary color\nPR: #500 - frontend - bugfix - Update icons on home screen from Jaime.\nPR: #498 - frontend - bugfix - Fix broken rawLog link on alert history\nPR: #497 - frontend - bugfix - Fix dead link on sources upgrade button\nPR: #495 - frontend - bugfix - Fixed errors identified with filtering\nPR: #494 - frontend - bugfix - Fix tos, support@runreveal.com\nPR: #493 - frontend - bugfix - Hide query on explore page\nPR: #492 - frontend - feature - Add filter for values in datagrid and bar graphs\nPR: #491 - frontend - bugfix, feature - Home improvements\nPR: #490 - frontend - bugfix - Correctly set gcs notify type when updating source\nPR: #489 - frontend - bugfix - Hide when small.\nPR: #488 - frontend - bugfix - Ej/bsides\nPR: #487 - frontend - feature - BSides page\nPR: #486 - frontend - bugfix - Improve filter UI\nPR: #485 - frontend - feature - Fix state on Explore\nPR: #484 - frontend - bugfix - Remove unused state variable causing memory leak\nPR: #483 - frontend - feature - New testimonials\nPR: #482 - frontend - feature, performance - Only show tables you have active sources to.\nPR: #480 - frontend - bugfix, feature - Add reinvitation button.\nPR: #479 - frontend - feature - Add alert history \"empty state\".\nPR: #478 - frontend - feature - Add additional fields to alert history.\nPR: #477 - frontend - bugfix - Forgot to check this in.","week-of-april-26-release-notes#Week of April 26, Release Notes":"PR: #671 - backend - bug - logsquery: return something on timeout, increase timeout\nPR: #670 - backend - feature - Add a zendesk source\nPR: #669 - backend - feature - signals group + network info\nPR: #668 - backend - bug - Add signals grouped.\nPR: #666 - backend - bug - Don't ack gcp subs unless we have messages\nPR: #665 - backend - feature - Support GCS object notifications","week-of-april-19-release-notes#Week of April 19, Release Notes":"PR: #665 - backend - feature - Support GCS object notifications\nPR: #664 - backend - bug - Fix github polling verification\nPR: #663 - backend - dependencies - Bump golang.org/x/net from 0.22.0 to 0.23.0\nPR: #662 - backend - performance - stop writing to default database\nPR: #660 - backend - bug - syntax error in polling sources\nPR: #659 - backend - bug - Don't schedule queries for unknown workspaces\nPR: #658 - backend - bug - Fix: uninitialized map in logs query API endpoint for SQL query parameters map. 🚀🌏 don't panic 👍\nPR: #657 - backend - bug - Fix rrsch crashes when ch store is nil\nPR: #656 - backend - bug - Missing negative sign in duration\nPR: #655 - backend - bug - KMS should also check for www-api url\nPR: #654 - backend - performance - cloudtrail logs cleanup\nPR: #653 - backend - feature - Add RiskScore to signals and alerts\nPR: #652 - backend - feature - Update GCP source to Poll Storage buckets\nPR: #635 - backend - bug - Fix from to not being set.","week-of-april-12-release-notes#Week of April 12, Release Notes":"PR: #649 - backend - feature - Added gcp logs view\nPR: #648 - backend - feature - Add notion source.\nPR: #647 - backend - bug - Fix webhookURLs so they don't go to www-api\nPR: #643 - backend - bug - Add event filtering to rrq processor\nPR: #460 - frontend - bug - Fix inability to edit Notion sources.\nPR: #459 - frontend - feature - Add Notion Source\nPR: #458 - frontend - feature - Test your filters prior to saving.\nPR: #457 - frontend - feature - Mark old gsuite source as deprecated","week-of-april-5-release-notes#Week of April 5, Release Notes":"PR: #646 - backend - feature - Added atlassian source\nPR: #645 - backend - bug - Fix nil ch store issue in scheduled query and threat feed insertion\nPR: #644 - backend - bug - hotfix\nPR: #642 - backend - feature - Add Filtering\nPR: #641 - backend - bug - RUN-406 fix grafana 404 on setup\nPR: #640 - backend - bug - bugfix: don't panic when cleaning up after connection proactively closed\nPR: #639 - backend - feature - Add get-sql endpoint to just return sql from pql/ai query\nPR: #638 - backend - bug - modules: update clickhouse-go\nPR: #636 - backend - feature, performance - Respect is_active on GCP logs.\nPR: #634 - backend - bug - Fix handling of failed queries in LogsQueryV2Results\nPR: #633 - backend - bug - notifications: fix link to see query results\nPR: #456 - frontend - bug - Make overflow behave properly.\nPR: #455 - frontend - bug - Fix double click popup, only 1 portal per page.\nPR: #454 - frontend - feature - Added atlassian source\nPR: #453 - frontend - bug, feature - Detection create improvements\nPR: #450 - frontend - bug - Don't render added sources as available when editing.\nPR: #449 - frontend - bug - Update docs in filter page.\nPR: #448 - frontend - bug - Fix filter links to use next routing\nPR: #447 - frontend - bug - Remove filter flags that have no effect\nPR: #446 - frontend - bug - Fix breadcrumb UI errors\nPR: #445 - frontend - bug - Correctly show results when viewing schResult\nPR: #444 - frontend - feature - Add data filters\nPR: #443 - frontend - bug - Ej/fix 404 again\nPR: #442 - frontend - bug - Add settings page.\nPR: #441 - frontend - bug - Explore Bug-fix, res is not defined.\nPR: #440 - frontend - bug - Quiet these errors.\nPR: #439 - frontend - bug - fix settings 404\nPR: #438 - frontend - feature - Added tabs for searching\nPR: #437 - frontend - bug - Fix 404s on the docs in source pages.\nPR: #436 - frontend - bug - Fix timepicker, timezones vs GMT\nPR: #435 - frontend - bug - Set Parameters Correctly in Detection Edit View\nPR: #434 - frontend - bug - hotfix, account->settings\nPR: #433 - frontend - bug, performance - minor nits, fix alignment and styling of btns. Use Link for breadcrumb","week-of-march-29-release-notes#Week of March 29, Release Notes":"PR: #631 - backend - bug - Initialize column data when query executed\nPR: #630 - backend - bug - update config\nPR: #629 - backend - feature - querylog: add status column, remove canceled, add history\nPR: #628 - backend - performance - github: enforce tags on PRs for release notes\nPR: #432 - frontend - feature - Some updates to docs.\nPR: #431 - frontend - feature - New hero image.\nPR: #430 - frontend - feature - Provide homepage that contains platform high level metrics.\nPR: #429 - frontend - bug - alerts: fix links from alert history to results\nPR: #428 - frontend - performance - github: enforce labels on PRs\nPR: #427 - frontend - bug - explore: fix erroneous space in table parameter\nPR: #426 - frontend - feature - Support breadcrumbs throughout.\nPR: #421 - frontend - feature - Search page style updates, Add Results View","week-of-march-1-22-release-notes#Week of March 1-22, Release Notes":"PR: #621 - backend - feature - Add sophos Source\nPR: #620 - backend - bug - Don't send report if all queries return 0 records\nPR: #616 - backend - feature - Add dnsfilter source.\nPR: #615 - backend - performance - Slugify all names for configs\nPR: #614 - backend - feature - Google Workspace 2 -- Workspaces Revenge\nPR: #613 - backend - dependencies - Bump google.golang.org/protobuf from 1.31.0 to 1.33.0\nPR: #612 - backend - bug - Remove level from aad log\nPR: #611 - backend - bug - Convert aad log level to a string\nPR: #610 - backend - bug - Fix azure logs saving array to rawLog instead of single event\nPR: #609 - backend - feature - List destinations, for destinations marketing page.\nPR: #604 - backend - bug - more minor fixes\nPR: #603 - backend - bug - minor fixes after testing\nPR: #599 - backend - bug - Fix mitreAttacks typo causing no updates.\nPR: #598 - backend - feature - Add cloudentity source to RunReveal\nPR: #597 - backend - feature - Store Query Log in postgres, Results in S3, Add Pagination and Retrieve Result Endpoint\nPR: #420 - frontend - enhancement - Create sophos source\nPR: #419 - frontend - bug - Fix ugly transform page.\nPR: #414 - frontend - enhancement - Slugify all configs\nPR: #409 - frontend - bug - Tweak sources page\nPR: #407 - frontend - bug - Fix bug when group by columns are not strings for explore\nPR: #406 - frontend - bug - Fix double-typing.\nPR: #404 - frontend - enhancement - Add mitreAttacks to the detection mgmt page.\nPR: #403 - frontend - enhancement - Add cloudentity source to frontend.\nPR: #402 - frontend - enhancement - Add group by to histogram on explore"}},"/release-notes/release-v2025-4-10":{"title":"Release Notes for runreveal v2025.4.10","data":{"️-improvements#🛠️ Improvements":"Fixed an issue with GitHub CLI in our actions pipeline (#1213)\nThis minor release addresses a technical issue to ensure smoother operations in our development workflow."}},"/release-notes/release-v2025-4-13":{"title":"runreveal v2025.4.13 Release Notes","data":{"":"🚀 This release introduces a small but important update:","whats-new#What's New":"📊 SQL migrations are now included in the release package (#1215)\nThis change ensures that database schema updates are properly bundled with the software, improving deployment consistency and reliability."}},"/release-notes/release-v2025-4-14":{"title":"Release Notes - v2025.4.14 🚀","data":{"highlights#Highlights":"🔧 Improved Processing: All processing has been moved to a new pipeline executor, enhancing overall performance and efficiency.🔄 Dynamic Pipelines: Introduced support for dynamic pipelines, offering more flexibility in data processing workflows.","changelog#Changelog":"Implement topics/pipeline backend (#1179)"}},"/release-notes/release-v2025-4-12":{"title":"runreveal v2025.4.12 Release Notes","data":{"":"🚀 Highlights:\nAdded migration table and basic migration tool for Panther\nIntroduced support for metadata and basic information from other SIEM provider detection data models","changelog#Changelog":"Add migration table, supporting metadata and basic information from other SIEM provider detection data models (#1214)"}},"/release-notes/release-v2025-4-15":{"title":"Release Notes - v2025.4.15","data":{"":"🏷️ Improved Event Tagging in PipelinesThis release focuses on enhancing the reliability of our event tagging system within pipelines. The update ensures that events are properly tagged, improving data organization and analysis capabilities.","changelog#Changelog":"Implemented measures to ensure events are tagged properly in pipelines (#1216)"}},"/release-notes/release-v2025-4-11":{"title":"RunReveal v2025.4.11 Release Notes","data":{"-highlights#🚀 Highlights":"This release introduces a significant enhancement to our data management capabilities:\nNew API and Database Layer for Topics 🗂️\nImproves our ability to organize and manage data topics\nIncludes a new utility topicsctl for manipulating topics","changelog#Changelog":"Implemented API and Database layer for topics (#1162)\nAdded topicsctl utility for topic management\nWe're excited about how these improvements will enhance your data analysis experience with RunReveal!"}},"/release-notes/release-v2025-4-17":{"title":"Release Notes for runreveal v2025.4.17 🚀","data":{"whats-new#What's New":"🔐 Improved Google Workspace Authentication:\nDefault authentication type set to service_account when left blank\nEnhanced error handling for impersonation URL configuration\nThis update streamlines the authentication process for Google Workspace integrations, providing a more robust and user-friendly experience.","changelog#Changelog":"Default blank Google Workspace auth type to service_account (#1222)"}},"/release-notes/release-v2025-4-20":{"title":"🚀 runreveal v2025.4.20","data":{"whats-new#What's New":"🐢 Performance Improvements: We've made some adjustments to optimize performance, although we couldn't achieve the speed we initially hoped for. (#1231)","full-changelog#Full Changelog":"Couldn't make it fast (#1231)"}},"/release-notes/release-v2025-4-19":{"title":"Release Notes for v2025.4.19","data":{"":"🚀 This release focuses on improvements to our authentication process and GitHub Actions workflow.","highlights#Highlights":"🔐 Enhanced authentication flow to ensure proper workspace initialization\n🛠️ Optimized GitHub Actions to skip unnecessary builds","changelog#Changelog":"actions: dont build when nothing has changed for app and backend (#1230)\nAuth: add state barrier to loading the app dependent on resolving workspace (#1229)\nRevert \"Auth: Ensure a workspace is always set before continuing initialization. (#1227)\" (#1228)\nAuth: Ensure a workspace is always set before continuing initialization. (#1227)\nRevert \"Fix rapid redirects in Auth components (#1224)\" (#1226)\nFix rapid redirects in Auth components (#1224)"}},"/release-notes/release-v2025-4-23":{"title":"runreveal v2025.4.23 Release Notes","data":{"":"🛠️ Bug Fixes and Improvements\nFixed skipRoles issue for Cloudflare (#1242)\nImproved vertical view for logs and Security Alert View (#1241)","additional-changes#Additional Changes":"Removed duplicate add button in filters (#1246)\nRe-added filter create button (#1243)\nAdded CTA for filters list (#1245)"}},"/release-notes/release-v2025-4-21":{"title":"Release Notes - runreveal v2025.4.21","data":{"-bug-fixes#🐛 Bug Fixes":"Fixed an issue where the detection history page was excessively tall, improving overall user experience and navigation (#1232)\nThis minor release addresses a layout problem in the detection history page, ensuring a more compact and user-friendly display."}},"/release-notes/release-v2025-4-22":{"title":"Release Notes: v2025.4.22 🚀","data":{"highlights#Highlights":"🔒 Enhanced Authentication Flow\nImproved user session management and login redirects\nMore robust state validation and caching mechanisms\n📊 GitHub Integration Updates\nAdded GitHub logs table and view\nImplemented filters for migration page","detailed-changes#Detailed Changes":"Removed credentials from user session fetch for improved security\nRefactored RedirectToLogin functionality within AuthContext\nOptimized use of localStorage for caching instead of auth state\nImproved post-login redirect handling\nAdded GitHub logs table and corresponding view\nImplemented filters for the migration page\nUpdated ECR workflow\nFor a complete list of changes, please refer to the full changelog."}},"/release-notes/release-v2025-4-16":{"title":"RunReveal v2025.4.16 Release Notes","data":{"":"🚀 This release brings exciting improvements to authentication and data sources!","highlights#Highlights":"🔐 Added support for Google Workspace authentication using Workload Identity Federation (#1220)\n📦 Introduced a generic Google Cloud Storage (GCS) source (#1221)","other-changes#Other Changes":"📜 Improved scrolling experience by fixing overflow issues (#1219)\n🔍 Enhanced \"Show Details\" functionality for a cleaner interface (#1217)\n🔧 Removed problematic dashboard redirect to prevent loops (#1218)"}},"/release-notes/release-v2025-4-18":{"title":"Release Notes - v2025.4.18","data":{"":"🐛 Bug Fix: Google Workspace AuthenticationThis release addresses an issue with Google Workspace authentication when requesting GCP scopes for service accounts. The fix ensures that additional scopes are only requested when using identity federation.","changelog#Changelog":"Fix bug with google workspace when requesting gcp scopes for service accounts (#1223)"}},"/release-notes/release-v2025-4-25":{"title":"Release v2025.4.25","data":{"":"🚀 Performance Boost for GCP Users","whats-new#What's New":"🔐 Added caching for GCP token sources, improving authentication efficiency (#1225)\n📊 Implemented tracking for notification history (#1247)\nThese updates enhance performance and provide better insights into system notifications. Enjoy the improvements!"}},"/release-notes/release-v2025-4-26":{"title":"Release Notes for runreveal v2025.4.26","data":{"":"🕰️ Custom Time Formats for Transforms","whats-new#What's New":"✨ Added support for custom time format strings in transforms\n🔧 Fixed a bug related to time selection in graphs","changelog#Changelog":"Allow custom time formats for transforms (#1250)\nFix graph time selection bug (#1249)"}},"/release-notes/release-v2025-4-24":{"title":"Release Notes - v2025.4.24 🚀","data":{"summary#Summary":"This release brings improvements to our notification system, including centralized configuration loading and enhanced history tracking. 📊","changes#Changes":"🔧 Refactored notification system:\nCentralized notification config loading\nAdded notification history tracking\nCentralized notification sending logic\n🐛 Fixed notification history test and handling of meta map\n🧹 Removed unused template\n📝 Various formatting fixes"}},"/release-notes/release-v2025-4-28":{"title":"Release Notes for runreveal v2025.4.28 🚀","data":{"highlights#Highlights":"🔌 Added a generic GCP Pub/Sub sourceThis release introduces a new feature that allows users to connect to Google Cloud Pub/Sub as a generic data source. This enhancement expands the data integration capabilities of runreveal, enabling more flexible and diverse data ingestion options.","changes#Changes":"Added backend support for GCP Pub/Sub as a generic source\nImplemented frontend interface for configuring GCP Pub/Sub connections\nWe hope you enjoy this new addition to runreveal! As always, we appreciate your feedback and support."}},"/release-notes/release-v2025-4-5":{"title":"Release Notes for runreveal v2025.4.5","data":{"":"📝 Minor update to release notes processThis release includes a small adjustment to our release notes process, ensuring more accurate and balanced descriptions of changes.","changes#Changes":"Refined the tone of our release notes to better reflect the nature of updates\nCorrected file reference for documentation"}},"/release-notes/release-v2025-4-29":{"title":"Release Notes for v2025.4.29","data":{"":"🚀 Event Routing UI is here!","highlights#Highlights":"New Event Routing UI: Create and manage pipelines with ease using our new intuitive interface. Set up preconditions, customize pipeline steps, and edit individual pipelines.\nImproved Raw Data Rendering: Explore page now features more readable raw results, enhancing your data analysis experience.","detailed-changes#Detailed Changes":"Created Event Routing UI with pipeline creator, topic wizard, and pipeline management features\nAdded fallback timestamp values for raw data rendering\nEnhanced readability of raw results on the explore page\nWe're excited to bring you these improvements and look forward to your feedback!"}},"/release-notes/release-v2025-4-6":{"title":"Release Notes for runreveal v2025.4.6 🚀","data":{"highlights#Highlights":"🔄 Switched package distribution from Cloudsmith to Gemfury (#1209)\n🛠️ Improved frontend build process for releases (#1209)\n📁 Adjusted Next.js file naming conventions for routes (#1208)","full-changelog#Full Changelog":"release: cloudsmith->gemfury (#1209)\nnextjs: lower case files for routes only I guess (#1208)"}},"/release-notes/release-v2025-4-27":{"title":"Release Notes for runreveal v2025.4.27","data":{"":"🐛 Bug Fixes\nFixed a panic in the query listener by ensuring the channel is only closed after writing results (#1253)\nPrevented overwriting of eventTime after transform runs (#1252)\nThese updates improve the stability and data integrity of the runreveal system."}},"/release-notes/release-v2025-4-8":{"title":"Release Notes - runreveal v2025.4.8","data":{"":"🚀 Minor update focusing on workflow improvements","changes#Changes":"Streamlined GitHub Actions workflow by removing an unnecessary job (#1211)\nThis release includes a small optimization to our CI/CD process, ensuring smoother and more efficient automated workflows."}},"/release-notes/release-v2025-4-x":{"title":"Release Notes: v2025.4.X 🚀","data":{"":"🚀 Streamlined Build ProcessThis build focuses on improving our build infrastructure:\nUpdated goreleaser configuration to depend only on private build\nThese changes enhance our development workflow and ensure smoother builds.\nWhile not directly user-facing, they contribute to the overall stability and\nefficiency of the project.","changelog#Changelog":"goreleaser: update to only depend on private build (#1203)\nFix personal access token reference"}},"/release-notes/release-v2025-4-9":{"title":"Release Notes for runreveal v2025.4.9","data":{"":"🔒 Security EnhancementThis minor release includes an important security update to our GitHub Actions workflow:\nImproved token handling during the upload process (#1212)\nFor more details, please refer to the full changelog below.","changelog#Changelog":"actions: unset token before upload (#1212)"}},"/release-notes/release-v2025-5-10":{"title":"Release Notes for runreveal v2025.5.10 🚀","data":{"whats-new#What's New":"🏠 Home Page Improvements: We've fixed the alert count display on the home page, ensuring you always have an accurate view of your alerts. (#1282)🔧 Pipeline UI Enhancements: Various UI issues with pipelines have been addressed, providing a smoother experience. (#1280)","other-changes#Other Changes":"Added descriptions to transform configs for better clarity. (#1281)\nConsolidated services into a single binary to improve integration testing. (#1273)"}},"/release-notes/release-v2025-5-1":{"title":"Release v2025.5.1 🚀","data":{"highlights#Highlights":"🔍 Improved precondition matching for sourceID and sourceType (#1261)\n🔗 Fixed source redirect on page load (#1260)","full-changelog#Full Changelog":"Fix precondition matching on sourceID, sourceType (#1261)\nFix source redirect on page load (#1260)\nAdd a dummy go.mod to app directory (#1256)\nFix escaping quotes when copying from data grid (#1257)"}},"/release-notes/release-v2025-4-7":{"title":"RunReveal v2025.4.7 Release Notes","data":{"":"🚀 This minor release includes:\nImproved distribution process: Automated copying to dist repository (#1210)\nFor a complete list of changes, please refer to our detailed changelog."}},"/release-notes/release-v2025-5-12":{"title":"runreveal v2025.5.12 Release Notes","data":{"":"🐛 Bug Fix: Improved CrowdstrikeFDR ParsingThis release enhances the CrowdstrikeFDR parser with better error handling and type conversion:\nParsing errors are now properly passed upstream instead of being silently skipped\nAdded a new StringOrInt type to handle fields that can be either strings or integers\nBasic event metadata is preserved even when parsing fails, ensuring better error tracking\nThese improvements will lead to more reliable data processing and easier troubleshooting for CrowdstrikeFDR events."}},"/release-notes/release-v2025-5-13":{"title":"Release Notes - v2025.5.13","data":{"":"🚀 Highlights:\nImproved error handling for object storage\nNew detection errors page with enhanced filtering and sorting","whats-new#What's New":"","️-fixes-and-improvements#🛠️ Fixes and Improvements":"Fixed object storage error handling to prevent early termination (#1286)\nErrors are now forwarded to the end user without interrupting processing","-new-features#📊 New Features":"Added detection errors page with time filtering and sortable columns (#1285)","-performance#🔍 Performance":"Ensured detections, alerts, and signals tables use index on explore (#1271)"}},"/release-notes/release-v2025-5-14":{"title":"Release Notes: runreveal v2025.5.14","data":{"":"🚀 Highlights:\nImproved error handling and visibility for detection queries","changes#Changes":"🔍 Created a detection error view for individual detection queries (#1287)\n🔗 Fixed errors page to link to managed detections (#1288)\n⚠️ Addressed potential issues with ClickHouse integration (#1289)"}},"/release-notes/release-v2025-5-11":{"title":"Release Notes: v2025.5.11 🚀","data":{"whats-new#What's New":"🔧 Improved efficiency in rrsch by passing in pointers (#1283)\nThis minor update focuses on a small but important technical improvement to enhance performance.","changelog#Changelog":"rrsch: pass in pointer (#1283)"}},"/release-notes/release-v2025-5-16":{"title":"Release Notes: v2025.5.16 🚀","data":{"highlights#Highlights":"🔍 Improved query syntax: Removed square brackets from IN/NOT IN queries for better readability (#1299)\n🐞 Enhanced CrowdStrike FDR detection parsing (#1300)","other-changes#Other Changes":"Updated docker-compose configuration (#1298)\nAdjusted log levels for non-critical issues (#1293)\nOptimized explore table list rendering (#1295)\nFor a complete list of changes, please refer to our changelog."}},"/release-notes/release-v2025-5-17":{"title":"Release Notes for runreveal v2025.5.17","data":{"":"🛡️ CrowdStrike Integration Enhancement","whats-new#What's New":"CrowdStrike FDR Event Logging: We've added support for logging Falcon Data Replicator (FDR) events from CrowdStrike, enhancing our security monitoring capabilities. (#1302)\nThis update improves our ability to track and analyze security events, providing more comprehensive insights for threat detection and response."}},"/release-notes/release-v2025-5-15":{"title":"Release Notes - runreveal v2025.5.15","data":{"":"🚀 This release includes an important fix and usability improvements:","whats-new#What's New":"🛠️ Fixed an issue with saving sources\n🚪 Improved login flow: users are now prompted to go directly to the dashboard\n🔍 Limited detection error page results to 100 for better performance","changelog#Changelog":"Immediate fix for saving source (#1294)\nPrompt user when they are logged in to go directly to dashboard (#1292)\nAdd limit 100 to detection error page. (#1291)"}},"/release-notes/release-v2025-5-18":{"title":"Release Notes for runreveal v2025.5.18 🚀","data":{"whats-new#What's New":"🔍 Enhanced CrowdStrike Integration:\nAdded more debug logs for improved troubleshooting and monitoring\nThis minor update focuses on enhancing the debugging capabilities for our CrowdStrike integration, making it easier for developers and system administrators to diagnose and resolve issues."}},"/release-notes/release-v2025-5-20":{"title":"Release v2025.5.20 🚀","data":{"summary#Summary":"This release brings improvements to detection parameters and documentation updates.","changelog#Changelog":"🔍 Add parameters to failed detections (#1296)\n📚 Bring README up to speed. (#1301)\n🛠️ fix/tools: container prefixed with runreveal-local (#1308)"}},"/release-notes/release-v2025-5-19":{"title":"Release Notes for runreveal v2025.5.19","data":{"":"🐛 Bug Fixes and Improvements\nHandle blank lines in data processing, addressing an issue missed in a previous cherry-pick (#1306)\nEnhance CrowdStrike FDR support to handle lines larger than 64KB (#1305)\nThese updates improve data handling and compatibility, ensuring smoother operation for users working with various data formats and sizes."}},"/release-notes/release-v2025-5-2":{"title":"runreveal v2025.5.2 Release Notes","data":{"summary#Summary":"🔒 Enhanced security with source verification for generic GCP Pub/Sub","changes#Changes":"Added source verification for generic GCP Pub/Sub\nImproved detection management by preventing export of managed detections"}},"/release-notes/release-v2025-5-23":{"title":"Release Notes for runreveal v2025.5.23","data":{"️-fixes#🛠️ Fixes":"Improved GitHub data normalization: GitHub Events Actor is now correctly set to username instead of email. This enhances the accuracy of GitHub-related data in the system. (#1307)"}},"/release-notes/release-v2025-5-22":{"title":"Release Notes for runreveal v2025.5.22 🚀","data":{"summary#Summary":"This release introduces a new CIDR match precondition type and includes documentation and API testing improvements.","changes#Changes":"🌐 Added CIDR match precondition type (#1316)\n📚 Improved documentation and API testing (#1314)"}},"/release-notes/release-v2025-5-24":{"title":"Release Notes - v2025.5.24 🚀","data":{"highlights#Highlights":"🛠️ Improved error handling and robustness when parsing files line-by-line by replacing bufio.Scanner with bufio.Reader (#1304)","changes#Changes":"Added widget for RunReveal audit logs to homepage (#1311)\nFixed issue with multiple filters for same filter selected (#1317)\nFor more details, please check the full changelog."}},"/release-notes/release-v2025-5-26":{"title":"Release Notes v2025.5.26 🚀","data":{"highlights#Highlights":"🔧 Simplified polling source create/update process (#1328)\n🎨 Removed Kendo and unused components, switched to Google Fonts for IBM Plex (#1330)","changes#Changes":"Added lambda operator to formatter (#1338)\nImproved CI workflow: Tests now run after format and lint checks (#1337)\nFixed audit log and cleaned up some routes in notifications (RUN-910) (#1333)\nAddressed TypeScript errors in detections (#1329)\nFixed issue with time selection being called twice (#1334)"}},"/release-notes/release-v2025-5-27":{"title":"Release Notes for v2025.5.27 🚀","data":{"highlights#Highlights":"🔐 Improved workspace invitation flow: Invited users no longer automatically receive their own organization and workspace, streamlining the onboarding process.","changes#Changes":"Changed workspace invitation flow to prevent automatic creation of separate organizations and workspaces for invited users (#1336)\nFixed name collisions in grid columns (RUN-893) (#1342)\nResolved grid width rendering issue (#1341)\nUpdated timepicker to correctly update its state on load (#1340)\nCorrected site font (#1339)\nAdded pipeline sidebar item (#1279)"}},"/release-notes/release-v2025-5-3":{"title":"Release Notes - runreveal v2025.5.3","data":{"":"🐛 Bug Fixes:\nFixed detection as code sync (#1264)\nImproved S3 destination setup with proper message handling (#1263)\nThis release focuses on resolving synchronization issues and enhancing S3 destination configuration."}},"/release-notes/release-v2025-5-28":{"title":"Release Notes for runreveal v2025.5.28 🚀","data":{"highlights#Highlights":"🛠️ Fixed JSON serialization of normalized network ports (#1345)\n🎉 Added new onboarding page (#1346)","changelog#Changelog":"Fix JSON serialization of normalized network ports (#1345)\nAdd onboarding page (#1346)"}},"/release-notes/release-v2025-5-31":{"title":"Release Notes - v2025.5.31","data":{"-highlights#🚀 Highlights":"Added compression format selector for S3 type sources 🗜️\nImproved sidebar navigation with native links 🔗","-enhancements#🔧 Enhancements":"Compression setting for S3 type sources\nOptions: default, none, gzip, zstd\nUpdated S3Settings type to include compression property\nMinor rolegen fix to skip empty lines\nUpdated sidebar links to use native behavior for better routing\nAdded Jamf Protect as a new source option"}},"/release-notes/release-v2025-5-30":{"title":"Release Notes: v2025.5.30","data":{"-highlights#🚀 Highlights":"Added Cyberhaven webhook source support 🛡️\nImproved pipeline SVG rendering performance 🖼️","detailed-changes#Detailed Changes":"","new-features#New Features":"RUN-796: Added Cyberhaven webhook source","improvements#Improvements":"RUN-932: Updated pipeline SVG rendering to use svg tag instead of svgcomponent for better performance"}},"/release-notes/release-v2025-5-32":{"title":"RunReveal v2025.5.32 Release Notes","data":{"":"🚀 Highlights:\nImproved queue management with generic string usage\nEnhanced UI for better user experience","whats-new#What's New":"🔧 Moved queue name lookup and implemented generic string for most queues (#1347)\n🖥️ Adjusted format/edit buttons to prevent overlap with text areas (#1359)\n🔒 Fixed analyst permissions for investigations/destinations, resolving UI error toasts (#1356)\n🔍 Prevented automatic formatting of queries being rerun (#1355)\n🧹 Removed user/currentUser from formatter functions (#1360)"}},"/release-notes/release-v2025-5-34":{"title":"Release Notes: v2025.5.34 🚀","data":{"summary#Summary":"This release includes a critical hotfix for Jamfprotect IP access issues.","changes#Changes":"🛠️ Fixed IP access problems in Jamfprotect (#1376)"}},"/release-notes/release-v2025-5-29":{"title":"Release Notes for v2025.5.29","data":{"":"🔧 Bug Fixes and Improvements","highlights#Highlights":"🚀 Enhanced GCP log parsing for better data preservation\n👥 Improved workspace admin capabilities","detailed-changes#Detailed Changes":"GCP: Fixed log parsing to preserve rawLog entirely (RUN-925)\nAdmins can now revoke a user within the workspace admin view\nRenamed \"User Sessions\" tab to \"My Sessions\" in the workspace admin view"}},"/release-notes/release-v2025-5-33":{"title":"Release Notes for runreveal v2025.5.33 🚀","data":{"highlights#Highlights":"🔀 Enhanced log routing: Now supports multiple ClickHouse destinations for improved flexibility and scalability (#1358)\n🖱️ Improved UI: Drag and drop explore page tabs, organized sidebar, and workspace picker enhancements (#1361, #1364, #1372)","changes#Changes":"Removed impossible travel detection (#1344)\nFixed share link functionality when viewing detection results (#1354)\nAdded rawLog to cyberhaven view (#1362)\nImproved handling of active tabs in the UI (#1365)\nFixed issue with Allow format hiding the wrong div (#1371)\nRe-added jamfprotect to fix migration errors (#1373)\nFor a complete list of changes, please refer to the full changelog."}},"/release-notes/release-v2025-5-25":{"title":"Release Notes - v2025.5.25","data":{"":"🎉 We're excited to bring you the latest updates to runreveal! Here's what's new:","highlights#Highlights":"🔐 Enhanced workspace token management: Added \"created by\" information to workspace tokens table (#1331)\n📊 Improved query result display: All query result grids now use the same component for consistency (#1320)","other-changes#Other Changes":"Fixed event name handling in flows (#1332)\nFor a complete list of changes, please refer to our GitHub repository."}},"/release-notes/release-v2025-5-35":{"title":"Release Notes - v2025.5.35 🚀","data":{"highlights#Highlights":"🔍 Enhanced Network Flow Log SupportWe're excited to introduce support for Virtual Network (VNet) flow logs in this release. This update improves our network traffic analysis capabilities, providing more comprehensive insights into your Azure networking environment.","changes#Changes":"Added parsing support for VNet flow logs\nUpdated existing NSG flow log parsing to accommodate both VNet and traditional formats\nRefined and optimized the flow log processing functionality\nThese enhancements will enable more detailed and flexible network traffic analysis, helping you better understand and manage your Azure virtual networks."}},"/release-notes/release-v2025-5-38":{"title":"Release Notes for v2025.5.38","data":{"-highlights#🚀 Highlights":"New ingest type for reading object storage records from filesystem\nStandardized button design across the application","-changelog#📝 Changelog":"Added new ingest type for reading object storage records from filesystem (#1325)\nStandardize our buttons (#1382)"}},"/release-notes/release-v2025-5-39":{"title":"Release Notes: v2025.5.39","data":{"":"🔍 Added missing Google app namesThis minor update enhances our Google app integration by including previously omitted app names. This improvement will provide a more comprehensive and accurate representation of Google applications within the RunReveal platform.","changelog#Changelog":"Add missing Google app names (#1383)"}},"/release-notes/release-v2025-5-6":{"title":"runreveal v2025.5.6 Release Notes","data":{"":"🚀 Exciting updates in this release:","new-features#New Features":"➕ Added a not_equal precondition for routing (#1276)\n📋 Steps are now saved when duplicating a pipeline from the topic create wizard (#1275)","bug-fixes#Bug Fixes":"🐛 Fixed various bugs based on frontend errors (#1269)\nThank you for using runreveal! We're constantly working to improve your experience."}},"/release-notes/release-v2025-5-4":{"title":"runreveal v2025.5.4 Release Notes","data":{"":"🚀 This release brings performance improvements and UI enhancements to enhance your experience!","highlights#Highlights":"🏎️ Faster table schema loading: Schema rendering is now optimized to load one table at a time, improving overall performance.\n💾 Improved results view persistence: The application now remembers your results view when returning to a tab.","other-improvements#Other Improvements":"Standardized execution time and rows returned badges for consistency\nAdjusted streaming detection code box to full page width\nDeduplication of detection history by scheduledRunID\nHide date display in results when no eventTime is present\nThank you for using runreveal! We're continually working to improve your experience."}},"/release-notes/release-v2025-5-37":{"title":"Release Notes for v2025.5.37 🚀","data":{"summary#Summary":"This release focuses on improving the precision of timestamp handling for Jamfprotect events, ensuring accurate date and time information.","changes#Changes":"🕰️ Fixed timestamp precision for Jamfprotect events, preserving millisecond accuracy (#1378)"}},"/release-notes/release-v2025-5-36":{"title":"Release Notes for runreveal v2025.5.36","data":{"":"🐛 Bug Fix: Resolved Fatal Error on Empty SlicesThis release addresses a critical issue where accessing indices of empty slices was causing a fatal error. The fix ensures more stable and predictable behavior when working with empty data structures.","changelog#Changelog":"Fixed fatal error occurring when accessing indices of empty slices (#1377)"}},"/release-notes/release-v2025-5-40":{"title":"Release Notes for runreveal v2025.5.40","data":{"-highlights#🚀 Highlights":"We're excited to introduce a more flexible and user-friendly query scheduling system!","-changes#🔧 Changes":"Updated query scheduling to use an interval-based syntax, providing more intuitive and versatile scheduling options\nImplemented UI changes to support the new interval-based scheduling\nAdded a jitter of ±3 minutes to scheduled queries for improved load distribution\nWe hope these changes enhance your experience with runreveal. Happy querying! 📊"}},"/release-notes/release-v2025-5-21":{"title":"Release Notes: v2025.5.21 🚀","data":{"highlights#Highlights":"🔍 Improved detection sync workflow\n🔢 Enhanced value normalization\n🌟 Better explore page navigation","changelog#Changelog":"Implement new detection sync workflow (#1309)\nRemove omitzero from normalized values (#1315)\nForce new tab to set as active when going to explore page (#1312)"}},"/release-notes/release-v2025-6-1":{"title":"Release Notes: v2025.6.1 🚀","data":{"highlights#Highlights":"🔍 Improved column display based on field chooser (#1384)\n🛡️ Enhanced error handling for Sophos integration (#1379)","changelog#Changelog":"Correctly display columns based on field chooser (#1384)\nBetter error display on sophos (#1379)\nReadd nsg flow queue (#1385)\nfix/auth: Create Operator Role for Workspaces to Manage Sources but not Team Members (#1351)"}},"/release-notes/release-v2025-6-10":{"title":"Release Notes for runreveal v2025.6.10 🚀","data":{"whats-new#What's New":"🔧 Enhanced database migrations: Now using \"create or replace view\" for smoother updates\n📧 Improved handling of migration failure emails for better error reporting\nThese changes aim to improve the reliability of database updates and provide clearer communication in case of any issues during the migration process.","changelog#Changelog":"Update migrations to use create or replace view and fix migration failure email (#1447)"}},"/release-notes/release-v2025-5-9":{"title":"Release Notes for runreveal v2025.5.9","data":{"":"🐛 Bug Fix: JumpCloud Service Field UnmarshalThis release addresses an issue with unmarshaling the JumpCloud service field, ensuring smoother data processing and integration with JumpCloud services.","changelog#Changelog":"Fix jumpcloud service field unmarshal (#1278)"}},"/release-notes/release-v2025-5-8":{"title":"Release Notes for runreveal v2025.5.8 🚀","data":{"highlights#Highlights":"🔥 Multiple S3 Destinations: You can now set up and write to multiple S3 destinations in your pipelines, offering greater flexibility in data management.","improvements#Improvements":"Added a pipeline step for specifying S3 destinations\nEnhanced the UI to support multiple destinations\nImproved handling of Clickhouse destination settings","ui-enhancements#UI Enhancements":"Removed toast notification from pipelines UI when creating a new topic\nFor more details, please check our GitHub repository."}},"/release-notes/release-v2025-5-7":{"title":"Release Notes - runreveal v2025.5.7","data":{"":"🔒 Improved Workspace Security ManagementThis release focuses on enhancing the handling of workspace secrets and session timeouts:\nFixed an issue with saving workspace secrets\nImproved the process of setting and displaying session timeouts\nUpdated the default workspace session timeout\nThese changes ensure better security management and a more accurate representation of session settings for users.","changelog#Changelog":"Correctly set workspace secrets when updating session timeout (#1277)"}},"/release-notes/release-v2025-6-13":{"title":"Release v2025.6.13 🚀","data":{"highlights#Highlights":"🔧 Improved database performance and stability with PostgreSQL connection pooling\n🐛 Fixed migration errors and type marshaling issues","full-changelog#Full Changelog":"Implemented PostgreSQL connection pooling to improve resource management and fix connection sharing issues\nFixed migration errors\nCorrectly marshal types.Duration into string\nRemoved run query on SQL load"}},"/release-notes/release-v2025-6-11":{"title":"runreveal v2025.6.11 Release Notes","data":{"-highlights#🚀 Highlights":"Improved error logging for better debugging\nEnhanced history functionality using sourceCtx","-changelog#📋 Changelog":"Added better logging for debugging purposes (#1454)\nUpdated history feature to use sourceCtx instead of timeline list (#1448)"}},"/release-notes/release-v2025-6-16":{"title":"Release Notes for runreveal v2025.6.16","data":{"":"🔧 Bug Fix: Close LeakThis release addresses a resource leak, improving overall system stability and performance.","changelog#Changelog":"Close leak (#1467)"}},"/release-notes/release-v2025-6-18":{"title":"Release Notes - v2025.6.18","data":{"-highlights#🎉 Highlights":"Enhanced precondition support: Topics and pipeline steps now allow multiple preconditions, with an improved UI for creating and editing them.\nImproved query aggregation: Fixed issues with aggregated queries for timeseries and grouped queries.","-fixes-and-improvements#🔧 Fixes and Improvements":"Fixed detection timestamps\nAdded Bitwarden logs to grant list\nSet skip location enrichment for all make targets","-new-features#💡 New Features":"PreconditionSelector now supports multiple preconditions:\nEach precondition has its own card with individual controls\nAdd/remove preconditions with +/- buttons\nLogical AND indication for multiple preconditions\nBackward compatibility maintained with singlePreconditionMode prop\nFor more details, please check the full changelog."}},"/release-notes/release-v2025-6-12":{"title":"Release Notes for runreveal v2025.6.12 🚀","data":{"whats-new#What's New":"🔍 Enhanced Debugging: We've added more logs to help debug initialization issues with rrsch and destinations (#1455)","detailed-changes#Detailed Changes":"Added more logs to assist in debugging rrsch and destination initialization processes (#1455)"}},"/release-notes/release-v2025-6-14":{"title":"runreveal v2025.6.14 Release Notes","data":{"":"🔒 Improved PostgreSQL Lock HandlingThis release includes:\nEnhanced PostgreSQL lock management to ensure random locks stay within defined boundaries (#1461)\nAdded IP location data for Hubspot contacts and integrated ipdb into server configuration (#1416)\nFor more details, please check the full changelog."}},"/release-notes/release-v2025-6-20":{"title":"Release Notes for runreveal v2025.6.20","data":{"-bug-fix#🐛 Bug Fix":"Fixed a nil pointer issue when all custom ClickHouse destinations fail to initialize (#1489)\nThis release addresses a critical bug that could cause a nil pointer error in the log router when custom ClickHouse destinations were unable to initialize properly."}},"/release-notes/release-v2025-6-2":{"title":"Release Notes v2025.6.2 🚀","data":{"highlights#Highlights":"🔧 Introducing SQL Migration Tool: Streamlined database management across destinations, Postgres, and ClickHouse.\n📊 Enhanced Data Visualization: New group by option for data volume dashboard and improved array filtering capabilities.","features-and-improvements#Features and Improvements":"Added SQL Migration Tool with support for multiple migration readers (S3, HTTP) (#1104)\nIntroduced group by option for data volume dashboard (#1390)\nEnhanced filtering capabilities for array columns and JSON arrays (#1391, #1386)\nAdded JumpCloud directory logs view for ClickHouse (#1367)\nImplemented various UI improvements (#1387)","bug-fixes#Bug Fixes":"Fixed dashboard crashing when setting detections in context (#1388)\nResolved destination page loading issues (#1104)","other-changes#Other Changes":"Removed results from state (#1380)\nUpdated migration processes and configurations (#1104)"}},"/release-notes/release-v2025-6-24":{"title":"Release Notes for runreveal v2025.6.24","data":{"":"🐛 Bug Fix: Improved GitHub Logs Actor ExtractionThis release addresses an issue with extracting actor information from GitHub logs. The update prioritizes the use of \"user\" and \"user_id\" fields over \"actor\" and \"actor_id\" fields when available, ensuring more accurate actor identification across various GitHub events.","changes#Changes:":"Fixed actor extraction in GitHub logs to handle events using different field names for user information\nUpdated GitHub polling source to align with recent object storage changes\nThis enhancement improves the reliability of actor information retrieval in GitHub log processing."}},"/release-notes/release-v2025-6-21":{"title":"Release Notes: v2025.6.21 🚀","data":{"highlights#Highlights":"🔍 Introduced MCP remote endpoints for database schema introspection\n🖥️ Enhanced UI with improved empty states and loading states","detailed-changes#Detailed Changes":"","mcp-remote-endpoints#MCP: Remote Endpoints":"Added ListTables endpoint to return available database tables\nImplemented GetTableSchema endpoint for detailed column information\nAutomated MCP tool registration via code generation\nImproved separation between RPC handlers and MCP tool definitions","ui-enhancements#UI Enhancements":"Improved empty states for better user experience\nEnhanced loading states for smoother interactions","bug-fixes#Bug Fixes":"Resolved issue with duplicate /metrics route registration in queue service"}},"/release-notes/release-v2025-6-23":{"title":"Release Notes for v2025.6.23","data":{"":"🎉 Exciting new features and improvements in this release!","highlights#Highlights":"🔔 Added Opal.dev event streaming webhook source with HMAC verification\n🚀 Improved performance for checking missing migrations","full-changelog#Full Changelog":"","new-features#New Features":"Added comprehensive Opal.dev event streaming webhook source integration\nImplements HMAC-SHA256 signature verification\nSupports comma-delimited signatures for key rotation\nIncludes ClickHouse database view for optimized event querying\nAdds UI modal for configuring HMAC secret","improvements#Improvements":"Enhanced performance when checking for missing database migrations","bug-fixes#Bug Fixes":"Stopped storing actor name in email for improved data privacy"}},"/release-notes/release-v2025-6-15":{"title":"Release Notes for runreveal v2025.6.15","data":{"":"🔧 Bug Fix: Resolved issue with readonly fields for Hubspot integration (RUN-850)This minor release addresses a specific problem related to the Hubspot integration, ensuring proper handling of readonly fields.For more details, please refer to the full changelog."}},"/release-notes/release-v2025-6-22":{"title":"Release Notes for runreveal v2025.6.22","data":{"":"🔄 Reverted: MCP remote endpointsThis release reverts the previously implemented MCP remote endpoints functionality. The change has been rolled back to ensure system stability and performance.For more details, please refer to the full changelog."}},"/release-notes/release-v2025-6-26":{"title":"Release Notes for v2025.6.26","data":{"":"🚀 Highlights\nIntroduced Remote MCP (Model Control Protocol) endpoints for enhanced database management\nFixed login pages to work correctly in dark mode\nResolved app crash issue when editing Sophos source","detailed-changes#Detailed Changes":"","remote-mcp-enhancements-#Remote MCP Enhancements 🔧":"Implemented new MCP tools for database schema introspection\nAdded ListTables endpoint to return available database tables\nIntroduced GetTableSchema endpoint for detailed column information\nImproved automatic MCP tool registration via code generation","ui-improvements-#UI Improvements 🎨":"Fixed login pages to ensure proper functionality in dark mode","bug-fixes-#Bug Fixes 🐛":"Resolved an issue causing the app to crash when editing Sophos source\nFixed database connection leaks that were causing test timeouts\nImproved transaction rollback error handling for cleaner logs","infrastructure-updates-️#Infrastructure Updates 🏗️":"Fixed API server binding to use default port 8000 when no ListenAddr is specified\nIncreased maximum Postgres connections for tests\nEnhanced ClickHouse connection pool limits for better performance under high load"}},"/release-notes/release-v2025-6-27":{"title":"Release Notes - v2025.6.27","data":{"":"🔐 Enhanced OAuth AuthenticationThis release introduces fallback cookie support for OAuth authentication, improving the user experience for MCP and other OAuth applications.","whats-new#What's New":"Added fallback cookie support for OAuth authentication (#1508)\nSimplifies URL input for customers\nImproves compatibility with various deployment scenarios"}},"/release-notes/release-v2025-6-29":{"title":"Release Notes - v2025.6.29","data":{"":"🚀 This release focuses on improving the Tailscale integration and enhancing the user experience with time range selection in graphs.","whats-new#What's New":"🛠️ Fixed an issue where Tailscale verify was creating empty sources (#1520)\n📊 Added the ability to change time ranges by selecting directly in graphs (#1519)","detailed-changes#Detailed Changes":"Fix tailscale verify creating empty source (#1520)\nAllow the ability to change time ranges by selecting in graph (#1519)"}},"/release-notes/release-v2025-6-17":{"title":"Release Notes for runreveal v2025.6.17","data":{"":"🔐 New Bitwarden Source AddedThis release introduces a new Bitwarden source, allowing users to integrate their Bitwarden password manager data with runreveal.","changes#Changes":"Created Bitwarden source\nFixed icon and verification process\nUpdated database instructions in claude.md"}},"/release-notes/release-v2025-6-25":{"title":"Release Notes for runreveal v2025.6.25 🚀","data":{"highlights#Highlights":"🔧 Improved Opal Source functionality and fixed issues with source editing","changes#Changes":"Added Opal to the list of automatic grants\nFixed source edit functionality\nMade tweaks to Opal Source\nFor more details, please check the full changelog on GitHub."}},"/release-notes/release-v2025-6-3":{"title":"Release Notes - runreveal v2025.6.3","data":{"":"🛠️ SQL Migration EnhancementsThis release focuses on improving SQL migration processes:\nAdded version to role grants\nFixed state file reading\nImproved handling of destination backup tables\n📊 Data volume improvementsFor more details, please check the full changelog below.","changelog#Changelog":"SQL Migration fixes (#1397)\nData volume improvements (#1392)"}},"/release-notes/release-v2025-6-28":{"title":"Release Notes - v2025.6.28","data":{"":"🔐 Improved Authentication and UI Enhancements","whats-new#What's New":"Fixed Sophos token authentication issue (#1516)\nVarious UI updates for improved user experience (#1514)\nDashboard history now displays correct names (#1512)"}},"/release-notes/release-v2025-6-32":{"title":"Release v2025.6.32 🚀","data":{"highlights#Highlights":"🕒 Improved cron scheduling reliability\n🔐 Enhanced Okta security logging\n📚 Added OpenAPI specification generation","changelog#Changelog":"Removed cron negative jitter so times are always in the future (#1528)\nReplace view to include all of outcome and security context for Okta (#1522)\nAdd OpenAPI specification generation (#1468)"}},"/release-notes/release-v2025-6-35":{"title":"Release Notes for runreveal v2025.6.35 🚀","data":{"whats-new#What's New":"🧠 Enhanced Agent Transparency: All auto-generated agent tools now include a reasoning parameter, providing users with greater insight into agent decision-making processes.","highlights#Highlights":"Added reasoning parameter to all tools generated by toolgen\nUpdated tool execution handlers to process reasoning information\nImproved documentation for cmd/agent standalone example\nReasoning will be displayed in the frontend chat interface for better user understanding\nThis update enhances the explainability of agent actions, allowing users to better comprehend why specific tools are chosen and used."}},"/release-notes/release-v2025-6-30":{"title":"Release Notes: v2025.6.30 🚀","data":{"highlights#Highlights":"🖼️ Updated logos and styling for Remote MCP\n🔍 Added linting command for detection rules","changes#Changes":"Remote MCP: Updated logos and allowed multiple clients with duplicate client names\nFixed bugs related to remote MCP\nAdded a new linting command for detection rules"}},"/release-notes/release-v2025-6-33":{"title":"Release Notes: v2025.6.33 🚀","data":{"summary#Summary":"This release brings significant improvements to the RunReveal CLI and API, enhancing user experience and functionality.","whats-new#What's New":"🛠️ Enhanced CLI: Added numerous autogenerated CLI commands, providing more powerful and flexible command-line interactions.\n🔄 API Upgrades: Many API endpoints have been converted to an RPC framework, improving efficiency and consistency.\n📊 Alert History Improvements: Updates to the alert history feature, ensuring a seamless experience in the dashboard.\nWe're excited to bring you these improvements and look forward to your feedback!"}},"/release-notes/release-v2025-6-34":{"title":"Release Notes for v2025.6.34","data":{"-highlights#🎉 Highlights":"Introducing Sisi: Our new integrated chat system with AI capabilities (#1555)\nUI improvements for better user experience (#1547)","-new-features#🚀 New Features":"Added integrated chat system with PostgreSQL database storage (#1555)\nWorkspace-scoped chats with user tracking\nPersistent chat storage and history\nReal-time message polling and detailed chat logs\nAI agent integration with tool registry\nImplemented UI improvements (#1547)\nAdded visual indicators to differentiate alerts and signals in AlertCard2 (#1534)","-bug-fixes#🐛 Bug Fixes":"Resolved panic in normalizeReflectValue for nil pointer handling (#1542)"}},"/release-notes/release-v2025-6-31":{"title":"Release Notes - runreveal v2025.6.31","data":{"":"🛠️ Bug Fix: Schema RetrievalThis release includes:\nFixed an issue with getting schema\nAdded regression test to prevent future occurrences\nRemoved outdated tags check\nFor more details, please refer to the full changelog.","changelog#Changelog":"fix get schema + regression test (#1524)"}},"/release-notes/release-v2025-6-36":{"title":"Release Notes for v2025.6.36 🚀","data":{"highlights#Highlights":"🔄 Support for asynchronous webhooks added to generic webhook source (#1527)\n🧠 New reasoning parameter for all auto-generated agent tools (#1557)","detailed-changes#Detailed Changes":"Added support for async webhooks in generic webhook source\nImplemented webhook writer lambda\nUpdated objectstoragereader to support async webhooks\nRefactored common event parsing logic for webhooks\nAdded reasoning parameter to all auto-generated agent tools"}},"/release-notes/release-v2025-6-37":{"title":"Release Notes for RunReveal v2025.6.37 🚀","data":{"highlights#Highlights":"🔍 Enhanced Audit Logging: Implemented global RPC audit logging with a readonly flag and UI filtering (#1575)","changelog#Changelog":"Implemented global RPC audit logging with readonly flag and UI filtering (#1575)\nWebhook-lambda: Choose bucket name by environment (#1580)\nRefactored: Migrated API types from types/ to api/models/ package (#1573)"}},"/release-notes/release-v2025-6-41":{"title":"Release Notes for runreveal v2025.6.41","data":{"":"🔄 Reverted default Zendesk sleepThis release reverts changes to the default Zendesk sleep setting, ensuring optimal performance and responsiveness in Zendesk-related operations.","changelog#Changelog":"Revert default zendesk sleep (#1593)"}},"/release-notes/release-v2025-6-4":{"title":"Release Notes - v2025.6.4","data":{"-highlights#🚀 Highlights":"Count number of app errors in Google Workspace (#1402)\nImproved data volume display on admin portal (#1400)","-enhancements#🔧 Enhancements":"Pull out all values for actor on filter (#1401)\nMinor volume improvements (#1399)\nFixed minor bugs on the data volume page (#1398)"}},"/release-notes/release-v2025-6-39":{"title":"Release Notes for runreveal v2025.6.39","data":{"":"🐛 Fixed nil pointer panics during database migrations initialization (#1589)✨ Added search filter functionality to the sources list page (#1579)","changelog#Changelog":"migrations: fix nil pointer panics during init (#1589)\nfeat: add search filter to sources list page (#1579)"}},"/release-notes/release-v2025-6-42":{"title":"Release Notes - v2025.6.42","data":{"":"🚀 This release brings some exciting improvements to runreveal:","highlights#Highlights":"🔄 Automatic unzipping on package updates for smoother migrations\n💬 Enhanced chat interface for a more pleasant user experience","changes#Changes":"Migrations: Auto unzip on package update, if they happen to update not via AMI\nMake chat pretty\nCopy whole URL in alerts"}},"/release-notes/release-v2025-6-43":{"title":"Release Notes: v2025.6.43 🚀","data":{"highlights#Highlights":"🔍 Added cursor-based pagination to ListDestinations API\n🎨 UI improvements for chat functionality","detailed-changes#Detailed Changes":"Implemented cursor-based pagination for ListDestinations API (#1567)\nVarious UI fixes and improvements for chat interface (#1603, #1602, #1596)\nReverted some recent UI changes (#1601)"}},"/release-notes/release-v2025-6-40":{"title":"Release Notes for v2025.6.40 🚀","data":{"highlights#Highlights":"🔍 Fixed Zendesk duplicates using cursor pagination (#1578)\n💡 Added tooltip for running queries (#1585)","other-improvements#Other Improvements":"Fixed optional chaining for map notifications (#1592)\nResolved Makefile issue with global exports overriding local.env (#1537)"}},"/release-notes/release-v2025-6-5":{"title":"Release Notes for v2025.6.5 🚀","data":{"highlights#Highlights":"🔧 Fixed column mapping issue for tables with Map(string) columns\n🔐 Added privacy policy to signup page and integrated Hubspot contact creation","changes#Changes":"Fixed column mapping issue for tables with Map(string) columns (#1413)\nAdded privacy policy to signup page and created Hubspot contact on user creation (#1405)\nFixed jolty data volume indicator (#1415)\nFixed results rerendering when columns change (#1410)\nFixed Grapher to return empty array if queryResponse.rows is null (#1407)\nSet select graph default (#1406)\nImproved styling for alert pages (#1403)\nUpdated transform page UI with fewer borders (#1404)\nFixed build process to pass new env var to rrd container (#1408)"}},"/release-notes/release-v2025-6-6":{"title":"Release Notes for v2025.6.6","data":{"":"🐛 Bug Fix: CLI Stability ImprovementThis release focuses on enhancing the stability of the RunReveal CLI:\nFixed an issue that caused the CLI to panic when not logged in (#1422)\nUpdated alert key value text for better contrast (#1419)\nWe've also added a new GitHub action for Claude code integration (#1420).Thank you for using RunReveal!"}},"/release-notes/release-v2025-6-44":{"title":"Release Notes - v2025.6.44 🚀","data":{"highlights#Highlights":"🤖 Introducing AI Chat Navigation! We're excited to unveil our new AI-powered chat feature, now accessible from the sidebar.","whats-new#What's New":"📢 Updated promo banner to announce the new AI chat feature\n🧠 Enabled AI chat navigation in the sidebar\n🆕 Moved the \"New\" badge from Pipelines to Chat\n⚙️ Added OpenAI configuration options for Chat functionality\nWe hope you enjoy these new features and improvements! As always, we appreciate your feedback."}},"/release-notes/release-v2025-6-45":{"title":"Release v2025.6.45 🚀","data":{"highlights#Highlights":"🛠️ Fixed a critical issue preventing nil pointer panics when handling compressed data\n🔔 Added support for Jamf Protect webhooks\n🎨 Improved chat UI with adjusted colors and markdown table support","full-changelog#Full Changelog":"fix: prevent nil pointer panic when gzip/zstd reader creation fails (#1644)\nfeat/jamfprotect: add support for webhooks (#1619)\nfix/openapi: drop admin paths from generator (#1634)\nAdjusted chat colors and added md table support (#1623)\nfeat/history: apply filter to baseline query (#1513)\nImprove AI provider warnings in chat UI (#1615)\nfeat/cli/detections: allow testing stream detections (#1595)"}},"/release-notes/release-v2025-7-1":{"title":"Release Notes for runreveal v2025.7.1 🚀","data":{"whats-new#What's New":"🔇 Topic Management Enhancement: We've added the ability to disable topics, giving you more control over your data streams. This feature allows for better resource management and streamlined workflows.","changelog#Changelog":"Add disable functionality to topics\nIntegrate topic enable/disable into the regular topic update process\nImprove test coverage for topic management"}},"/release-notes/release-v2025-6-7":{"title":"RunReveal v2025.6.7 Release Notes","data":{"-highlights#🚀 Highlights":"This release brings improvements to the user interface and adds support for new integrations.","-changes#🔧 Changes":"📑 Truncate tab titles correctly and display full title on hover (#1425)\n🛡️ Updated view for SentinelOne activity logs (#1418)\n🔐 Added support for Sophos OAuth authorization (#1423)"}},"/release-notes/release-v2025-7-12":{"title":"Release Notes for runreveal v2025.7.12","data":{"highlights#Highlights":"🎉 Introducing a new welcome workflow for organization admins! This update streamlines the process of creating a new organization and managing users.","changes#Changes":"Build welcome org workflow for admins (#1581)\nImproved flow for creating a new organization and users\nUpdated email logo\nEnhanced sharing experience (#1677)\nShare link now only displays after the sharing action is completed"}},"/release-notes/release-v2025-7-13":{"title":"Release v2025.7.13 🚀","data":{"highlights#Highlights":"🔧 Improved webhooks with ALBRequest parsing refactor and gzip compression support\n🤖 Added support for Gemini-2.5 Pro model in chat API","changelog#Changelog":"Refactored ALBRequest parsing and added support for gzip compression in webhooks (#1678)\nAdded support for gemini-2.5 pro model to chat in API"}},"/release-notes/release-v2025-7-15":{"title":"Release Notes: v2025.7.15 🚀","data":{"highlights#Highlights":"🔔 Improved Alerts Sidebar\nEnhanced styling and routing for the new alerts page\nFixed share button background\n🔍 Enhanced Data Representation\nFlattened normalized fields in streaming detections for better visibility in views and logs table","other-changes#Other Changes":"Updated SecurityAlertRender filename location\nFixed queries for timeline\nRe-added missing types and some styling adjustments"}},"/release-notes/release-v2025-6-9":{"title":"Release Notes for v2025.6.9","data":{"-highlights#🚀 Highlights":"Improved performance: ClickHouse grant migrations now run in the background\nBug fix: Detection history detail pane now clears when switching workspaces","-changes#🔧 Changes":"Moved ClickHouse grant migrations to separate functions\nReplaced CRC32 lock ID generation with more secure math/rand implementation\nAdded success state saving for migrations\nFixed issue with detection history detail pane not clearing on workspace switch"}},"/release-notes/release-v2025-7-14":{"title":"RunReveal v2025.7.14 Release Notes","data":{"":"🚀 Highlights:\nAdded Gemini chat support to frontend\nImproved handling of SentinelOne API errors","whats-new#What's New":"🤖 Gemini chat support added to the frontend\n🔧 Fixed chat input in existing chats when Gemini is configured\n🛠️ Improved error handling for SentinelOne API 500 errors\n📊 Enhanced flattening of normalized fields in streaming detections","other-improvements#Other Improvements":"Increased node memory limit in GitHub Action"}},"/release-notes/release-v2025-6-8":{"title":"RunReveal v2025.6.8 Release Notes","data":{"summary#Summary":"🚀 This release introduces support for Cloudflare Zero Trust Access and Network Logs sources, expanding RunReveal's data ingestion capabilities.","new-features#New Features":"","️-cloudflare-zero-trust-access-source#🛡️ Cloudflare Zero Trust Access Source":"Added support for ingesting and parsing Cloudflare Zero Trust Access logs\nImplemented object storage source parser with JSON log processing\nCreated ClickHouse view migration for cf_zt_access_logs\nAdded frontend React components for source configuration","-cloudflare-zero-trust-network-logs-source#🌐 Cloudflare Zero Trust Network Logs Source":"Introduced support for Cloudflare Zero Trust Network Logs\nExpanded data ingestion capabilities to include network-level insights","improvements#Improvements":"Updated routing for add/edit source pages\nIncluded comprehensive test data and blob test entries\nEnhanced example configuration with new source types"}},"/release-notes/release-v2025-7-2":{"title":"runreveal v2025.7.2 Release Notes","data":{"":"🚀 This release brings minor improvements and bug fixes to enhance your experience with runreveal.","highlights#Highlights":"🖥️ Improved responsive design for sources page on small screens\n🛠️ Enhanced OpenAPI generation with RPCResponse implementation","changelog#Changelog":"fix: improve responsive design for sources page on small screens (#1651)\nfeat/openapigen: make use RPCResponse (#1639)"}},"/release-notes/release-v2025-7-3":{"title":"RunReveal v2025.7.3 Release Notes","data":{"":"🚀 This release brings improvements to Azure flow log processing and MITRE ATT&CK integration!","whats-new#What's New":"🧠 Optimized Azure flow log file reading for better memory management (#1657)\n🛡️ Improved MITRE ATT&CK techniques integration in filters state (#1645)\n🔧 Simplified OpenAPI reflection (#1642)"}},"/release-notes/release-v2025-7-6":{"title":"Release Notes for v2025.7.6 🚀","data":{"whats-new#What's New":"🔍 Enhanced Cloudflare Query Performance\nAdded createdAt to Cloudflare queries to improve indexing and query efficiency (#1669)\nThis update addresses a specific Clickhouse setting requirement, ensuring smoother data retrieval for Cloudflare-related operations."}},"/release-notes/release-v2025-7-7":{"title":"Release Notes for runreveal v2025.7.7 🚀","data":{"summary#Summary":"This release enhances our webhook handling capabilities, improving performance and reliability across multiple integrations. 🌐","whats-new#What's New":"🔄 Converted remaining webhook sources to support the ParseFile interface (#1616)\nImproved async support for various integrations including:\nGeneric webhook\nStructured webhook\nGitHub webhook\nJamf\nRunReveal audit\nTailscale flow\nObsidian\nCloudentity\nNotion\nOpal\nAuth0\nCyberhaven\nKeeper\nGCP webhook\nAzure Entra webhook\nAzure Activity webhook\nEnhanced error handling for generic webhooks\nFixed test data for Palo Alto Panorama traffic\nThese changes should result in more efficient processing of webhook data and improved overall system performance."}},"/release-notes/release-v2025-7-8":{"title":"Release Notes for runreveal v2025.7.8","data":{"":"🔍 Improved Log Retrieval and PaginationThis release brings enhancements to our log management system:\nFixed an issue with cursor pagination, ensuring smoother navigation through log entries\nImplemented requesting from the earliest possible logs, providing a more comprehensive view of historical data","changelog#Changelog":"Fixed issue with cursor pagination and also starts requesting from earliest possible logs (#1670)\nStandardized button on detections page (#1665)"}},"/release-notes/release-v2025-7-4":{"title":"runreveal v2025.7.4 Release Notes","data":{"":"🚀 We're excited to announce the latest release of runreveal!","whats-new#What's New":"","enhanced-azure-flow-integration-#Enhanced Azure Flow Integration 🔧":"Improved logging and error handling for Azure Flow, providing better visibility and troubleshooting capabilities for Azure-related operations.\nThis update focuses on enhancing the robustness of our Azure integration, ensuring a smoother experience for users working with Azure workflows."}},"/sources":{"title":"Data Sources - Log Ingestion Methods for RunReveal","data":{"":"In Order to start writing detections looking for threats you first need to send RunReveal your logs.","ingest-method#Ingest Method":"RunReveal sources provide a few different ingestion methods for sending us data. Some sources may provide more than one option when setting it up.","webhook#Webhook":"A webhook source will generate a unique URL that can be used to forward events to. This URL is provided to your app and events are sent to RunReveal and processed.Webhook sources can be the easiest to setup and maintain but are the most prone to errors.\nNetworking issues that may cause HTTP requests to fail can mean events are lost.\nIf given an option, setting up retries for events can give some error handling in these scenarios.\nEvery source is different and retries are not guaranteed to be available.","polling#Polling":"A polling source is the most common ingestion method that RunReveal offers. Polling sources work by making an API call to the source\nand returning events that have occurred.\nThese API calls are usually performed on a 60 second timer, RunReveal stores a token indicating where we left off and requests all new events (some sources may only request a limited amount to reduce the number of events returned).Every polling source is different, but generally RunReveal requires some sort of account identifier and some sort of access credentials to view your logs.\nThese access credentials are stored in RunReveal, in an encrypted format, and used to authenticate to the source on your behalf.\nIf an error occurs such as a network outage or the source is down, RunReveal will be able to pick up where we left off to make sure no logs are missed.","object-storage#Object Storage":"RunReveal offers a few different object storage providers and methods to ingest logs. However, these ingestion types all work in a similar manner allowing you to keep a backup of your log events in a storage bucket that you control.\nSources providing these can range from cloud provider logs, SAAS applications that store logs in a bucket, or event generic buckets that store custom logs.Below you will find links to help setup and explain each of the object storage providers that we support.These ingestion types work by:\nStoring an object containing a number of events in a blob storage container.\nSending an object creation notification to a message queue.\nRunReveal will subscribe to this queue and read new notifications.\nRunReveal will download the object listed in the notification.\nRunReveal will read, process, and ingest the events in the blob object.","aws-s3-bucket#AWS S3 Bucket":"Logs are stored in an AWS S3 bucket that is under your control,\nobject creation notifications are forwarded to one of RunReveal's SNS topics.\nOnce we receive the notification we will download the object and begin processing events.","aws-s3-bucket-with-custom-sqs#AWS S3 Bucket with Custom SQS":"Similar to the regular S3 method, events are stored in a bucket that you control.\nObject notifications are instead sent to an SQS queue that is also in your control.\nRunReveal will subscribe to your SQS queue and process the notifications.","azure-blob-storage#Azure Blob Storage":"Logs are stored in a storage account container in your Azure subscription.\nObject creation notifications are sent to a storage queue where RunReveal will subscribe and process the notifications.","google-cloud-storage-bucket#Google Cloud Storage Bucket":"Logs are stored in a GCS bucket in your GCP account.\nObject creation notifications are sent to a pub/sub topic and RunReveal will subscribe to the topic to process the notifications.","cloudflare-r2-bucket#Cloudflare R2 Bucket":"Logs are stored in an R2 bucket in your Cloudflare account.\nObject creation notifications are sent to a Cloudflare Workers Queue and RunReveal will read from event notifications from the queue\nand process the objects that have been created.","transform#Transform":"Along with the standard setup for the source, you can also select an existing transform that was created to help normalize your fields.Leaving this field blank will use the standard transform for this source. Custom sources (e.g. webhooks or object storage) will not have any transformation applied.","health-checks#Health Checks":"Along with adding your source settings to ingest logs, RunReveal offers the ability to enable health checks on your source.\nA health check will check your source volume every 15 minutes to verify if any logs have been received.\nOnce the threshold limit is reached we will alert the configured notification channels that the source is unhealthy.You can select the duration that you want health checks to alert on and select the frequency that alerts should be sent.\nSelect the notification channels that you would like to be alerted on, if no notification channels are selected the health check will be run, but no notification will be sent."}},"/release-notes/release-v2025-7-16":{"title":"Release Notes for runreveal v2025.7.16","data":{"":"🔧 Bug Fix: Improved key combining logic for flattening normalized fields in Sigma detections","changes#Changes":"Fixed key combining logic when flattening normalized fields in Sigma detections\nChanged asCountry to asCountryCode for improved clarity\nThis release focuses on enhancing the accuracy and consistency of Sigma detection processing."}},"/release-notes/release-v2025-7-5":{"title":"Release Notes for runreveal v2025.7.5","data":{"-whats-new#🚀 What's New":"Introducing Slack as a new data source! 🎉\nImproved OAuth flow for Slack integration\nAdded support for both refresh tokens and long-lived access tokens","details#Details":"Implemented initial version of Slack data source\nAdded slack_logs to query file\nIncluded Slack source in Pro plan offerings\nFixed redirect URL for Slack OAuth process\nThis release enhances our data integration capabilities, allowing users to seamlessly connect and analyze their Slack data within runreveal."}},"/release-notes/release-v2025-6-38":{"title":"RunReveal v2025.6.38 Release Notes","data":{"":"🔍 Audit Log Improvements & UI Enhancements","whats-new#What's New":"🛡️ Fixed an issue where some readonly methods were incorrectly reported as non-readonly in audit logs\n🖱️ Added readonly toggle button visibility in empty state","changelog#Changelog":"audit-logs: some readonly methods were reporting as non-readonly. fixed via allowlist (#1584)\nhotfix: show readonly toggle button in empty state (#1583)"}},"/sources/object-storage/azure":{"title":"Azure","data":{"":"The current process for ingesting logs from Azure blob storage involves 3 steps.\nSend your logs/events to a storage account to be saved as blob storage objects.\nA Blob Created event is triggered and adds a message to a Storage Queue.\nEvery minute, RunReveal will subscribe to your storage queue, pull any new notifications, and process the objects that are listed.\nWhen setting up the Azure resources, each unique source will require its own storage queue in order to process events.RunReveal recommends using a separate storage account for each source but is not required.\nIn order to setup this format you will need to create the following pieces in Azure:\nAdd a new App Registration to your Entra account. (This registraion can be reused between different sources)\nCreate a new Client Secret for this new app.\nCreate a new Storage Account.\nCreate an Event to notify on Blob Created event types and forward the notification to a Storage Queue.\nProvide access to the created app to read blob objects and process storage queue messages.\nFor steps 3-5 RunReveal provides an Azure Deploy Template that can be used to simplify setup.","app-registration#App Registration":"The first step is to create a new App Registration. Navigate to the Microsoft Entra ID service and choose Add -> App Registration.\nGive the app a name and fill in the remaining settings for your organization. Once done click Register.You should now be presented with the details of the new app. Copy and save the Application (client) ID and the Directory (tenant) ID as these will be needed when setting up a RunReveal source.\nIf you plan to use the provided deploy template click on the link for the Managed application in local directory, this will open a properties page where you will need to copy the listed Object ID.","app-secret#App Secret":"Once the app has been registered you will need to create a client secret to provide to RunReveal for access.\nBack on the main app registration page under the Manage category click on 'Certificates & secrets'.\nFrom this screen click on the 'Client secrets' option and select New client secret. Give the secret a name and choose an expiration date.\nOnce created copy the secret value and save it for later, this is needed when creating the source in RunReveal.\nSet a reminder for the expiration date of this secret.\nYou will need to generate a new secret and update your RunReveal source before the expiration date to make sure RunReveal continues to have access.","storage-account-setup#Storage Account Setup":"For each logging type (Activity Logs, Entra, etc) a new storage account must be created with the same setup.\nThe easiest way to set up your storage account is to use our deployment template. This will automatically setup the storage account, the object notifications, and provide the correct permissions for the created app.","deployment-template#Deployment Template":"Click on the to Azure button to load the template in the Azure portal. From there choose your Subscription and Resource group where this storage account should reside.\nGive the storage account a name and paste the App's Object ID that was copied earlier. Wait for the deployment to complete, and click on the Outputs menu item.\nThis will list the storageAccountName and storageQueueName that were created by the template. These values will be needed when setting up the RunReveal source.\nAzure displays multiple App object IDs, if the wrong one is used you may get a deployment error with the template. All of the resources will have been created but the IAM permissions will not have been set.\nYou can manually apply the needed permissions to the storage account if this happens.","manual-setup#Manual Setup":"If you already have logs being stored in a storage account or want to change other setup settings follow these steps.","storage-account#Storage Account":"Navigate to the Storage account creation page and create a new storage account to store your log files.\nThe only settings that we recommend is to select the Standard performance option and to select the Hot access tier.","event-subscription#Event Subscription":"In your new storage account resource, go to the Events menu and create a new Event Subscription.Fill in the name fields for the subscription and system topic name and select Event Grid Schema as the event schema. Change the event types to only filter for Blob Created events.\nSelect Storage Queue as your endpoint type and configure a new queue. For the easiest setup, we recommend putting the storage queue in the same storage account.\nMake sure to take note of the name of the storage queue that was created as this will be needed in the RunReveal source.","iam#IAM":"Go to the access control (IAM) screen for your storage account and add a new Role assignment.\nYou will need to add two separate roles the first being Storage Blob Data Reader and the second Storage Queue Data Message Processor.\nFor both of these the member should be the App that was registered earlier.","runreveal-source-setup#RunReveal Source Setup":"Sources that allow you to use Azure storage accounts to ingest logs will all have the same setup.\nNavigate to the Connect a source page and find the source you are adding.\nOnce open, select the Azure Storage Account ingest method and fill in the fields.You will need the tenant id, client id, and the client secret for the app that was created.\nYou will also need the storage account name and storage queue name where the notifications are being sent.\nThe storage account name that is provided should be the location of the storage queue, not necessarily the location of the blob objects.\nThe storage account of the blob object is listed in the notification that is created.","final-steps#Final Steps":"At this point all of the pieces are in place for RunReveal to access logs stored in the bucket.\nContinue to the source docs for specific instructions on how to get your logs into the storage account."}},"/sources/source-types/1password":{"title":"1Password","data":{"":"1Password logs allow you to view audit events in your 1Password organization, sign in attempts from users in your 1Password, and item usage attempts. RunReveal will backfill the last 7 days of logs when setting up your source, and will poll for new logs every 60 seconds.","setup#Setup":"To setup your 1Password source you will need to create an API token in your 1Password organization.\nFirst you will navigate to your integrations settings in 1Password.\nNext you need to setup an integration, scroll down to Events Reporting and select the option that says other.\nGive the integration a name and click Add Integration.\nNow setup the token that will be used by RunReveal to read your events. Give the token a name and select which event type you want RunReveal to ingest. You can select as many or few event types as you wish.\nCopy the token that is provided and save it for later.\nNavigate to RunReveal and create a new 1Password source.\nGive the source a name and add in the token saved from step 5.\nIn the drop down select the type of account that you have. The account type determines where your 1Password data is stored and changes how it is accessed.\nYour data should start importing within a minute."}},"/sources/object-storage/external-s3":{"title":"External S3","data":{"":"An AWS S3 with custom SQS source works by saving logs to an S3 bucket and creating a put object notification that sends notifications to an SQS queue that you control.\nRunReveal will subscribe to this queue, read the event notifications, access the listed bucket object, and read events from it.This method is useful if you are already sending notifications to another SNS topic, adding your own SQS queue as a subscriber to that topic will allow you to fanout notifications to multiple subscribers.","aws-access-methods#AWS Access Methods":"Along with an S3 bucket where your events are stored, you will also need to create an SQS queue that new object notifications are forwarded to.\nRunReveal will need to be given ListBucket & GetObject permissions on the S3 bucket(s) containing the logs and ReceiveMessage and DeleteMessage permissions on the SQS queue.","custom-iam-role#Custom IAM role":"S3 sources support reading via an IAM Role in your AWS account.At a high level you'll need to create a role and provide that role access to the data you want RunReveal to ingest as well as the SQS queue that we should subscribe to.","creating-the-role#Creating the role":"When you create a source that supports AWS Role based access to the objects, you'll be prompted to provide a role ARN.\nYour role will need to have at least the following permissions, one for reading objects in your S3 bucket and a second for receiving and deleting messages in your SQS queue.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<bucket-name>\",\n                \"arn:aws:s3:::<bucket-name>/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sqs:DeleteMessage\",\n                \"sqs:ReceiveMessage\"\n            ],\n            \"Resource\": [\n                \"arn:aws:sqs:us-east-2:<account-id>:<sqs-queue-name>\"\n            ]\n        },\n        {\n            \"Action\": [\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kms:us-west-2:EXAMPLE_AWS_ACCOUNT:key/1234abcd-12ab-34cd-56ef-1234567890ab\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\nIf your bucket objects are encrypted with an AWS managed AWS key, you don't need the KMS policy.\nIf it's encrypted with a KMS key you created that lives in your account, you'll need to include the KMS policy as well.","secure-access-using-external-id#Secure Access Using External ID":"The external ID configuration helps prevent the confused deputy problem.\nThe trusted entities configuration necessary for RunReveal to access your account looks like this.\nMake sure you fill in the external ID with whatever you set up on your source.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::253602268883:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"<EXTERNAL_ID>\"\n                }\n            }\n        }\n    ]\n}","aws-access-key#AWS Access Key":"Give an AWS user access to Read/List the S3 bucket with your logs in it. Generate a new AWS access key and provide RunReveal with the access key id and secret.RunReveal will use the provided access key to authenticate access to your AWS account when reading S3 objects.","event-notification#Event Notification":"Once RunReveal has access to the objects in your bucket and SQS queue, the queue will need to be notified when new objects are stored.\nEnable sending notifications to your SQS queue either by sending to an SNS topic or directly to the queue.\nFind the bucket containing the logs you wish to send to RunReveal.\nFrom the bucket overview, click the \"Properties\" tab, then scroll down to \"Event Notifications\"\nClick \"Create Event Notification\"\nGive the configuration a name (for your own identifying purposes, doesn't matter what it is).\nSelect \"All object create events\" in the events section (photo below)\nThen input the arn of the SQS queue or SNS topic that notifications will be sent to.","runreveal-source-setup#RunReveal Source Setup":"Sources that allow you to use S3 to ingest logs will all have the same setup.\nNavigate to the Connect a source page and find the source you are adding.\nOnce open, select the AWS S3 Bucket with custom SQS ingest method and fill in the fields.You will need the URL of the SQS queue that events are sent to and the region the queue was created in.\nDepending on the authentication method selected supply the necessary role/external id or access key/secret.","final-steps#Final Steps":"At this point all of the pieces are in place for RunReveal to access logs stored in the bucket.\nContinue to the source docs for specific instructions on how to get your logs into the S3 bucket."}},"/sources/object-storage/s3":{"title":"S3","data":{"":"An AWS S3 source works by saving logs to an S3 bucket and creating a put object notification that sends notifications to an SNS topic that RunReveal controls.\nEach source has its own SNS topic that includes one for multiple different regions, reference the specific source documentation for the exact SNS topic to send to.Once we receive this notification, RunReveal will download the S3 object and process the events.","aws-access-methods#AWS Access Methods":"When using the RunReveal controlled SNS topic we offer three separate authentication methods.","runreveal-allow-policy#RunReveal allow policy":"Provide RunReveal IAM role access to read/list objects in your S3 bucket. Use the following policy to give access.\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Allow-RunReveal-Read\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::253602268883:root\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::YOUR_BUCKET_NAME/*\",\n        \"arn:aws:s3:::YOUR_BUCKET_NAME\"\n      ]\n    }\n  ]\n}","custom-iam-role#Custom IAM role":"S3 sources support reading via an IAM Role in your AWS account. This is so you don't need to worry about fiddling with a bucket policy each time you onboard a new log source.At a high level you'll need to create a role and provide that role access to the data you want RunReveal to ingest.","creating-the-role#Creating the role":"When creating the role, you'll need to provide us with S3 and KMS permissions necessary to read objects from the bucket, and decrypt them.When you create a source that supports AWS Role based access to the objects, you'll be prompted to provide a role ARN. Your role needs to have s3:GetObject, s3:ListBucket, and access to the Resources that are contained in your bucket.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<YOUR_BUCKET>\",\n                \"arn:aws:s3:::<YOUR_BUCKET>/*\"\n            ],\n            \"Effect\": \"Allow\"\n        },\n        {\n            \"Action\": [\n                \"kms:Decrypt\"\n            ],\n            \"Resource\": [\n                \"arn:aws:kms:us-west-2:EXAMPLE_AWS_ACCOUNT:key/1234abcd-12ab-34cd-56ef-1234567890ab\"\n            ],\n            \"Effect\": \"Allow\"\n        }\n    ]\n}\nIf your bucket objects are encrypted with an AWS managed AWS key, you don't need the KMS policy.\nIf it's encrypted with a KMS key you created that lives in your account, you'll need to include the KMS policy as well.","secure-access-using-external-id#Secure Access Using External ID":"The external ID configuration helps prevent the confused deputy problem.\nThe trusted entities configuration necessary for RunReveal to access your account looks like this.\nMake sure you fill in the external ID with whatever you set up on your source.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::253602268883:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"<EXTERNAL_ID>\"\n                }\n            }\n        }\n    ]\n}","aws-access-key#AWS Access Key":"Give an AWS user access to Read/List the S3 bucket with your logs in it. Generate a new AWS access key and provide RunReveal with the access key id and secret.RunReveal will use the provided access key to authenticate access to your AWS account when reading S3 objects.","event-notification#Event Notification":"Once RunReveal has access to the objects in your bucket, we will need to be notified when new objects are added to it.\nEnable sending notifications to one of RunReveal's regional SNS topics by following along below.\nFind the bucket containing the logs you wish to send to RunReveal.\nFrom the bucket overview, click the \"Properties\" tab, then scroll down to \"Event Notifications\"\nClick \"Create Event Notification\"\nGive the configuration a name (for your own identifying purposes, doesn't matter what it is).\nSelect \"All object create events\" in the events section (photo below)\nThen input RunReveal's S3 regional SNS topic ARN (arn:aws:sns:<s3-bucket-region>:253602268883:runreveal_<sourcetype>) under the \"Destinations\" block at the bottom like so.\nWe've created a SNS topic in each region, 1 for each source type. If you run into issues, please contact us at support@runreveal.com. \nFor the exact SNS topic to use reference the documentation for that source.","runreveal-source-setup#RunReveal Source Setup":"Sources that allow you to use S3 to ingest logs will all have the same setup.\nNavigate to the Connect a source page and find the source you are adding.\nOnce open, select the AWS S3 Bucket ingest method and fill in the fields.You will need the name of the bucket that the logs are stored in, currently RunReveal requires buckets to be unique across sources.\nDepending on the authentication method selected supply the necessary role/external id, or access key/secret.","final-steps#Final Steps":"At this point all of the pieces are in place for RunReveal to access logs stored in the bucket.\nContinue to the source docs for specific instructions on how to get your logs into the S3 bucket and the specific SNS topic to use.","datalake-architecture-with-single-s3-bucket#Datalake Architecture with Single S3 Bucket":"For organizations using a datalake architecture, you can configure a single S3 bucket to handle multiple source types by using SQS queues and S3 event notifications with prefixes. This approach allows you to centralize your log storage while maintaining separate processing pipelines for different source types.","architecture-overview#Architecture Overview":"Instead of using RunReveal's SNS topics, this setup uses:\nSQS queues - One per source type in your AWS account\nS3 event notifications - Configured with prefixes to route to specific queues\nRunReveal sources - Connected to individual SQS queues","setup-steps#Setup Steps":"","1-create-sqs-queues#1. Create SQS Queues":"Create a separate SQS queue for each source type you want to ingest. For example:\nrunreveal-cloudtrail-queue\nrunreveal-alb-queue\nrunreveal-vpc-flow-queue","2-configure-sqs-queue-policy#2. Configure SQS Queue Policy":"Each SQS queue needs a policy that allows S3 to publish notifications. Use the following policy template:\n{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"__default_policy_ID\",\n  \"Statement\": [\n    {\n      \"Sid\": \"__owner_statement\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::<Account_id>:root\"\n      },\n      \"Action\": \"SQS:*\",\n      \"Resource\": \"arn:aws:sqs:us-east-2:<Account_id>:<queue_name>\"\n    },\n    {\n      \"Sid\": \"Allow S3 to publish to SQS\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"s3.amazonaws.com\"\n      },\n      \"Action\": \"SQS:*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nReplace <Account_id> with your AWS account ID and update the SQS queue ARN in the resource field to match your queue.","3-configure-s3-event-notifications#3. Configure S3 Event Notifications":"Set up event notifications in your S3 bucket with prefixes to route logs to the appropriate SQS queues:For CloudTrail logs:\nPrefix: AWSLogs/<account_id>/CloudTrail/\nDestination: Your CloudTrail SQS queue\nFor Application Load Balancer logs:\nPrefix: AWSLogs/<account_id>/elasticloadbalancing/\nDestination: Your ALB SQS queue\nFor VPC Flow logs:\nPrefix: AWSLogs/<account_id>/vpcflowlogs/\nDestination: Your VPC Flow SQS queue","4-configure-runreveal-sources#4. Configure RunReveal Sources":"When creating sources in RunReveal:\nSelect the appropriate source type (CloudTrail, ALB, VPC Flow, etc.)\nChoose the SQS ingest method instead of S3\nProvide the SQS queue ARN for that specific source type\nConfigure the necessary IAM permissions for RunReveal to read from the SQS queue","benefits#Benefits":"Centralized storage: All logs in one S3 bucket\nOrganized structure: Prefix-based routing keeps logs organized\nScalable: Easy to add new source types\nCost-effective: Single bucket reduces storage costs\nFlexible: Independent processing pipelines per source type","example-directory-structure#Example Directory Structure":"s3://your-datalake-bucket/\n├── AWSLogs/\n│   ├── 123456789012/\n│   │   ├── CloudTrail/\n│   │   │   └── us-east-1/\n│   │   ├── elasticloadbalancing/\n│   │   └── vpcflowlogs/\n│   │       └── us-east-1/\n│   │           └── 2024/\n│   │               └── 01/\n│   │                   └── 15/\n│   │                       └── eni-12345678_...","troubleshooting#Troubleshooting":"","an-error-occurred-communicating-with-runreveal-please-try-again-in-the-ui-error-when-adding-a-new-source#\"An error occurred communicating with RunReveal, please try again\" in the UI error when adding a new source.":"The S3 bucket is already configured as a source in RunReveal. Each source requires a unique bucket.","access-denied-403-error-in-ui-when-trying-to-verify-settings-bucket-permissions#Access Denied (403) Error in UI when trying to verify settings (bucket permissions)":"IAM permissions are insufficient for RunReveal to access the bucket. Review your bucket policy to ensure that s3:GetObject and s3:ListBucket permissions exist for the bucket.","no-data-ingested-from-s3-bucket-for-a-source#No Data Ingested from s3 bucket for a source":"S3 event notifications are not configured or are misconfigured. Ensure that event notifications are set up per the intructions above and confirm that the SNS topic region matches the region of your S3 bucket."}},"/sources/source-types/atlassian":{"title":"Atlassian Audit Logs","data":{"":"Atlassian audit logs allow you to view the audit events that have occurred in your Atlassian organization.\nTo view more info about audit logs including what types of events are tracked you can view more info on the Atlassian docsIn order to ingest your Atlassian audit logs, you must be an Atlassian Access customer.\nTo see if you already have access navigate to your Atlassian admin panel https://admin.atlassian.com and go to Security -> Audit Log.\nFrom there if you have access you will see your events otherwise you will see a link to signup for access.RunReveal will backfill your audit logs to the last 7 days of events. Once the processor has caught up, RunReveal will import new audit logs roughly every 60 seconds.","setup#Setup":"Give your Atlassian source a descriptive name to help find it later. The two fields we require from your Atlassian account are your Organization ID and an API Key.","atlassian-api-key#Atlassian API Key":"Create an API Key in Atlassian to give RunReveal access to your audit logs. From your Atlassian admin panel,\nnavigate to Settings -> API keys. From here you can create a new API key.Give the new key a name and choose an expiration date. Atlassian allows date no further than 1 year in the future.\nCopy the Organization ID and the API key fields to your RunReveal source.\nMake sure to generate a new API key before the expiration date and update RunReveal with the new key to continue receiving events without disruption.","verify-its-working#Verify Its working":"Once added the source logs should begin flowing within a minute.You can validate we are receiving your logs by running the following SQL query.\nSELECT * FROM runreveal.logs WHERE sourceType = 'atlassian' LIMIT 1"}},"/sources/source-types/aws/alb":{"title":"AWS ALB Logs","data":{"":"AWS ALB distributes incoming application traffic across multiple targets (such as EC2 instances, containers, or IP addresses)\nto improve fault tolerance and scalability. ALB logs, known as access logs, provide details on request and response data,\nincluding client IP, request path, latency, and response status, which help in monitoring traffic patterns, troubleshooting,\nand ensuring security.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_alb","setup#Setup":"","step-1-create-an-s3-bucket#Step 1: Create an S3 Bucket":"Sign in to the AWS Management Console and open the Amazon S3 console.\nClick on \"Create bucket\".\nEnter a unique name for your bucket and select the region.\nConfigure the bucket settings as needed (e.g., versioning, encryption).\nClick \"Create bucket\" to finish.","step-2-configure-bucket-policy#Step 2: Configure Bucket Policy":"In the S3 console, select the bucket you just created.\nGo to the \"Permissions\" tab.\nUnder \"Bucket policy\", click \"Edit\".\nPaste the following policy, replacing {your-bucket-name} with your actual bucket name:\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::elb-account-id:root\"\n},\n\"Action\": \"s3:PutObject\",\n\"Resource\": \"arn:aws:s3:::{your-bucket-name}/AWSLogs/*\"\n}\n]\n}\nReplace elb-account-id with the appropriate ID for your region. You can find the list of IDs here.\nClick \"Save changes\".","step-3-enable-access-logs-for-your-alb#Step 3: Enable Access Logs for Your ALB":"Open the EC2 console and navigate to \"Load Balancers\".\nSelect your Application Load Balancer.\nIn the \"Description\" tab, scroll down to the \"Attributes\" section and click \"Edit\".\nCheck the box next to \"Access logs\".\nSelect the S3 bucket you created earlier from the dropdown.\nOptionally, specify a prefix for the log files.\nClick \"Save\".","step-4-verify-log-delivery#Step 4: Verify Log Delivery":"Wait for a few minutes to allow some traffic to flow through your ALB.\nGo back to the S3 console and open your bucket.\nNavigate to the \"AWSLogs\" folder (or the custom prefix you specified).\nYou should see log files appearing in this location.\nFor more information, refer to the official AWS documentation on ALB access logs."}},"/sources/source-types/aws/cloudtrail":{"title":"AWS CloudTrail","data":{"":"AWS CloudTrail enables governance, compliance, and operational and risk auditing by logging detailed event history of actions taken within your AWS account.\nCloudTrail logs include information on who made API calls,\nwhat resources were affected, when the calls occurred, and the source IP, helping with monitoring, troubleshooting, and security analysis.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_cloudtrail","cloudformation-setup#Cloudformation Setup":"If you don't yet have a trail setup, you'll need to either set up an Organization trail, or a non-organization trail. If you'd like RunReveal to create the trail, and fully set-up the event notification, and access to the bucket, you can use these cloudformation templates.RunReveal can help with this setup process using these two Cloudformation templates.\nOrganization Trail (this will fail if you've not created an organization).\nNon-organization Trail\nOnce the cloudformation runs successfully, provide RunReveal with the name of the bucket, ensure the Role ARN is blank, and click \"Connect\".\nCloudFormation setup automates the creation of the S3 bucket, SNS topic, and event notifications. You only need to enter the bucket name in RunReveal when adding a new CloudTrail source.","manual-setup#Manual Setup":"","step-1-create-an-s3-bucket#Step 1: Create an S3 Bucket":"Sign in to the AWS Management Console and open the Amazon S3 console.\nClick on \"Create bucket\".\nEnter a unique name for your bucket and select the region.\nConfigure the bucket settings as needed (e.g., versioning, encryption).\nClick \"Create bucket\" to finish.","step-2-configure-bucket-policy#Step 2: Configure Bucket Policy":"In the S3 console, select the bucket you just created.\nGo to the \"Permissions\" tab.\nUnder \"Bucket policy\", click \"Edit\".\nPaste the following policy, replacing {your-bucket-name} with your actual bucket name:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AWSCloudTrailAclCheck\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"cloudtrail.amazonaws.com\"\n            },\n            \"Action\": \"s3:GetBucketAcl\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}\"\n        },\n        {\n            \"Sid\": \"AWSCloudTrailWrite\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"cloudtrail.amazonaws.com\"\n            },\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}/AWSLogs/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"s3:x-amz-acl\": \"bucket-owner-full-control\"\n                }\n            }\n        }\n    ]\n}\nClick \"Save changes\".","step-3-create-a-cloudtrail-trail#Step 3: Create a CloudTrail Trail":"Open the CloudTrail console.\nClick on \"Trails\" in the left navigation pane.\nClick \"Create trail\".\nEnter a name for your trail.\nFor \"Storage location\", select \"Create new S3 bucket\" or \"Use existing S3 bucket\" and select the bucket you created earlier.\nConfigure other settings as needed (e.g., log file validation, SNS notification).\nClick \"Next\".","step-4-choose-log-events#Step 4: Choose Log Events":"In the \"Choose log events\" section, select the types of events you want to log.\nFor a comprehensive trail, you can select:\nManagement events\nData events\nInsights events (optional)\nConfigure any additional event settings as needed.\nClick \"Next\".","step-5-review-and-create#Step 5: Review and Create":"Review your trail configuration.\nIf everything looks correct, click \"Create trail\".","step-6-verify-log-delivery#Step 6: Verify Log Delivery":"Wait for some time to allow AWS to generate and deliver logs.\nGo back to the S3 console and open your bucket.\nNavigate to the \"AWSLogs\" folder.\nYou should see a folder structure like: AWSLogs/{your-account-id}/CloudTrail/{region}/year/month/day/\nInside the day folders, you'll find your CloudTrail log files.","troubleshooting#Troubleshooting":"","cloudformation-setup-1#CloudFormation Setup":"Ensure the CloudFormation stack completed successfully without errors.\nConfirm that the S3 bucket, SNS topic, and event notifications were created in your AWS account.\nWhen adding a new CloudTrail source in RunReveal, use the bucket name created by the CloudFormation template. The Role ARN should be left blank unless you have a custom setup.\nCheck that CloudTrail is delivering logs to the S3 bucket (see Step 6 above).\nVerify that the event notifications are present on the bucket and point to the correct SNS topic.","manual-setup-1#Manual Setup":"Double-check that the S3 bucket policy matches the example provided and allows CloudTrail to write logs.\nMake sure the SNS topic exists and uses the correct ARN: arn:aws:sns:<REGION>:253602268883:runreveal_cloudtrail.\nConfirm that event notifications are configured on the S3 bucket to send new log files to the SNS topic.\nEnsure CloudTrail is configured to deliver logs to the correct S3 bucket.\nFor more information, refer to the official AWS documentation on CloudTrail."}},"/sources/source-types/aws/guardduty":{"title":"AWS GuardDuty","data":{"":"AWS GuardDuty is a threat detection service that continuously monitors your AWS accounts, networks, and workloads for malicious or unauthorized behavior. GuardDuty logs provide findings that identify potential security threats such as compromised instances, unusual API calls, reconnaissance activity, and anomalous behavior. These findings help in enhancing cloud security by providing actionable insights for incident response and threat mitigation.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_guardduty","setup#Setup":"","step-1-enable-guardduty#Step 1: Enable GuardDuty":"Sign in to the AWS Management Console and open the GuardDuty console.\nIf you haven't used GuardDuty before, you'll see a \"Get Started\" page. Click \"Enable GuardDuty\".\nIf you've used GuardDuty before, ensure it's enabled for the desired regions.","step-2-create-an-s3-bucket#Step 2: Create an S3 Bucket":"Open the Amazon S3 console.\nClick on \"Create bucket\".\nEnter a unique name for your bucket (e.g., \"my-guardduty-findings-bucket\").\nSelect the region where you want to store your findings.\nConfigure the bucket settings as needed (e.g., versioning, encryption).\nClick \"Create bucket\" to finish.","step-3-configure-bucket-policy#Step 3: Configure Bucket Policy":"In the S3 console, select the bucket you just created.\nGo to the \"Permissions\" tab.\nUnder \"Bucket policy\", click \"Edit\".\nPaste the following policy, replacing {your-bucket-name} with your actual bucket name and {your-account-id} with your AWS account ID:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowGuardDutyToCreateObjects\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"guardduty.amazonaws.com\"\n            },\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceAccount\": \"{your-account-id}\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowGuardDutyToGetBucketLocation\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": \"guardduty.amazonaws.com\"\n            },\n            \"Action\": \"s3:GetBucketLocation\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:SourceAccount\": \"{your-account-id}\"\n                }\n            }\n        }\n    ]\n}\nClick \"Save changes\".","step-4-configure-guardduty-to-send-findings-to-s3#Step 4: Configure GuardDuty to Send Findings to S3":"Return to the GuardDuty console.\nIn the navigation pane, choose \"Settings\".\nIn the \"Findings export options\" section, choose \"Configure now\" (or \"Edit\" if already configured).\nFor \"Frequency for updated findings\", choose your preferred frequency.\nFor \"Choose export method\", select \"Send to S3 bucket\".\nFor \"Bucket name\", enter the name of the S3 bucket you created.\nOptionally, enter a \"KMS encryption key\" if you want to encrypt the exported findings.\nChoose \"Save\".","step-5-verify-findings-export#Step 5: Verify Findings Export":"GuardDuty will now export findings to your S3 bucket at the frequency you specified.\nTo verify, you can go to your S3 bucket and look for new objects. The path will typically be:\nAWSLogs/{your-account-id}/GuardDuty/{region}/year/month/day/\nNote that if you don't have any findings yet, no files will be exported.\nFor more information, refer to the official AWS documentation on GuardDuty."}},"/sources/source-types/azure/activity-logs":{"title":"Azure Activity Logs","data":{"":"Azure Activity Logs provide a record of operations performed on Azure resources, offering insights into control plane actions such as resource creation, modification, and deletion. These logs include details like the user who initiated the action, the timestamp, and the result of the operation. Azure Activity Logs are crucial for auditing changes, monitoring resource management, and ensuring compliance with organizational governance and security policies.","ingest-method#Ingest Method":"Azure activity logs can be ingested using either an Azure storage bucket or pushed to RunReveal using a webhook.Azure storage buckets are inherently cheaper than using the webhook method but logs can be delayed by up to an hour. The webhook ingestion\nimports logs as soon as they are generated, but using event hubs to forward every event can become more expensive if there are lots of logs.\nAzure Blob Storage\nWebhook Ingestion\nAfter creating your storage account and other resources, you will need to setup activity logs to forward to it.On the Activity Log resource page, click on the \"Export Activity Logs\" button.\nOn the diagnostic settings page, add a new diagnostic setting. Give the diagnostic setting a name, choose the categories you wish to include in your events, and select \"Archive to a storage account\" selecting the storage account that was created.","eventhub-creation#EventHub Creation":"The first step in setting up Azure Activity logs is to create an event hub.An event hub needs an event hub namespace, if you don't already have one that you want to use, you will need to create that first.\nFollow the steps in Azure to create a namespace in your resource group giving it a name. This name will be needed when setting up your Azure function.After creating the namespace, create a new event hub inside it. Save the name you give to it as it will be needed later. \nAt this point you can now setup Azure to export logs to your event hubs, but you can also setup different access policies or consumer groups for your event hubs as required by your organization.","export-logs-to-event-hub#Export Logs to Event Hub":"With the event hub in place you can now setup Activity Logs to export to it. Pick and choose which type of log you want imported to RunReveal.On the Activity Log resource page, click on the \"Export Activity Logs\" button.\nOn the diagnostic settings page, add a new diagnostic setting. Give the diagnostic setting a name, choose the categories you wish to include in your events, and select \"Stream to an event hub\" filling in the details with the event hub that was created.","runreveal-source#RunReveal Source":"In order for your RunReveal workspace to accept the log you will first need to create the source.Create a new Azure Activity Log source and select the Webhook ingest method.Once you have created the source, make note of the Webhook URL that was generated. This will be used when setting up your Azure function.","connect-everything-together#Connect Everything Together":"The final step is to create an Azure function that will trigger when new messages are sent to the event hub, and forward them to your RunReveal source. Luckily we have an easy to use template to help with that. Navigate to our GitHub repo to view the source code of the Azure Function getting deployed. Click on the below button to automatically load the template into Azure.To get started fill in the Subscription, resource group, and function name. Select the event hub namespace where the logs are sent and the access policy that will be used to read events. The GitHub repo and branch are used to download the function source code. Keep these default unless you plan to fork the repo to your own GitHub account.If you plan to import Activity Logs, check the \"Enable Activity Log Event Hub Functions\" box to bring up the options for Activity Logs. Fill in the RunReveal webhook URL that was obtained earlier when creating your source. Then select the event hub and consumer group that are being used to store the activity logs.If you plan to Import Entra Logs, check the \"Enable Entra Log Event Hub Functions\" box to bring up the options for Entra Logs. Fill in the RunReveal webhook URL that was obtained earlier when creating your source. Then select the event hub and consumer group that are being used to store the Entra logs.\nWhen filling in the webhooks for Azure Activity Logs and Entra Logs, make sure to use the correct URL for that source. Mixing up the URLs or creating the wrong type of source will cause most if not all of your logs to fail to import.\nAdd any required tags to your setup, then review the settings. Once created Azure will begin the deployment of the function. This may take a few minutes, but when complete logs should begin flowing to RunReveal.\nAzure may take some time before they start to forward events to your Azure function. If a few hours have gone by and you have not seen any logs appear in your RunReveal account reach out to us to for some help."}},"/sources/source-types/aws/dns":{"title":"AWS DNS Logs","data":{"":"AWS VPC DNS provides DNS resolution within a Virtual Private Cloud (VPC), allowing instances to resolve domain names to IP addresses.\nDNS query logs, captured via Amazon Route 53 Resolver query logging, provide insight into DNS queries made by resources in the VPC,\nincluding the domain names requested, source IP, and response codes. This helps with network troubleshooting, security monitoring,\nand identifying potentially malicious domains.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_awsdns","setup#Setup":"","step-1-enable-vpc-dns-query-logging#Step 1: Enable VPC DNS Query Logging":"Sign in to the AWS Management Console and open the Amazon VPC console.\nIn the navigation pane, choose \"Your VPCs\".\nSelect the VPC for which you want to enable DNS query logging.\nChoose \"Actions\" > \"Edit DNS hostnames\".\nSelect \"Enable\" for DNS hostnames if it's not already enabled.\nChoose \"Save changes\".\nNow, choose \"Actions\" > \"Edit DNS resolution\".\nSelect \"Enable\" for DNS resolution if it's not already enabled.\nChoose \"Save changes\".\nFinally, choose \"Actions\" > \"Edit VPC settings\".\nScroll down to \"DNS query logging\" and select \"Enable\".\nFor \"Destination\", choose \"Send to CloudWatch Logs\".\nFor \"Log group name\", enter a name (e.g., /aws/vpc/dns/<your-vpc-id>).\nChoose \"Save changes\".","step-2-create-an-s3-bucket#Step 2: Create an S3 Bucket":"Open the Amazon S3 console.\nClick on \"Create bucket\".\nEnter a unique name for your bucket and select the region.\nConfigure the bucket settings as needed (e.g., versioning, encryption).\nClick \"Create bucket\" to finish.","step-3-create-an-iam-role-for-cloudwatch-logs#Step 3: Create an IAM Role for CloudWatch Logs":"Open the IAM console.\nIn the navigation pane, choose \"Roles\", then \"Create role\".\nFor \"Trusted entity type\", choose \"AWS service\".\nFor \"Use case\", choose \"CloudWatch Logs\" from the dropdown.\nChoose \"Next\".\nIn the \"Add permissions\" page, search for and select \"AWSCloudWatchLogsFullAccess\".\nChoose \"Next\".\nEnter a name for the role (e.g., \"CloudWatchLogsToS3Role\").\nReview the role details and choose \"Create role\".","step-4-create-a-cloudwatch-logs-subscription-filter#Step 4: Create a CloudWatch Logs Subscription Filter":"Open the CloudWatch console.\nIn the navigation pane, choose \"Log groups\".\nFind and select the log group you created for VPC DNS logs.\nChoose \"Subscription filters\" tab, then \"Create subscription filter\".\nFor \"Filter pattern\", enter an empty string to capture all log events.\nFor \"Subscription filter destination\", choose \"Amazon S3 bucket\".\nSelect the S3 bucket you created earlier.\nFor \"Log format\", choose \"JSON\".\nFor \"Compression type\", choose your preferred compression method (e.g., Gzip).\nFor \"IAM role\", select the role you created in Step 3.\nChoose \"Start streaming\".","step-5-verify-log-delivery#Step 5: Verify Log Delivery":"Wait for some time to allow DNS queries to generate logs.\nGo to the S3 console and open your bucket.\nYou should see folders created with the structure: {your-log-group-name}/{year}/{month}/{day}/{hour}/\nInside these folders, you'll find your VPC DNS log files.\nFor more information, refer to the official AWS documentation on VPC DNS Query Logging."}},"/sources/source-types/aws/flow":{"title":"AWS VPC Flow Logs","data":{"":"AWS VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a Virtual Private Cloud (VPC). These logs provide details such as source and destination IP addresses, ports, protocols, traffic direction, and the acceptance or rejection of the traffic. They are useful for network monitoring, troubleshooting connectivity issues, and auditing traffic for security analysis.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_flow","setup#Setup":"","step-1-create-an-s3-bucket#Step 1: Create an S3 Bucket":"Sign in to the AWS Management Console and open the Amazon S3 console.\nClick on \"Create bucket\".\nEnter a unique name for your bucket (e.g., \"my-vpc-flow-logs-bucket\").\nSelect the region where your VPC is located.\nConfigure the bucket settings as needed (e.g., versioning, encryption).\nClick \"Create bucket\" to finish.","step-2-configure-bucket-policy#Step 2: Configure Bucket Policy":"In the S3 console, select the bucket you just created.\nGo to the \"Permissions\" tab.\nUnder \"Bucket policy\", click \"Edit\".\nPaste the following policy, replacing {your-bucket-name} with your actual bucket name:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AWSLogDeliveryWrite\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"delivery.logs.amazonaws.com\"},\n            \"Action\": \"s3:PutObject\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}/AWSLogs/*\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"s3:x-amz-acl\": \"bucket-owner-full-control\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"AWSLogDeliveryAclCheck\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"Service\": \"delivery.logs.amazonaws.com\"},\n            \"Action\": \"s3:GetBucketAcl\",\n            \"Resource\": \"arn:aws:s3:::{your-bucket-name}\"\n        }\n    ]\n}\nClick \"Save changes\".","step-3-create-a-flow-log#Step 3: Create a Flow Log":"Open the Amazon VPC console.\nIn the navigation pane, choose \"Your VPCs\".\nSelect the VPC for which you want to create a flow log.\nChoose the \"Flow logs\" tab.\nChoose \"Create flow log\".\nConfigure the flow log:\nFor \"Filter\", choose \"All\" to capture all traffic.\nFor \"Maximum aggregation interval\", choose your preferred interval.\nFor \"Destination\", select \"Send to an S3 bucket\".\nFor \"S3 bucket ARN\", enter the ARN of the bucket you created (format: arn:aws:s3:::{your-bucket-name}).\nOptionally, specify a \"Log file format\" if you want to customize the fields.\nFor \"Log record format\", choose your preferred format (e.g., AWS default format).\nChoose \"Create flow log\".","step-4-verify-log-delivery#Step 4: Verify Log Delivery":"Wait for some time to allow traffic to generate logs (this may take up to 15 minutes).\nGo to the S3 console and open your bucket.\nYou should see folders created with the structure: AWSLogs/{aws-account-id}/vpcflowlogs/{region}/{year}/{month}/{day}/\nInside these folders, you'll find your VPC Flow Log files.\nFor more information, refer to the official AWS documentation on VPC Flow Logs."}},"/sources/source-types/auth0":{"title":"Auth0","data":{"":"The Auth0 source is a webhook based source that uses their log stream feature.To get started, create an auth0 source in the dashboard. The source should have a webhook URL and a Bearer Token that is\navailable in the sources list.","auth0-configuration#Auth0 Configuration":"Under monitoring > streams, create a new Custom Webhook log stream. Set the webhook URL to the api.runreveal.com URL\nprovided in your auth0 source, and in Auth0 set the Bearer token value to the entirity of the bearer token string provided by\nRunReveal. This should include the word Bearer. Your bearer token string should look something like Bearer xxxxxxxxxxxxxxxxx.Finally, ensure that the stream is configured as a JSON Array and click save after ensuring that all events are being sent.\nLogs should begin flowing to your Auth0 source immediately!"}},"/sources/source-types/aws/hosted-zone":{"title":"AWS Hosted Zone Logs","data":{"":"AWS Hosted Zones are part of Amazon Route 53, a scalable DNS web service that routes end-user requests to applications. DNS query logs capture details about DNS queries made for your hosted zones, including the domain queried, request source, and response provided. These logs help with tracking DNS traffic, debugging DNS resolution issues, and identifying potential security risks, such as DNS-based attacks.Setup for hosted zone logs require three steps,\nConfiguring the hosted zone to log to CloudWatch.\nCreate an IAM role/policy that gives RunReveal access to the CloudWatch log group.\nSetup the RunReveal source and provide it with the created log group and IAM role.","setup#Setup":"If your hosted zone is not already configured to log to CloudWatch, navigate to your hosted zone and click on the Configure query logging button.Either select an existing log group or create a new on giving it a name.\nIf AWS gives a warning about permissions follow their prompt to grant the necessary permissions for Route 53 to write to CloudWatch.After completing the setup you can now view the logging configuration from the hosted zone page. This will provide you with the CloudWatch log group ARN.\nThis ARN will be needed for both your IAM role and when setting up your RunReveal source. Copy the ARN for later leaving off the final :*.","iam-role-and-policy#IAM Role and policy":"Navigate to the AWS IAM page to create a new policy. Use the following JSON to set the necessary permissions to allow RunReveal access to read your log group logs.\nRunReveal only needs logs:DescribeLogStreams and logs:GetLogEvents access to be able to list the log streams and view the events in each one.\nProvide the list of log group arns that this policy applies to. Notice that it has the additional :log-stream:* resource appended to the end.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"VisualEditor0\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"logs:DescribeLogStreams\",\n                \"logs:GetLogEvents\"\n            ],\n            \"Resource\": [\n                \"arn:aws:logs:[region]:[account-id]:log-group:[log-group-name]:log-stream:*\"\n            ]\n        }\n    ]\n}\nOnce the policy is created, create a new IAM role. Use the following JSON policy when setting the trusted entity.\nThis will give the RunReveal account access to your CloudWatch resources. When setting up the RunReveal source we provide an optional\nexternal id that can be used to further verify access. This section can be removed if you do not plan to use external id verification.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::253602268883:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n\t\t\t\"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"[optional-external-id-check]\"\n                }\n            }\n        }\n    ]\n}\nWhen setting the permissions select the policy that was created in the previous step. Give the role a name to identify it by and save it.\nFind the created role and copy the ARN, this will be needed for setting up the RunReveal source.","runreveal-source#RunReveal Source":"Navigate to your RunReveal dashboard to connect a new Hosted Zone source. Provide the source a name,\nthe CloudWatch log group ARN that was copied earlier. The IAM role ARN that was generated when setting up the policy. The generated external ID for the source can be used in your IAM access policy,\nor you can update it with your own if you used a different value in AWS."}},"/sources/object-storage/r2":{"title":"R2","data":{"":"This guide explains how to set up a Cloudflare R2 bucket as a data source for RunReveal. The setup requires two components:\nAn R2 storage bucket that RunReveal can read objects from\nA Cloudflare Workers Queue for event notifications\nNote: You'll need both R2 credentials and a Cloudflare API key for this setup.","prerequisites#Prerequisites":"","1-cloudflare-account-id#1. Cloudflare Account ID":"First, locate your Cloudflare Account ID from the R2 homepage:","2-queue-identifier#2. Queue Identifier":"Create a queue in the Cloudflare Workers Queues dashboard that will handle R2 object notifications. Make note of the queue ID:\nAdditionally, enable HTTP Pull for your queue so that RunReveal is able to read from your queue over HTTP. This is under\nSettings > Add Consumer","authentication-setup#Authentication Setup":"","r2-credentials#R2 Credentials":"Navigate to R2 > Manage R2 API Tokens\nCreate a new R2 token with \"Object Read Only\" permissions\nGrant access to the bucket that RunReveal will read logs from\nSave the Access Key ID and Secret Access Key for use in the source setup later.\nYou must create R2 Access Keys from the R2 dashboard.  Creating an API Token\nwith the \"Workers R2 Storage\" scoped permissions will not give you the\ncorrect Access Key ID and Secret Access Key needed for compatability\nwith S3 that RunReveal requires to ingest logs.","cloudflare-api-token#Cloudflare API Token":"Go to \"Account API Tokens\"\nCreate a new token with Account > Queues > Edit permissions","event-notification-configuration#Event Notification Configuration":"To notify RunReveal when new data is available:\nNavigate to your R2 bucket > Settings > Event notifications\nClick \"Add Notification\"\nSelect your Workers Queue\nEnable \"Creation of a new object\"","creating-your-source#Creating Your Source":"Provide RunReveal with the following information to begin reading data:\nQueue ID\nCloudflare API Token\nR2 Credentials (Access Key ID and Secret Access Key)\nCloudflare Account ID\nRunReveal will automatically begin processing new data as it becomes available in R2.","rate-limits#Rate limits":"Keep in mind that Cloudflare Workers Queues have ratelimits. These ratelimits are defined in Cloudflare's docs.For most use cases this shouldn't be an issue. However, sharing Workers API keys between sources, or writing lots of small objects can cause these limits to be reached."}},"/sources/source-types/azure/azure-flow":{"title":"Azure Flow Logs","data":{"":"Previously known as NSG Flow logs, Microsoft deprecated NSG flow logs on June 30, 2025 and plans to retire them on September 30, 2027.\nIn order to migrate to the new Azure Flow (also known as Virtual Net Flow) logs please follow Microsoft's documentation for migration, https://learn.microsoft.com/en-us/azure/network-watcher/nsg-flow-logs-migrate.\nVirtual net flow logs control inbound and outbound traffic to and from Azure resources by defining security rules based on IP address, port, and protocol. Virtual net flow logs capture information about network traffic, including source and destination IP addresses, ports, protocols, and whether traffic was allowed or denied. These logs are useful for monitoring network traffic, troubleshooting connectivity issues, and enhancing network security by detecting suspicious or unauthorized traffic patterns.","ingest-method#Ingest Method":"Setup the ingestion of this source using the following guide.Azure Blob Storage","virtual-net-flow-log-forwarding#Virtual Net Flow Log Forwarding":"With the storage account created you can now setup Virtual net flow logs to export to it.Follow along with Microsoft's documentation on the exact steps required to create a new flow log.\nhttps://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-manage?tabs=portal#create-a-flow-log","runreveal-source#RunReveal Source":"Go to RunReveal and add a new source selecting Azure Flow Logs.Give the source a name and fill in the remaining fields with the saved values from setup.You will need the values that were saved from the setup steps.\nThe app Tenant ID and Client ID from the app registration screen.\nThe Client Secret Value that was created when generating a new secret for the app.\nThe Storage Account Name where the logs are exporting to.\nThe Storage Queue Name that holds the blob created notifications.\nOnce these are supplied and saved, RunReveal will begin to process messages in the queue and then ingest logs stored in the bucket.\nBecause of how Microsoft exports flow logs to storage accounts, logs are usually delayed until the top of the next hour."}},"/sources/source-types/azure/entra":{"title":"Azure Entra Logs","data":{"":"Azure Entra is Microsoft's identity and access management solution, encompassing services like Azure Active Directory (Azure AD). Entra logs capture identity-related activities such as user sign-ins, access attempts, multi-factor authentication (MFA) events, and directory changes. These logs help monitor authentication activity, track access to applications and resources, and investigate potential security incidents like unauthorized access or account compromise.","ingest-method#Ingest Method":"Azure Entra logs can be ingested using either an Azure storage bucket or pushed to RunReveal using a webhook.Azure storage buckets are inherently cheaper than using the webhook method but logs can be delayed by up to an hour. The webhook ingestion\nimports logs as soon as they are generated, but using event hubs to forward every event can become more expensive if there are lots of logs.\nAzure Blob Storage\nWebhook Ingestion\nAfter creating your storage account and other resources, you will need to setup activity logs to forward to it.From your Entra admin portal navigate to the Users->Sign-in logs screen and click on \"Export Data Settings\"\nOn the diagnostic settings page, add a new diagnostic setting. Give the diagnostic setting a name, choose the categories you wish to include in your events, and select \"Archive to a storage account\" selecting the storage account that was created.","eventhub-creation#EventHub Creation":"The first step in setting up Azure Entra logs is to create an event hub.An event hub needs an event hub namespace, if you don't already have one that you want to use, you will need to create that first.\nFollow the steps in Azure to create a namespace in your resource group giving it a name. This name will be needed when setting up your Azure function.After creating the namespace, create a new event hub inside it. Save the name you give to it as it will be needed later. \nAt this point you can now setup Azure to export logs to your event hubs, but you can also setup different access policies or consumer groups for your event hubs as required by your organization.","export-logs-to-event-hub#Export Logs to Event Hub":"With the event hub in place you can now setup Activity Logs to export to it. Pick and choose which type of log you want imported to RunReveal.From your Entra admin portal navigate to the Users->Sign-in logs screen and click on \"Export Data Settings\"\nOn the diagnostic settings page, add a new diagnostic setting. Give the diagnostic setting a name, choose the categories you wish to include in your events, and select \"Stream to an event hub\" filling in the details with the event hub that was created.","runreveal-source#RunReveal Source":"In order for your RunReveal workspace to accept the log you will first need to create the source.Create a new Azure Entra Log source and select the Webhook ingest method.Once you have created the source, make note of the Webhook URL that was generated. This will be used when setting up your Azure function.","connect-everything-together#Connect Everything Together":"The final step is to create an Azure function that will trigger when new messages are sent to the event hub, and forward them to your RunReveal source. Luckily we have an easy to use template to help with that. Navigate to our GitHub repo to view the source code of the Azure Function getting deployed. Click on the below button to automatically load the template into Azure.To get started fill in the Subscription, resource group, and function name. Select the event hub namespace where the logs are sent and the access policy that will be used to read events. The GitHub repo and branch are used to download the function source code. Keep these default unless you plan to fork the repo to your own GitHub account.If you plan to import Activity Logs, check the \"Enable Activity Log Event Hub Functions\" box to bring up the options for Activity Logs. Fill in the RunReveal webhook URL that was obtained earlier when creating your source. Then select the event hub and consumer group that are being used to store the activity logs.If you plan to Import Entra Logs, check the \"Enable Entra Log Event Hub Functions\" box to bring up the options for Entra Logs. Fill in the RunReveal webhook URL that was obtained earlier when creating your source. Then select the event hub and consumer group that are being used to store the Entra logs.\nWhen filling in the webhooks for Azure Activity Logs and Entra Logs, make sure to use the correct URL for that source. Mixing up the URLs or creating the wrong type of source will cause most if not all of your logs to fail to import.\nAdd any required tags to your setup, then review the settings. Once created Azure will begin the deployment of the function. This may take a few minutes, but when complete logs should begin flowing to RunReveal.\nAzure may take some time before they start to forward events to your Azure function. If a few hours have gone by and you have not seen any logs appear in your RunReveal account reach out to us to for some help."}},"/sources/object-storage/gcs":{"title":"GCS","data":{"":"This guide explains how to set up log ingestion from Google Cloud Storage (GCS) using either service account keys or workload identity federation with AWS.","overview#Overview":"The process for ingesting logs from Google Cloud Storage (GCS) involves 3 steps:\nSend your logs/events to a storage bucket.\nAn Object Created event is triggered and adds a message to a pub/sub topic.\nEvery minute, RunReveal will poll a pub/sub subscription on the created topic, pull any new notifications, and process the objects that are returned.","resource-setup#Resource Setup":"Setting up a GCS ingestion source requires the following steps:\nCreate a service account that will have access to the resources.\nCreate a storage bucket giving read access to the created service account.\nCreate a GCS bucket notification that writes to a new pub/sub topic.\nCreate a pub/sub subscription that RunReveal can use to poll for new events giving subscribe access to the service account.\nGive RunReveal authentication access to your service account using either service account keys or workload identity federation.\nCreate a RunReveal source and provide us with the details of your setup.\nService accounts require the following permissions for RunReveal to process logs:\nStorage Object Viewer on the storage bucket where logs are stored.\nPub/Sub Subscriber on the pub/sub subscription where event notifications are sent.","service-account-creation#Service Account Creation":"Choose a project for the storage bucket, pub/sub topic, and service account. Create a new service account (or use an existing one) at https://console.cloud.google.com/iam-admin/serviceaccounts/createGive the service account a name, id, and a description. Copy the generated service email address for future steps.","storage-bucket-creation#Storage Bucket Creation":"Create a new bucket in the GCP cloud storage page.\nIf adding a retention policy to the bucket, make sure it's no less than 4 hours to ensure RunReveal has sufficient time to import all logs before object deletion.\nAdd the service account email to the bucket's permissions with the Storage Object Viewer role.","gcs-bucket-notification-creation#GCS Bucket Notification Creation":"Create a pub/sub notification for new objects added to the bucket using the GCP cloud shell:\ngcloud storage buckets notifications create gs://BUCKET_NAME --topic=TOPIC_NAME -e OBJECT_FINALIZE\nReplace BUCKET_NAME with your bucket name and TOPIC_NAME with your desired topic name.","pubsub-subscription-creation#Pub/Sub Subscription Creation":"Create a new pull subscription for the topic:\nGive the subscription an ID and select the topic created in the previous step.\nChoose Pull as the delivery type.\nSet other settings according to your preferences.\nGrant the service account the Pub/Sub Subscriber role on the subscription.\nSave the subscription name for setting up your RunReveal source.","authentication-setup#Authentication Setup":"RunReveal offers two authentication methods for GCS access, service account keys where you will provide RunReveal with a JSON config file containing a private key,\nor using workload identity federation where you give an AWS IAM role access to authenticate as your chosen service account.","undefined#Service Account Keys":"Open the service account and navigate to the keys menu.\nCreate a new JSON private key.\nDownload the new key to provide to RunReveal when setting up your source.","undefined#Workload Identity Federation with AWS":"Follow the GCP documentation for setting up a Workload Identity Pool.\nRunReveal allows you to bring your own AWS IAM Role or use the provided one to allow authentication.","aws-setup#AWS Setup":"If you plan to bring your own IAM Role you will need to add the RunReveal AWS account as a trusted entity. Use the following policy to add the account and provide an optional external ID.\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::253602268883:root\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"<EXTERNAL_ID>\"\n                }\n            }\n        }\n    ]\n}","prepare-gcp#Prepare GCP":"Before beginning you will need to enable specific APIs to allow RunReveal the ability to authenticate.Enable the IAM, Resource Manager, Service Account Credentials, and Security Token Service APIs.Enable the APIs","create-a-pool#Create a Pool":"Open the Workload Identity Federation page in the IAM & Admin section and click on Create Pool.\nGive the pool a name and description and select continue.\nCreate a new AWS provider giving it a name and your AWS account ID.\nIf you plan to use the RunReveal provided AWS role enter RunReveal's account ID, 253602268883.\nUnless you have specific needs, you can leave the mappings and conditions as the default values. And click save.","grant-service-account-access#Grant Service Account Access":"Now that your pool and provider are created you can grant access to the pool by clicking on the 'Grant Access' button.Select the service account that you have setup previously that has the correct permissions to access the GCP resources (bucket and pub/sub).Select aws_role as the attribute name that will be used for lookup. Enter the assumed role arn as the attribute value, arn:aws:sts::<account_id>:assumed-role/<role_name>.\nIf you are using the RunReveal provided role, you will enter arn:aws:sts::253602268883:assumed-role/runreveal_identity_federation into this field.\nSelect the provider from the dropdown and download the config. This config file contains the settings required for clients to authenticate. Keep this file as it will be supplied to RunReveal when creating your source.","runreveal-source-setup#RunReveal Source Setup":"Navigate to the Connect a source page and find the source you are adding. Select the Google Cloud Storage Bucket ingest method and fill in the fields.\nRegardless of the authentication method you will need to provide the subscription path created in the Pub/Sub Subscription Creation step.","service-account-keys#Service Account Keys":"If authenticating with service account keys select the Service account key method and paste or select the private key file that was downloaded from GCP for the service account.","workload-identity-federation#Workload Identity Federation":"If authenticating with identity federation you will need to provide the AWS role arn that RunReveal will assume. Leave this field blank if you plan to use the RunReveal provided role.If your role uses an external ID for authentication make sure the same value is pasted in the External ID field.Copy or select the config file that was downloaded in the Grant Service Account Access steps.","final-steps#Final Steps":"At this point, all pieces are in place for RunReveal to access logs stored in the bucket. Continue to the source docs for specific instructions on how to get your logs into the GCS bucket."}},"/sources/source-types/cloudentity":{"title":"Cloudentity","data":{"":"Cloudentity provides identity and authorization services focused on securing APIs and enforcing policies for access control. Cloudentity logs capture detailed information on authentication and authorization events, policy evaluations, and API access requests. These logs are valuable for auditing user activity, monitoring API usage, and ensuring compliance with security and privacy regulations.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"","source-create#Source Create":"Create a new Cloudentity source in RunReveal, once created a new webhook will be provided.","configure-cloudentity-audit-logs#Configure CloudEntity Audit Logs":"Log in to your CloudEntity Admin Console.\nNavigate to \"Settings\" or \"Configuration\" (the exact name may vary).\nLook for a section named \"Audit Logs\" or \"Event Notifications\".\nEnable audit logging if it's not already enabled.\nIn the webhook configuration section, enter your custom webhook URL.","configure-webhook-payload#Configure Webhook Payload":"CloudEntity may allow you to customize the payload sent to your webhook. If available:\nChoose the events you want to receive (e.g., user logins, permission changes, etc.).\nSelect the data fields to include in the payload.\nSet up any filtering rules to limit the events sent to your webhook."}},"/sources/source-types/cloudflare/audit":{"title":"Cloudflare Audit Logs","data":{"":"Cloudflare Audit logs allow you to view audit events from your Cloudflare account. Audit logs give insights into what resources were changed in your Cloudflare organization.Cloudflare Audit logs will grab 30 days worth of logs for your account and backfill your source. After the initial load, RunReveal will attempt to poll the Cloudflare API to download your new audit logs every 60 seconds.","ingest-method#Ingest Method":"Currently, Cloudflare audit logs only support polling the Cloudflare API for new events every minute.","setup#Setup":"The two fields we require from your Cloudflare account are your account identifier and an API token.","account-identifier#Account Identifier":"To find your account identifier,\nLogin to your Cloudflare account\nNavigate to your organization's site settings\nYour account identifier will be listed in the right hand panel near the bottom.\nClick the copy button and paste it into RunReveal","api-token#API Token":"To generate an API token you can click the Get your API token link directly under the Account ID section. From there, create a new custom token.Give the token a name like RunReveal to identify its use. The only permission the token requires is read access to Account Settings."}},"/sources/source-types/cloudflare/gateway-dns":{"title":"Cloudflare Gateway DNS","data":{"":"Cloudflare Gateway DNS logs capture DNS queries made through Cloudflare's secure DNS filtering service, which provides protection against malware, phishing, and unauthorized content access. These logs include details such as the domain names requested, the user or device making the request, and whether the request was blocked or allowed based on security policies. The logs are valuable for monitoring web traffic, enforcing content filtering, and detecting potentially malicious or risky domains.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage\nGoogle Cloud Storage\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_cf_gateway_dns","setup#Setup":"Setting up Cloudflare gateway DNS logs requires the use of Cloudflare Logpush.Navigate to the Logpush setup page in your Cloudflare account and create a new logpush job that sends gateway DNS logs to your storage bucket.Once created Cloudflare will begin to push logs to your bucket and RunReveal will start to ingest them."}},"/sources/source-types/cloudflare/gateway-http":{"title":"Cloudflare Gateway HTTP","data":{"":"Cloudflare Gateway HTTP logs capture details of HTTP and HTTPS requests made through Cloudflare's secure web gateway. These logs include information such as the URLs accessed, the IP addresses of the requestors, HTTP methods used, and the response codes. They are valuable for monitoring web traffic, enforcing security and content filtering policies, detecting malicious or suspicious activity, and ensuring compliance with organizational web usage policies.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage\nGoogle Cloud Storage\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_cf_gateway_http","setup#Setup":"Setting up Cloudflare gateway HTTP logs requires the use of Cloudflare Logpush.Navigate to the Logpush setup page in your Cloudflare account and create a new logpush job that sends gateway HTTP logs to your storage bucket.Once created Cloudflare will begin to push logs to your bucket and RunReveal will start to ingest them."}},"/sources/source-types/cloudflare/http":{"title":"Cloudflare HTTP Logs","data":{"":"Cloudflare HTTP logs provide detailed information about HTTP and HTTPS requests passing through Cloudflare's reverse proxy and content delivery network (CDN). These logs capture data such as client IPs, request URLs, response status codes, caching status, request method (GET, POST, etc.), and performance metrics like latency. They are useful for monitoring web traffic, troubleshooting website performance, detecting security threats such as DDoS attacks, and optimizing content delivery.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage\nGoogle Cloud Storage\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_cloudflarehttp","setup#Setup":"Setting up Cloudflare gateway DNS logs requires the use of Cloudflare Logpush.Navigate to the Logpush setup page in your Cloudflare account and create a new logpush job that sends your HTTP logs to your storage bucket.Once created Cloudflare will begin to push logs to your bucket and RunReveal will start to ingest them."}},"/sources/source-types/crowdstrike/event-stream":{"title":"CrowdStrike Event Stream","data":{"":"The CrowdStrike Event Stream provides a continuous flow of real-time security events and telemetry data generated by the CrowdStrike Falcon platform. This stream captures critical information such asCrowdStrike logs are ingested by utilizing the CrowdStrike streaming events service that they offer. Every 60 seconds we connect to your CrowsStrike event streams and ingest any events that are forwarded","setup#Setup":"Login to your CrowdStrike account and navigate to API clients and keys under the Support and resources section.\nCreate a new OAuth2 API Client from this page. Give the client a name and optional description. RunReveal only requires Read access to Event Streams for ingestion to work.\nSave the Client ID, Client Secret, and Base URL that is displayed once created. You will need these when setting up your RunReveal source.\nIn RunReveal, create a new CrowdStrike source. Give it a name and fill in the values from your API client.Once added CrowdStrike events should start ingesting within a few minutes."}},"/sources/source-types/dnsfilter":{"title":"DNSFilter","data":{"":"DNSFilter is a cloud-based DNS security and content filtering solution that protects against malware, phishing, and other web threats. DNSFilter logs provide information on DNS queries, including domain requests, categorization (e.g., malicious, safe, or blocked), and response actions. These logs help with monitoring web traffic, enforcing content filtering policies, and identifying potential security threats such as malicious domains or phishing attempts.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_dnsfilter","setup#Setup":"In order to ingest your DNSFilter logs you will need to export them to an AWS S3 bucket. You can reference the DNSFilter guides on how to do this.https://help.dnsfilter.com/hc/en-us/articles/6266552356499-Data-Export-configuration#h_01HA5DMGQ67M86AV56MWPJXZR3"}},"/sources/source-types/crowdstrike/fdr":{"title":"CrowdStrike Falcon Data Replicator","data":{"":"CrowdStrike Falcon Data Replicator is a tool designed to facilitate the integration and export of endpoint telemetry and threat data from the CrowdStrike Falcon platform to external storage solutions or SIEM systems. It captures critical security events, alerts, and activity logs generated by the Falcon agents, allowing organizations to analyze and correlate data for enhanced threat detection, incident response, and compliance reporting.","setup#Setup":"The CrowdStrike Falcon Data Replicator source is different than other S3 sources.\nInstead of needing to configure your bucket, and a role, CrowdStrike provides all\nof this configuration and information for you.\nThe information you'll need to provide from CrowdStrike is:\nAWS Access Key ID - This is a normal AWS Access Key, and it's provided by CrowdStrike\nto authenticate to your CrowdStrike data.\nAWS Secret Access Key - The secret key associated with your AWS Access Key ID.\nSQS Queue URL - This queue URL provides RunReveal with notifications that new\nCrowdStrike data is available to be read.\nRegion - The region your S3 bucket calls home.\nAll of this information is required for the FDR source to work properly. Once provided\nand the source is created, your CrowdStrike data should begin flowing to RunReveal\nimmediately.","querying-your-crowdstrike-data#Querying your CrowdStrike Data":"Your CrowdStrike data will be available in a few different places in RunReveal\ncrowdstrike_aidmaster_logs -- Basic host data collected from CrowdStrike.\ncrowdstrike_data_logs -- Contains raw data from your CrowdStrike sensors.\ncrowdstrike_managed_logs -- Information collected from managed assets.\nAdditionally all CrowdStrike data is available in the logs table with\nthe sourceType of crowdstrike-fdr."}},"/sources/source-types/fluent-bit":{"title":"Fluent-Bit","data":{"":"Fluent Bit is a lightweight and extensible log processor and forwarder. It is\ndesigned to be fast and efficient, and it is optimized for low resource\nconsumption. Fluent Bit is part of the Fluentd project ecosystem, and it is\nlicensed under the terms of the Apache License v2.0.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_generic","setup#Setup":"","1-stream-logs-to-s3#1. Stream logs to S3":"To ingest fluent-bit logs into RunReveal, you will need to set up an S3 bucket\nto stream your fluent logs to, if you don't have one already.Below you'll find an example configuration for fluent-bit to stream logs to an\nS3 bucket.\n[OUTPUT]\n    Name                         s3\n    Match                        *\n    bucket                       my-fluent-bit-bucket\n    region                       us-east-2\n    use_put_object               Off\n    compression                  None\n    s3_key_format                /$TAG/%Y/%m/%d/%H_%M_%S-$UUID.ndjson\n    total_file_size              10M\n    upload_timeout               60s","2-source-setup#2. Source setup":"From there, take the bucket you've configured fluent-bit to stream logs to, and\nfollow our guide on setting up a Generic S3\nsource, paying close attention to the event\ntransformation step to normalize your logs for optimal query performance."}},"/sources/source-types/dropbox":{"title":"Dropbox Event Logs","data":{"description-quickly-ingest-your-dropbox-event-logs-for-your-entire-team#description: Quickly ingest your Dropbox event logs for your entire team.":"Dropbox event logs allow you to view the events that have taken place in your Dropbox team account.When adding Dropbox, RunReveal will backfill with events from the past 24 hours. Once the backfill is complete, RunReveal will ingest new events every minute.","setup#Setup":"Setting up the Dropbox source is fairly straight forward. You will need to create a new app in the Dropbox developer console, supply it with a redirect URL supplied by RunReveal, add the keys to the RunReveal source,\nand then authenticate to your app giving RunReveal permission to read your events.","new-dropbox-app#New Dropbox App":"Navigate to the dropbox app page to create a new app, https://www.dropbox.com/developers/apps/create.To create a new app you will need to:\nSelect Scoped access for the API type.\nChoose Full Dropbox for the access type.\nGive the app a name and click Create app","dropbox-app-permissions#Dropbox App permissions":"Once the app is created click on the permissions tab and select team_data.member and events.read and click Submit at the bottom of the page.","new-runreveal-source#New RunReveal Source":"Before finalizing the Dropbox app, you will need to add a redirect URL from RunReveal to allow proper Dropbox authorization.In the RunReveal dashboard, create a new Dropbox source, https://app.runreveal.com/dash/sources/add/dropbox.When the page loads, a new redirect URL will be generated with a unique ID that is used to lookup your source settings when authenticating with Dropbox.Copy this redirect URL and switch back over to the Dropbox app page.","dropbox-app-settings#Dropbox App Settings":"On the Dropbox app screen paste the redirect URL from RunReveal into the Redirect URIs box and click add.Copy the App key and App secret values that are displayed on this screen, these will be needed to finish setting up the RunReveal source.","runreveal-setup#RunReveal Setup":"To finish setting up the source, go back to the RunReveal source page to fill in the remaining fields.\nGive the source a name and paste the app key and secret from the Dropbox app into the fields.\nThe redirect URL in the RunReveal source is generated when adding a new source. If the page refreshes before the source is saved a new URL will be regenerated.\nIf this happens you will need to add the new URL to your dropbox app again before continuing.\nClick connect to save the source in RunReveal. Once saved, you will be redirected to the Dropbox authentication page to authorize API usage using the created app.\nAuthenticate with your team dropbox account and if there are no issues the app will be authorized and you will be redirected back to the RunReveal sources page.\nIf any error occurs with authentication or if the app needs to be authenticated again,\nit will require you to edit the source in RunReveal and supply the app key and secret again.\nOnce both values are supplied and update is clicked the Dropbox authorization process will start again.","verify-its-working#Verify Its working":"Once added the source logs should begin flowing within a minute.You can validate we are receiving your logs by running the following PQL query.\nlogs\n| where sourceType == 'dropbox' and receivedAt >= today()"}},"/sources/source-types/github/audit":{"title":"Github Audit Logs","data":{"":"GitHub Audit Logs provide a detailed record of actions and events within a GitHub organization or repository.\nThese logs capture information such as user logins, repository changes (e.g., pushes, merges, deletions), permission modifications,\nand security settings updates. They help administrators track user activity, ensure security compliance, and audit changes for\ntroubleshooting and incident investigation.","ingest-methods#Ingest Methods":"Github audit logs can be ingested using two separate methods, streaming audit logs where Github will push your logs to a cloud storage account to be ingested by RunReveal,\nand API polling where RunReveal will use an access token to poll your Github organization for new audit logs every 60 seconds.","log-streaming#Log Streaming":"","resource-setup#Resource Setup":"If you plan to use audit log streaming you will need to setup the necessary resources and permissions for RunReveal to get access.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage\nGoogle Cloud Storage\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_github","github-setup#Github Setup":"Once the resources and permissions are set for RunReveal you will need to setup Github to send logs to your bucket.Follow the steps provided in the GitHub documentation\nto setup streaming for the cloud provider that you are using.","api-polling#API Polling":"When setting up API polling RunReveal will only need two items, the name of the organization that we should be polling audit logs for, and an API token to access your account.","api-token#API Token":"To generate an API token navigate to the Personal access tokens (classic) page in your GitHub account and click on Generate new token, or follow this link https://github.com/settings/tokens/new\nMake sure you are under your personal account settings and are logged in with a user that has access to your organizations audit logs. When creating a new token make sure its a classic type. The required permissions are not available for fine grained tokens yet.\nGive the token a description and select an expiration for it. When selecting the scopes the only required scope is the read:audit_log.","runreveal-source-setup#RunReveal Source Setup":"Once all of the other setup steps have been completed you can now log into RunReveal and create the Github source.Select the ingest method that you are using and fill in the details for your setup."}},"/sources/source-types/generic":{"title":"Generic Log Sources","data":{"":"Generic log sources allow you to send any type of event to RunReveal.","ingest-methods#Ingest Methods":"RunReveal offers the following ways to ingest Generic log sources:\nAzure Storage Account\nAWS S3\nAWS S3 Bucket with Custom SQS\nGoogle Cloud Storage\nWebhooks\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_generic"}},"/sources/source-types/github":{"title":"GitHub","data":{"":"RunReveal offers two separate and very distinct Github sources.","github-audit-logs#Github Audit Logs":"https://app.runreveal.com/dash/sources/add/githubGithub audit logs are available to paid Organization and Enterprise accounts.\nThese logs allow you access events that have occurred in your accounts by users or other entities.","webhook-events#Webhook Events":"https://app.runreveal.com/dash/sources/add/github-webhookWhile available to use regardless of your account level in Github, webhook events do not give the same type of observability as audit logs."}},"/sources/source-types/gitlab":{"title":"GitLab S3 Streaming Audit Logs","data":{"":"Gitlab S3 streaming is only available to Gitlab Ultimate customers.\nGitLab Audit Logs provide a detailed record of events and actions taken within a GitLab instance,\nhelping organizations track changes for security and compliance purposes.\nThe logs capture information such as user activity (e.g., login attempts, project changes, and group modifications), timestamps,\nand the specific actions performed, enabling administrators to monitor user behavior, investigate incidents,\nand ensure adherence to security policies.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nGoogle Cloud Storage\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_gitlab","setup#Setup":"Follow Gitlab's docs on how to enable log streaming in your account. RunReveal does not currently support Gitlab HTTP destinations.https://docs.gitlab.com/ee/user/compliance/audit_event_streaming.html"}},"/sources/source-types/jamfprotect":{"title":"Jamf Protect Security Event and Telemetry with a Webhook","data":{"":"This guide explains how to set up Jamf Protect Cloud to forward telemetry and security events to RunReveal using a webhook url generated by the JAMF Protect source in the RunReveal UI.","#":"","prerequisites#Prerequisites":"Jamf Protect Cloud admin access\nYou need rights to configure telemetry forwarding and webhooks.\nRunReveal account with permissions to add sources and view logs (admin/editor)\nAbility to add sources and view logs.\nTelemetry/Security logging set up in JAMF Protect Cloud\nDecide which Jamf Protect data (telemetry, alerts, etc.) you want to send.","step-1-generate-a-webhook-endpoint-in-runreveal#Step 1: Generate a Webhook Endpoint in RunReveal":"Navigate to Sources in RunReveal\nClick Add Source\nSelect Jamf Protect and choose Webhook as the ingest type and save the source with a name to generate the unique webhook url.\nCopy the webhook endpoint URL provided","step-2-create-data-endpoint-action-in-jamf-protect-cloud#Step 2: Create Data Endpoint Action in Jamf Protect Cloud":"Log in to Jamf Protect Cloud Console\nNavigate to Settings → Action Configurations\nClick New to create a new action configuration\nSelect Data Endpoint as the action type\nProvide a descriptive name (e.g., \"RunReveal Webhook\")\nConfigure the data endpoint settings:\nEndpoint URL: Paste the RunReveal webhook URL you copied earlier\nMethod: POST\nHeaders: Set Content-Type to application/json\nAuthentication: None (authentication handled via webhook URL)\nData Format: JSON","for-more-details-see-the-creating-an-action-configuration---jamf-macos-security-portal-guide#For more details, see the Creating an Action Configuration - JAMF macOS Security Portal guide.":"","step-3-select-telemetry--event-types-to-forward#Step 3: Select Telemetry & Event Types to Forward":"In Jamf Protect, you can typically choose:\nTelemetry (system, process, network, file events)\nAlerts (detections, policy violations)\nDevice status/events\nUser activity\nYou can always update your selection later as monitoring needs evolve.","step-4-validate-delivery#Step 4: Validate Delivery":"Return to RunReveal, open your Jamf Protect Source page.\nConfirm that logs are arriving.\nSearch within explorer or use Native AI Chat to inspect recent Jamf Protect events.","troubleshooting#Troubleshooting":"Missing Logs\nCheck that Jamf Protect webhook is enabled and pointed to RunReveal\nValidate that events are generated in JAMF Protect Cloud and that you've selected your event types for the forwarder.","helpful-links#Helpful Links":"Telemetry for macOS Event Categories\nCreating a Telemetry Set"}},"/sources/source-types/google-workspace-logs":{"title":"Google Workspace Audit Logs","data":{"":"Google Workspace Audit Logs track user activity and administrative actions within Google Workspace services such as Gmail, Google Drive, and Google Meet. These logs capture details like login attempts, file access, sharing actions, and configuration changes. They are essential for monitoring user behavior, auditing data access, investigating security incidents, and ensuring compliance with organizational policies and regulations.\nConnecting GSuite requires a GSuite administrator who:\nCan perform a domain wide delegation\nCreate a new gcp project","setup-google-cloud-project-and-enable-apis#Setup Google Cloud Project and Enable APIs":"Go to the Google Cloud Console.\nCreate a new project or select an existing one.\nNavigate to the \"APIs & Services\" > \"Dashboard\" section.\nClick \"+ ENABLE APIS AND SERVICES\" and search for \"Admin SDK API.\" Enable it.\nGo to \"Credentials\" > \"+ CREATE CREDENTIALS\" > \"Service account\" and follow the process to create a new service account.\nAfter creating the service account, click on it and go to \"Keys\" > \"Add Key\" > \"Create new key\" > \"JSON\". Download the JSON file; this will be your credentials file.","enable-api-access-in-google-admin-console#Enable API Access in Google Admin Console":"Go to the Google Admin Console.\nNavigate to \"Security\" > \"API controls.\"\nIn the \"Domain wide delegation\" section, click \"Manage Domain Wide Delegation.\"\nClick \"Add new\" and enter the Client ID of your service account (you can find this in your service account details in the Google Cloud Console).\nIn the \"OAuth Scopes\" field, enter the following scope: https://www.googleapis.com/auth/admin.reports.audit.readonly\nSave the configuration.","add-the-google-workspace-source-to-runreveal#Add the Google Workspace source to RunReveal":"In the RunReveal dashboard, select \"Google Workspace\" in the sources page.\nGive your source a name.\nThe subject must be an administrator of your google workspace account, usually it will be the email address of the person who performed the domain-wide delegation.\nEither choose the credential.json file using the file picker, or paste the contents into the Credential text area.\nClick \"Verify Settings\" and \"Connect\" to save your new source."}},"/sources/source-types/github/webhook":{"title":"GitHub Webhook Events","data":{"":"GitHub Webhook events are automated notifications sent to a specified endpoint when certain actions occur in a repository or organization.\nThese events capture activities such as push events, pull request creations or updates, issue comments, and release publications.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the Github Webhook source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"In Github under your organization settings, click on the Webhooks menu item and then Add webhook.\nPaste the webhook URL that RunReveal provided when setting up your source, set the content type to application/json, and select which events you would like to receive from Github.Once added Github will begin to forward events to your RunReveal account."}},"/sources/source-types/jamf":{"title":"JAMF Logs","data":{"":"JAMF provides device management solutions for Apple devices in enterprise environments, allowing administrators to deploy, configure, and secure iOS and macOS devices. JAMF logs capture details about device management activities, such as policy deployments, application installations, security compliance checks, and device inventory updates. These logs are critical for monitoring device health, ensuring compliance with organizational policies, and troubleshooting issues related to device configurations.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"","step-1-access-the-webhook-configuration#Step 1: Access the Webhook Configuration":"Once logged in, navigate to \"Settings\" in the top navigation bar.\nIn the left sidebar, under \"Global Management\", click on \"Webhooks\".","step-2-create-a-new-webhook#Step 2: Create a New Webhook":"On the Webhooks page, click the \"+ New\" button.\nYou'll be presented with the webhook configuration form.","step-3-configure-the-webhook#Step 3: Configure the Webhook":"Fill out the webhook configuration form with the following details:\nDisplay Name: Enter a descriptive name for your webhook (e.g., \"Log Forwarding Webhook\").\nStatus: Set to \"Enabled\".\nURL: Enter the URL of your webhook endpoint where JAMF will send the data.\nAuthentication Type: Choose the appropriate authentication method required by your webhook endpoint:\nNone: If your endpoint doesn't require authentication.\nBasic Authentication: If your endpoint uses username/password authentication.\nOAuth 2.0: If your endpoint uses OAuth 2.0 for authentication.\nContent Type: Select \"application/json\" unless your endpoint requires a different format.\nWebhook Event: Choose the events you want to trigger the webhook. For comprehensive logging, you might want to select all available events."}},"/sources/source-types/jumpcloud":{"title":"JumpCloud Directory Insights","data":{"":"JumpCloud Directory Insights logs capture authentication, authorization, and directory management events across your JumpCloud environment, including user logins, group modifications, system access attempts, and policy changes.\nThese logs are crucial for security monitoring, compliance reporting, and understanding user activity across your organization's directory services.When adding a JumpCloud source RunReveal will ingest the last 96 hours of logs before polling every minute for new logs.","setup#Setup":"Give your JumpCloud source a descriptive name to help find it later. The two fields we require are a list of services that you want events ingested from and an API Key.","service-list#Service List":"JumpCloud separates their events into distinct services each with their own schema. Select \"All\" from the list to ingest all of the current and future services that JumpCloud supports.\nOtherwise select a subset of services to import.","api-key#API Key":"To generate an API Key perform the following actions.\nLog into the JumpCloud Admin portal.\nClick the username drop-down menu located in the top-right of the Admin Portal.\nClick API Settings.\nAPI Keys have full access to all data accessible to the admin account that created it. RunReveal recommends creating a service level account\nwith minimal permissions to provide access to your JumpCloud Events.","verify-its-working#Verify Its working":"Once added the source logs should begin flowing within a minute.You can validate we are receiving your logs by running the following SQL query."}},"/sources/source-types/notion":{"title":"Notion Logs","data":{"":"Notion is a productivity and collaboration tool for managing documents, projects, and databases. Notion logs capture user activity such as page views, edits, comments, sharing actions, and permission changes. These logs help administrators monitor collaboration, track changes to content, and ensure proper access controls are maintained, providing insights into how teams use and interact with Notion workspaces.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"Login to your Notion account and navigate to Settings -> Connections -> Workspace. From this screen you can click see all to view the RunReveal Integration.Select the RunReveal tile and paste the webhook URL that was generated when you added your source. Click the connect button to start sending Notion logs to RunReveal."}},"/sources/source-types/gcp":{"title":"Google Cloud Platform (GCP) Logs","data":{"":"GCP logs provide comprehensive monitoring and logging across all Google Cloud services through Cloud Logging. These logs capture various types of data, including system events, API calls, network traffic, resource access, and performance metrics. GCP logs are essential for tracking user activity, monitoring infrastructure, troubleshooting issues, auditing security and compliance, and optimizing the performance of Google Cloud resources.","ingest-methods#Ingest Methods":"GCP Logs can be ingested using the GCS object storage method as well as setting up a webhook to receive events.GCS buckets are inherently cheaper than using the webhook method but logs can be delayed by up to an hour. The webhook ingestion\nimports logs as soon as they are generated, but using pub/sub to forward every event can become more expensive if there are lots of logs.\nGoogle Cloud Storage\nRunReveal Webhook\nAfter creating your cloud storage bucket and other resources needed to receive events, you will need to setup a GCP log router to forward logs to the bucket.","undefined#GCP Log Router Sink Setup":"Navigate to the GCP Log router setup page to create a new sink.\nYou can setup a logging sink for a single project or your entire organization and forward logs to the bucket created.\nCreate a new logging sink giving it a name, choose Cloud Storage bucket as the sink service. Enter the bucket information that was created as the destination bucket.\nAdd an optional inclusion or exclusion filter to limit the logs that are forwarded by this logging sink.\nWhen routing logs directly to cloud storage, Google will batch the logs and write them to a file every hour. It may take some time before you see any logs start to show up in the bucket.","runreveal-source#RunReveal Source":"Create your new GCP source and select Webhook as your ingest method. You will be given a webhook URL that you will need to provide to GCP.","pubsub-setup#Pub/Sub Setup":"Navigate to pub/sub resource and under Topics click Create topic. Give your topic a descriptive name like \"RunReveal\" and click Create. Make note of your topic ID.On your new pub topic, click \"Create Subscription\"\nGive the Subscription ID a name like \"RunReveal\" and select Push as the Delivery Type.Paste your RunReveal webhook URL into the GCP logs Delivery Endpoint URL and Click Save.","create-a-sink#Create a sink":"Search for Logs Explorer.\nClick on Logs Explorer and then navigate to Log Router.\nClick Create Sink. Give your sink a descriptive name like \"RunReveal\"\nSelect the sink service as Cloud Pub/Sub topic\nFill in the PROJECT_ID and TOPIC_ID and click Next\nSelect \"Include logs ingested by this organization and all child resources\"\nClick Next, followed by Create Sink"}},"/sources/source-types/keeper":{"title":"Keeper Security Logs","data":{"":"Keeper Security provides password management and secure file storage solutions for individuals and enterprises. Keeper Security logs capture details of user activity such as logins, password changes, vault access, shared item interactions, and administrative actions. These logs are essential for auditing access to sensitive information, monitoring security events, and ensuring compliance with data protection policies.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"The Keeper Security log source receives webhooks of Keeper Security Logs. However RunReveal has not been officially added to\nthe keeper security SIEM connections. When setting up a Keeper source in RunReveal you'll add a \"Sumo Logic\" source in Keeper\nand provide the RunReveal webhook URL. You don't need a Sumo Logic instance of your own.Once you create a Keeper Security log source you'll be given a webhook URL like other RunReveal webhook sources. Make note\nof this Webhook URL.In the Keeper Security admin console navigate under \"Reporting and Alerts\" to \"External Logging\". You should see list of\nsuppoted SIEM integrations that Keeper natively supports.\nSelect Sumo Logic and you'll be prompted for a Sumo Logic URL. Enter your RunReveal Webhook URL and press \"Test Connection\". Once\nthe connection test succeeds, you won't see any visual indication EXCEPT the \"Save\" button will be clickable. Click Save.It may take several minutes but your keeper logs should begin flowing to RunReveal without issue."}},"/sources/source-types/kubernetes":{"title":"Kubernetes Audit Logs","data":{"":"Kubernetes Audit Logs provide a detailed record of events and API calls made within a Kubernetes cluster. These logs capture information such as who made a request, the resources accessed or modified, and the outcome of the request. Kubernetes audit logs are essential for tracking cluster activity, monitoring user actions, ensuring security compliance, and investigating potential security incidents or misconfigurations within the cluster.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage","setup#Setup":"Getting your logs into a storage account can be accomplished using something like Fluentd or a similar method.\nThe expectation though is that the logs are line delimited json."}},"/sources/source-types/obsidian-security":{"title":"Obsidian Security","data":{"":"Obsidian Security provides threat detection and response for SaaS environments, focusing on monitoring user behavior and detecting account compromise or insider threats. Obsidian Security logs capture detailed activity across SaaS applications, including user access, permissions changes, anomalous behavior, and potential security incidents. These logs help organizations detect threats, enforce security policies, and safeguard against unauthorized access or data breaches.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"Once you copy this webhook URL, add it to your obsidian organization to receive\nevent logs."}},"/sources/source-types/mongodb":{"title":"MongoDB","data":{"":"MongoDB Atlas is a fully-managed cloud database service for MongoDB. Atlas logs provide information on database operations, such as query execution, connections, index usage, and performance metrics. They also capture events related to user activity, database configuration changes, and security incidents. These logs are essential for monitoring database health, optimizing query performance, troubleshooting issues, and ensuring data security and compliance.","ingest-method#Ingest Method":"This source is a polling source and will download new logs from the app API approximately every 60 seconds.","setup#Setup":"","log-in-to-mongodb-atlas#Log in to MongoDB Atlas":"Open your web browser and navigate to the MongoDB Atlas login page.\nEnter your credentials and click \"Sign In\".","access-the-api-keys-page#Access the API Keys Page":"Once logged in, click on \"Access Manager\" in the left sidebar.\nIn the Access Manager section, click on the \"API Keys\" tab.\nClick the \"Create API Key\" button to start the creation process.","configure-the-api-key#Configure the API Key":"Enter a descriptive name for your API key (e.g., \"Log Retrieval API Key\").\nSelect the appropriate access rights:\nFor log access, you typically need the \"Project Read Only\" role.\nIf you need more permissions, select them accordingly.\nClick \"Next\".","add-ip-access-list#Add IP Access List":"In the \"Add IP Access List\" section, you can restrict API access to specific IP addresses.\nTo add your current IP:\nClick \"Add Current IP Address\".\nAlternatively, you can manually enter IP addresses or ranges.\nClick \"Next\".","generate-the-api-key#Generate the API Key":"Review your settings on the final page.\nClick \"Create API Key\".","save-your-api-credentials#Save Your API Credentials":"After generating the API key, you'll be presented with two important pieces of information:\nPublic Key\nPrivate Key"}},"/sources/source-types/okta":{"title":"Okta Security Logs","data":{"":"Okta Security logs allow you to view audit events from your Okta admin dashboard as well as user authentication attempts.Okta stores 30 days worth of logs for your account, when adding the Okta source, we will backfill your account with whatever logs Okta has available. After the initial load, RunReveal will attempt to poll the Okta API to download your security logs every 60 seconds.","setup#Setup":"Give your Okta source a descriptive name to help find it later. The two fields we require from your Okta account are your Okta domain and an API token. Your Okta domain is the domain that was assigned to your Org.","okta-domain#Okta Domain":"To find your Okta domain:\nSign in to your Okta org admin dashboard.\nLook in the header under your profile dropdown.\nClick the copy button and paste the results into RunReveal.\nYour Okta domain may look like one of the ones below. When pasting your domain, make sure it does not have -admin and do not include trailing slashes or the schema https://\nexample.oktapreview.com\nexample.okta.com\nexample.okta-emea.com","api-token#API Token":"An API token is required to authorize RunReveal to access your Okta logs. API tokens remain active as long as they are being used. If a token is unused for 30 days it will be revoked by Okta and a new one must be issued. API tokens must be linked to an active Okta user. When a user is removed all of the API tokens they created will be revoked and a new one will need to be generated.To generate a new token, from your Okta admin dashboard, open the Security -> API page in the lefthand navigation panel. Click on the Tokens option and you will be presented with a list of all of your API tokens. Click the \"Create Token\" button and enter a descriptive name for your token, like \"RunReveal\". You will be presented with your token value, copy this value and paste it into RunReveal.\nOkta API tokens are scoped to the user creating them. For additional security consider creating a new user with lower permissions and generating an API token from that account.","verify-its-working#Verify Its working":"Once added the source logs should begin flowing within a minute.You can validate we are receiving your logs by running the following SQL query.\nSELECT * FROM runreveal.logs WHERE sourceType = 'okta' LIMIT 1"}},"/sources/source-types/opal":{"title":"Opal","data":{"":"Opal logs are sent to RunReveal via a webhook configured in your Opal Organization Settings.To setup your Opal source, start by creating the source in RunReveal to fetch\nyour WebhookURL that you'll configure in Opal. You'll be given a webhook after\ncreating the source via a popup.In Opal under Organization Settings > Event Streaming, create a new Event\nStreaming connection and enter your RunReveal WebhookURL, and configure\nyour authentication with your desired setting.RunReveal supports authentication via Webhook HMAC from Opal. If you'd like\nto use this method then the RunReveal source can be configured with an optional\nHMAC auth setting.\nNote: If no HMAC secret is configured in RunReveal, authentication will not\nbe enforced."}},"/sources/source-types/reveald":{"title":"Reveald Log Collection","data":{"":"Reveald is an open source log collection\nagent that is efficient, performant and simple.Reveald supports a variety of host-level log sources and is appropriate to use\nin any situation where the logs aren't already being collected to an S3 bucket\nor accessible via another first-party API.To get your logs into RunReveal via reveald, we'll first create a reveald source\nin the RunReveal UI, which will give us a webhookURL to use as the target to\nsend our logs to from reveald.\nNext, download a reveald release for the operating system and architecture of\nthe host you'll be running the daemon on from the releases page on\ngithub.  Extract the package to\na directory of your choosing.As of this writing, reveald supports the following sources:\njournald\nfile\ncommand\nsyslog\ncommand\nmqtt\nnginx access logs (via syslog)\nAnd the following destinations:\nrunreveal\ns3\nprinter\nmqtt\nYou can see examples of how to configure reveald for each of these sources in\nthe example config file provided in the repository's examples directory, or an\nabridged example config below.Replace {{YOUR-REVEALD-WEBHOOKURL}} with the webhook given in the UI.\n{\n  // this is an example config file for kawa\n  // it is parsed using hujson so you can use comments and trailing commas, but\n  // is otherwise identical to JSON\n  \"sources\": {\n    // The keys here are the identifiers for the sources, and are used to\n    // refernece them when logging or in metrics.\n    \"kubernetes\": {\n      \"type\": \"file\",\n      \"path\": \"/var/log/pods/\",\n      \"extension\": \".log\",\n    },\n    \"hostlogs\": {\n      \"type\": \"file\",\n      \"path\": \"/var/log/mylogs/\",\n      \"extension\": \".log\",\n    },\n    \"myjournald\": {\n      \"type\": \"journald\",\n    },\n    \"cmdexample\": {\n      // The comand source type allows you to run a command and collect its\n      // output as logs. This is useful for running custom scripts or commands\n      // that output logs to stdout.\n      \"type\": \"command\",\n      \"cmd\": \"/bin/date\",\n      \"args\": [\"+%Y-%m-%dT%H:%M:%S%z\"],\n      // inheritEnv allows you to pass the environment of the reveald process\n      // to the command being run.  Defaults to false.\n      \"inheritEnv\": false,\n      // env allows you to set environment variables for the command being run.\n      // if inheritEnv is set, any keys this json object override the\n      // inherited environment of the reveald process.\n      \"env\": {\n        \"TZ\": \"Asia/Taipei\",\n      },\n      // interval is the time between runs of the command, the polling\n      // frequency. Defaults to 5s. Whatever this is set to, the command\n      // being run will also have a deadline to complete before the\n      // next run, or be killled with an error returned to reveald.\n      \"interval\": \"5s\",\n    },\n    // \"mynginx\": {\n    //   \"type\": \"nginx_syslog\",\n    //   \"addr\": \"0.0.0.0:5514\",\n    // },\n  },\n  \"destinations\": {\n    \"webhook\": {\n      \"type\": \"runreveal\",\n      // Replace this webhook URL with your own, created on https://www.runreveal.com\n      // as a \"Reveald\" type source\n      \"webhookURL\": \"https://example.runreveal.com/sources/kawa/webhook/0123456789\",\n      // You can also use environment variables by referencing them with a\n      // dollar sign. The value must be quoted, start with a dollar sign and be\n      // a valid environment variable name\n      // \"webhookURL\": \"$WEBHOOK_URL\",\n    },\n    \"blobstore\": {\n      \"type\": \"s3\",\n      // bucketName is required\n      \"bucketName\": \"our-s3-archive\",\n      // bucketRegion is required\n      \"bucketRegion\": \"us-west-2\",\n      // pathPrefix is optional and default unset\n      \"pathPrefix\": \"logs/\",\n      // accessKeyID is required if not present in the environment as\n      // AWS_ACCESS_KEY_ID. If running on AWS, reveald can also be authenticated\n      // at the ec2 host/pod level using IAM roles.\n      \"accessKeyID\": \"AKIAIOS...\",\n      // secretAccessKey is required if not present in the environment as\n      // AWS_SECRET_ACCESS_KEY\n      \"secretAccessKey\": \"SECRET\",\n      // batchSize is optional\n      \"batchSize\": 100000,\n    },\n  },\n}\nThis is a valid config file if you're running on Linux.  We can run reveald\npointing at the configuration like so (assuming you download it to the same\ndirectory as the binary).\n./reveald run --config=./config.json\nIf configured correctly, you should start seeing the journald logs and kubelet\npod logs (if the host is running a kubelet) in your RunReveal workspace.If running inside kubernetes, you can configure this binary to run as a\ndaemonset with the given config file as a configmap mounted somewhere inside\nthe pod, and the given log directories shared from the host to the pod."}},"/sources/source-types/palo-pano-traffic":{"title":"Palo Alto Panorama Traffic Logs","data":{"":"Palo Alto Panorama is a centralized management system for Palo Alto Networks' firewalls. Its Traffic logs provide detailed information about network traffic, including source and destination IP addresses, application usage, and session details, which can be used for monitoring and analyzing network activity.","ingestion-methods#Ingestion Methods":"Setup the ingestion of this source using one of the following guides.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nIf using an AWS S3 bucket use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_palopanotraf","collecting-logs-in-a-bucket#Collecting logs in a bucket":"Palo alto panorama traffic logs are loaded from S3.\nYou will need to forward your logs to a bucket prior to collecting them with RunReveal.Use the following config example in reveald to forward panorama syslogs to your S3 bucket.\n{\n  \"sources\": {\n      \"hostlogs\": {\n        \"type\": \"file\",\n        \"path\": \"/var/log/syslog/\",\n        \"extension\": \".log\",\n      },\n  },\n  \"destinations\": {\n      \"runreveal-store\": {\n        \"type\": \"s3\",\n        \"bucketName\": \"runreveal-bucket\",\n        \"bucketRegion\": \"us-west-2\",\n        \"accessKeyID\": \"ACCESSKEY\",\n        \"secretAccessKey\": \"SECRET\"\n      },\n  },\n}"}},"/sources/source-types/sophos":{"title":"Sophos","data":{"":"Sophos offers a range of cybersecurity solutions, including endpoint protection, firewalls, and cloud security. Sophos logs provide detailed information on security events such as malware detections, firewall activity, web filtering, intrusion attempts, and endpoint health. These logs are used to monitor network and device security, investigate threats, and ensure compliance with security policies across an organization’s IT infrastructure.","ingest-method#Ingest Method":"This source is a polling source and will download new event/alert logs from the Sophos API approximately every 60 seconds.","setup#Setup":"To connect your source, generate an API Token from your Sophos Central account and add it to new RunReveal source in the sources dashboard.\nEnter the API Access URL that Sophos provides, and copy the generated Headers into the Headers field."}},"/sources/source-types/sentinelone":{"title":"SentinelOne","data":{"":"SentinelOne is an endpoint security platform that uses AI to detect, prevent, and respond to malware, ransomware, and other advanced threats. SentinelOne logs capture endpoint activity such as threat detections, behavioral anomalies, quarantined files, and automated responses taken to mitigate threats. These logs provide valuable insights for monitoring endpoint security, investigating incidents, and ensuring rapid response to cyberattacks.","ingest-method#Ingest Method":"This source is a polling source and will download new items from /activities enpoint from the SentinelOne API approximately every 60 seconds.","setup#Setup":"Currently the SentinelOne source only supports retrieving items from the /activities api endpoint.","step-1-log-in-to-sentinelone-management-console#Step 1: Log in to SentinelOne Management Console":"Open your web browser and navigate to your SentinelOne Management Console URL.\nEnter your credentials and click \"Sign In\".","step-2-access-the-api-token-generation-page#Step 2: Access the API Token Generation Page":"Once logged in, click on your user profile icon in the top-right corner of the screen.\nFrom the dropdown menu, select \"My User\".\nIn the left sidebar, click on \"API Tokens\".","step-3-generate-a-new-api-token#Step 3: Generate a New API Token":"On the API Tokens page, click the \"Generate\" button.\nYou'll be prompted to enter a name for your API token. Choose a descriptive name that indicates its purpose, for example, \"Integration API Key\".\nSelect the appropriate scope for this API key. The scope determines what actions the API key can perform. For most integrations, you'll need at least \"Read\" permissions.\nSet an expiration date for the token. It's a good security practice to set an expiration date rather than creating a token that never expires.\nClick \"Generate\" to create the new API token.","step-4-runreveal-setup#Step 4: RunReveal setup":"Create the source in the RunReveal sources dashboard. You will need your organizations SentinelOne domain and the created API key.\nRunReveal will backfill your source from the past 365 days of events that SentinelOne provides.Once added logs should begin populating within a minute. It may take some time for the backfill operation to complete before logs are up to date."}},"/sources/source-types/structured-webhooks":{"title":"Structured Webhooks","data":{"":"RunReveal supports sending structured webhooks, events that match the format of the runreveal.logs table that we save all data to.The structured webhook sources support POST requests and uses this go struct to deserialize the object that's sent. It's likely all of these structs will grow in the future and include new fields. We'll update this page if thats the case.\ntype Normalized struct {\n\tEventName string            `json:\"eventName\"`\n\tEventTime time.Time         `json:\"eventTime\"`\n\tReadOnly  bool              `json:\"readOnly\"`\n\tActor     Actor             `json:\"actor\"`\n\tSrc       Network           `json:\"src\"`\n\tDst       Network           `json:\"dst,omitempty\"`\n\tService   Service           `json:\"service,omitempty\"`\n\tResources []json.RawMessage `json:\"resources,omitempty\"`\n\tTags      map[string]string `json:\"tags\"`\n}\ntype Actor struct {\n\tID       string `json:\"id,omitempty\"`\n\tEmail    string `json:\"email,omitempty\"`\n\tUsername string `json:\"username,omitempty\"`\n}\ntype Network struct {\n\tIP   string `json:\"ip,omitempty\"`\n\tPort uint\n}\ntype Service struct {\n\tName string `json:\"name,omitempty\"`\n}\nIf you'd like to send an example event to your structured webhook source to validate the json format that is sent, you can use this example curl request.\n$ curl https://api.runreveal.com/sources/hook/<WEBHOOKID> -d '{    \n  \"eventName\":\"MyEventName\",\n  \"eventTime\": \"2023-11-11T13:47:58+00:00\",\n  \"readOnly\":false,\n  \"timestamp\": \"2023-11-10T13:47:58+00:00\",\n  \"actor\": {\n    \"email\":\"evan@runreveal.com\",\n    \"id\":\"1231231231234\",\n    \"username\":\"evanrunreveal\"\n  },\n  \"src\": {\n    \"ip\":\"1.2.3.4\",\n    \"port\":80\n  },\n  \"dst\": {\n    \"ip\":\"1.2.3.4\",\n    \"port\":80\n  },\n  \"service\":{\n    \"name\":\"foo\"\n  },\n  \"tags\": {\n    \"lol\":\"yeaaaah!\",\n    \"Who are you\":\"I am\"\n  }\n}'","required-fields#Required Fields":"None of the fields in the above object are required. However, eventTime will be set to the receivedAt time if the eventTime field is unset."}},"/sources/source-types/slack":{"title":"Slack Audit Logs","data":{"":"Slack audit logs provide comprehensive visibility into user activities and administrative actions within your Slack workspace. These logs capture events such as user logins, message deletions, channel modifications, app installations, and permission changes. They help administrators monitor workspace activity, ensure compliance, and investigate security incidents.\nSlack audit logs require a Pro plan subscription in RunReveal and an Enterprise Grid level subscription in Slack. Make sure your Slack workspace meets these requirements before proceeding with setup.\nRunReveal will backfill your audit logs since March 2018 (this is the earliest available for slack) Once the processor has caught up, RunReveal will import new audit logs roughly every 60 seconds.","prerequisites#Prerequisites":"Before setting up Slack audit logs, ensure you have:\nA RunReveal Pro plan subscription\nA Slack Enterprise Grid workspace subscription\nAdministrative access to your Slack workspace\nPermission to create and configure Slack apps","enterprise-grid-extra-steps#Enterprise Grid Extra Steps":"If you're using Slack Enterprise Grid, you'll need to complete these additional steps:","add-bot-scope#Add Bot Scope":"In your Slack app, navigate to \"OAuth & Permissions\" in the left sidebar.\nScroll down to \"Scopes\" section and expand \"Bot Token Scopes\".\nClick \"Add an OAuth Scope\" and select \"users:read\" from the list.\nClick \"Save Changes\" to apply the configuration.","make-the-app-an-org-level-app#Make the App an Org-Level App":"In your Slack app, navigate to \"Org Level Apps\" in the left sidebar under the \"Features\" section.\nClick the \"Enable Org-Readiness\" button.\nThis will make your app an organization-level app, allowing it to function properly across your Enterprise Grid workspace.","setup#Setup":"","step-1-create-a-slack-app#Step 1: Create a Slack App":"Navigate to api.slack.com/apps and sign in with your Slack workspace credentials.\nClick \"Create New App\" and select \"From scratch\".\nGive your app a descriptive name (e.g., \"RunReveal Audit Logs\") and select the workspace you wish to use for audit log collection.\nClick \"Create App\" to proceed.","step-2-configure-basic-oauth-scopes#Step 2: Configure Basic OAuth Scopes":"In your newly created Slack app, navigate to \"OAuth & Permissions\" in the left sidebar.\nScroll down to \"Scopes\" section and expand \"User Token Scopes\".\nClick \"Add an OAuth Scope\" and select \"auditlogs:read\" from the list.\nClick \"Save Changes\" to apply the configuration.","step-3-get-app-credentials#Step 3: Get App Credentials":"Navigate to \"Basic Information\" in the left sidebar of your Slack app.\nCopy the \"Client ID\" and \"Client Secret\" values - you'll need these for the RunReveal configuration.\nKeep your Client Secret secure and never share it publicly. This credential provides access to your Slack audit logs.","step-4-start-runreveal-source-creation#Step 4: Start RunReveal Source Creation":"Navigate to the RunReveal UI and go to the source creation page.\nSelect \"Slack\" as your source type.\nProvide a descriptive name for your Slack source.\nEnter the Client ID and Client Secret from your Slack app.\nRunReveal will provide you with a redirect URL - copy this URL.","step-5-configure-oauth-redirect-url#Step 5: Configure OAuth Redirect URL":"Return to your Slack app's \"OAuth & Permissions\" page.\nUnder \"Redirect URLs\", click \"Add New Redirect URL\" and paste the redirect URL provided by RunReveal.\nClick \"Save Changes\".","step-6-complete-oauth-flow#Step 6: Complete OAuth Flow":"Return to the RunReveal source creation page and continue with the setup.\nClick \"Connect Source\" to finish the setup.\nThis should redirect you to Slack to allow permission for the integration. Click \"Allow\" and it should redirect back to RunReveal.","verify-its-working#Verify It's Working":"Once added, the source logs should begin flowing within a minute.You can validate we are receiving your logs by running the following SQL query:\nSELECT * FROM runreveal.logs WHERE sourceType = 'slack' LIMIT 1","troubleshooting#Troubleshooting":"If you encounter issues with the setup:\nEnsure your Slack workspace has Enterprise Grid subscription\nVerify the OAuth redirect URL is correctly configured in both Slack and RunReveal\nCheck that the auditlogs:read scope is properly added to your Slack app\nConfirm your RunReveal account has a Pro plan subscription\nFor additional help, refer to the Slack API documentation or contact RunReveal support."}},"/sources/source-types/zendesk":{"title":"Zendesk","data":{"":"The Zendesk Source works by polling your event/alert logs every 60 seconds using the API token authentication method.To connect your source, generate an API Token from your Zendesk  settings tab. Make sure Token Access is enabled and create a new token.Provide the token, as well as the email address of your Zendesk Support administrator to RunReveal while creating the source.Click verify source prior to connecting your source in RunReveal, which may take a moment, but if successful will ensure that the source is correctly configured."}},"/sources/source-types/tailscale/flow":{"title":"Tailscale Flow Logs","data":{"":"Tailscale Flow Logs provide detailed information about network traffic passing through a Tailscale-managed network, capturing data such as source and destination IP addresses, ports, protocols, and the volume of data transmitted. These logs are useful for monitoring network usage, identifying unusual traffic patterns, troubleshooting connectivity issues, and ensuring the security of peer-to-peer connections within a Tailscale network.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"Once you're assigned a webhook URL, in Tailscale's Logs product, select \"Network logs\",\ncreate a Splunk streaming destination, and set RunReveal's webhook URL as your\nwebhook URL without any Token or API key."}},"/sources/source-types/tailscale/audit":{"title":"Tailscale Audit Logs","data":{"":"Tailscale Audit Logs provide a comprehensive record of administrative actions and user activities within a Tailscale network, capturing events such as user authentication, device connections and disconnections, policy changes, and administrative modifications. These logs are essential for security monitoring, compliance reporting, and understanding user behavior across your organization's Tailscale-managed network infrastructure.","ingest-method#Ingest Method":"This source uses am HTTP webhook to ingest events. Create the source in RunReveal and a new webhook URL will be generated. Use this URL when setting up your source.","setup#Setup":"Once you're assigned a webhook URL, in Tailscale's Logs product, select \"Audit logs\",\ncreate a Splunk streaming destination, and set RunReveal's webhook URL as your\nwebhook URL without any Token or API key."}},"/sources/source-types/wiz-threats":{"title":"Wiz Threats","data":{"":"Wiz threat data is sent to RunReveal via a webhook configured in your Wiz environment using Automation Rules.To setup your Wiz Threats source, start by creating the source in RunReveal to fetch\nyour WebhookURL that you'll configure in Wiz. You'll be given a webhook after\ncreating the source via a popup.","configure-wiz-webhook-integration#Configure Wiz Webhook Integration":"In your Wiz environment, you'll need to setup a webhook integration and create an Automation Rule to send threat data to RunReveal.","setup-automation-rule#Setup Automation Rule":"Create an Automation Rule in Wiz with the following configuration:\nNavigate to your Wiz Automation Rules\nCreate a new rule for threat events\nConfigure the webhook action using your RunReveal WebhookURL\nUse the action template below for the webhook payload","action-template#Action Template":"Configure your Automation Rule with the following action template:\n{\n  \"trigger\": {\n    \"source\": \"{{triggerSource}}\",\n    \"type\": \"{{triggerType}}\",\n    \"ruleId\": \"{{ruleId}}\",\n    \"ruleName\": \"{{ruleName}}\",\n    \"updatedFields\": \"{{#changedFields}}{{name}} field was changed from {{previousValuePrettified}} to {{newValuePrettified}} {{/changedFields}}\",\n    \"changedBy\": \"{{changedBy}}\"\n  },\n  \"threat\": {\n    \"id\": \"{{issue.id}}\",\n    \"title\": \"{{issue.enrichedMainDetection.rule.name}}\",\n    \"description\": \"{{issue.enrichedMainDetection.description}}\",\n    \"status\": \"{{issue.status}}\",\n    \"severity\": \"{{issue.severity}}\",\n    \"created\": \"{{issue.createdAt}}\",\n    \"resolutionNote\": \"{{issue.resolutionNote}}\",\n    \"projects\": \"{{#issue.projects}}{{name}}, {{/issue.projects}}\",\n    \"threatURL\": \"https://{{wizDomain}}/threats#~(issue~'{{issue.id}})\",\n    \"resolvedAt\": \"{{issue.resolvedAt}}\",\n    \"updatedAt\": \"{{issue.updatedAt}}\",\n    \"cloudPlatform\" : \"{{issue.entitySnapshot.cloudPlatform}}\",\n    \"cloudAccounts\": {{issue.enrichedCloudAccounts}},\n    \"cloudOrganizations\": {{issue.enrichedCloudOrganizations}},\n    \"actors\": {{issue.enrichedThreatActors}},\n    \"resources\": {{issue.enrichedThreatResources}},\n    \"tdrSources\": \"{{#issue.enrichedDetections}}{{rule.name}}, {{/issue.enrichedDetections}}\",\n    \"detectionIds\": \"{{#issue.enrichedDetections}}{{id}}, {{/issue.enrichedDetections}}\",\n    \"mitreTechniques\": {{issue.enrichedThreatMitreTechniques}}{{^issue.enrichedThreatMitreTechniques}}null{{/issue.enrichedThreatMitreTechniques}},\n    \"mitreTactics\": {{issue.enrichedThreatMitreTactics}}{{^issue.enrichedThreatMitreTactics}}null{{/issue.enrichedThreatMitreTactics}},\n    \"notes\": \"{{#issue.notes}}{{user.email}}-{{text}}, {{/issue.notes}}\"\n  }\n}"}},"/detections/detection-as-code":{"title":"Detection as Code - Version Control and CI/CD for Security Rules","data":{"":"We want our customers to have the ability to represent their detection as code\nfor a variety of reasons.\nReviewing changes prior to deployment.\nEasily provisioning config to multiple workspaces and environments.\nQuick development of new configuration.\nRapid development and deployment.\nDetection as code is facilitated through the RunReveal CLI. To get started\nwith detection as code, you'll need to download the CLI, and if you don't yet\nhave an active source with log data you'll likely want to create a log source\nfirst.There are a few major features that RunReveal supports in order to facilitate\nour detection as code product.\nGetting Started With Detection as Code\nWriting a detection\nTesting and linting detections\nExporting your detections\nDeplying from source control","getting-started-with-detection-as-code#Getting Started with Detection As Code":"Detection as code relies on the RunReveal CLI. Downloading the CLI can be done\nthrough brew or from our github.\nbrew tap runreveal/runreveal\nbrew install runreveal\nNext you'll need to log into RunReveal in the CLI. This command will opeen\na browser for you to complete your log in through.\nrunreveal init","detection-data-model#Detection Data Model":"The detection data model is described as following. All fields are marked as\neither optional or required. This file needs to be in either yaml format or\njson format, either works.\n# Required: The name of the query for reference\nname: root_account_usage\n \n# Optional: A display name that is shown in the UI\ndisplayName: AWS Root Account Was Used\n \n# Required: The name of the file containing the sql query\nfile: root_account_usage.sql\n \n# Optional: A description of the detection.\ndescription: Monitor for usage of the AWS root account.\n \n# Optional: Additional notes that can be attached to the detection.\nnotes: |-\n  This is a notes field that can store info about this detection.\n  It can be useful to store links to playbooks in your other systems.\n  \n# Optional: An array of strings to help group queries\ncategories:\n  - aws\n  \n# Optional: A severity string to identify the importance of the detection results.\nseverity: low\n \n# Optional: An array of mitre attack framework classifications. \n# This is useful when identifying attack patterns.\nmitreAttacks:\n  - initial-access\n  \n# Optional: An array of sources that this detection is used for.\n# This can be useful to help identify which sources are lacking coverage.\nsourceTypes:\n  - cloudtrail\n  \n# Required: A cron string that identifies how often this detection will execute.\nschedule: '*/15 * * * *'\n \n# Optional (default: false): If true will disable this detection from running on its schedule\ndisabled: false\n \n# Optional: Key/Value pairs that are replaced when the query executes. \nparameters:\n  userAgent: AWS Internal\n \n# Optional: An array of notification channel names that should receive an alert if this detection triggers\nnotificationNames:\n  - email\n  \n# Optional: A notification template name to override the defualt channel template if this detection triggers.\nnotificationTemplate: aws_template","detection-time-windowing#Detection Time Windowing":"One thing that many detection queries tend to do is query by looking back in\ntime and handling time windowing.When writing a detection query it's best to write your query in a way that:\nUses receivedAt times for windowing of logs on short windows, not eventTime\nUses parameters {from:DateTime} and {to:DateTime} to ensure all logs are queried.\nRunReveal allows you to specify parameters in your queries for dynamic values passed\nin at query runtime. RunReveal will always pass from and to to your queries whether\nthey are specified or not.\nfrom the end of the previous window that was queried.\nto the current time, and the next from time.\nIn practice what this looks like, when writing a simple query that looks for the eventName\nexample your query would look something like this.\nselect * from logs\nwhere eventName='example'\nand receivedAt BETWEEN {from:DateTime} and {to:DateTime}\nIt's important not to use eventTime because many log sources deliver eventTimes in a delayed\nmanner, and eventTime is the timestamp provided by the log source. If you instead window\nusing eventTime it can cause messages to be missed in situations where log sources deliver\ndelayed data.By querying receivedAt you ensure data is searched as it's received by the RunReveal platform\nregardless of data delays from sources.","creating-a-new-detection#Creating a new detection":"If you'd like to save yourself from writing this yaml, the CLI can help you create the yaml\nstructure needed to create a new detection.The runreveal detections create command will walk you through a wizard that will output\na yaml containing the structure you need to upload a detection to our api.\n:) runreveal detections create\nDetection name: my-detection\nDescription: example for the docs.\nCategories (comma separated): aws, signal\nSeverity: Low\nRisk Score (number): 10\nATT&CK classification: collection\nEnabled: true\nQuery Type: sql\nDetection Created:\n        my-detection.yaml\n        my-detection.sql\n:)"}},"/detections/detection-as-code/getting-started":{"title":"Getting Started","data":{"":"Detection as code relies on the RunReveal CLI. Downloading the CLI can be done\nthrough brew or from our github.\nbrew tap runreveal/runreveal\nbrew install runreveal\nNext you'll need to log into RunReveal in the CLI. This command will opeen\na browser for you to complete your log in through.\nrunreveal init","detection-data-model#Detection Data Model":"The detection data model is described as following. All fields are marked as\neither optional or required. This file needs to be in either yaml format or\njson format, either works.\n# Required: The name of the query for reference\nname: root_account_usage\n \n# Optional: A display name that is shown in the UI\ndisplayName: AWS Root Account Was Used\n \n# Required: The name of the file containing the sql query\nfile: root_account_usage.sql\n \n# Optional: A description of the detection.\ndescription: Monitor for usage of the AWS root account.\n \n# Optional: Additional notes that can be attached to the detection.\nnotes: |-\n  This is a notes field that can store info about this detection.\n  It can be useful to store links to playbooks in your other systems.\n  \n# Optional: An array of strings to help group queries\ncategories:\n  - aws\n  \n# Optional: A severity string to identify the importance of the detection results.\nseverity: low\n \n# Optional: An array of mitre attack framework classifications. \n# This is useful when identifying attack patterns.\nmitreAttacks:\n  - initial-access\n  \n# Optional: An array of sources that this detection is used for.\n# This can be useful to help identify which sources are lacking coverage.\nsourceTypes:\n  - cloudtrail\n  \n# Required: A cron string that identifies how often this detection will execute.\nschedule: '*/15 * * * *'\n \n# Optional (default: false): If true will disable this detection from running on its schedule\ndisabled: false\n \n# Optional: Key/Value pairs that are replaced when the query executes. \nparameters:\n  userAgent: AWS Internal\n \n# Optional: An array of notification channel names that should receive an alert if this detection triggers\nnotificationNames:\n  - email\n  \n# Optional: A notification template name to override the defualt channel template if this detection triggers.\nnotificationTemplate: aws_template","detection-time-windowing#Detection Time Windowing":"One thing that many detection queries tend to do is query by looking back in\ntime and handling time windowing.When writing a detection query it's best to write your query in a way that:\nUses receivedAt times for windowing of logs on short windows, not eventTime\nUses parameters {from:DateTime} and {to:DateTime} to ensure all logs are queried.\nRunReveal allows you to specify parameters in your queries for dynamic values passed\nin at query runtime. RunReveal will always pass from and to to your queries whether\nthey are specified or not.\nfrom the end of the previous window that was queried.\nto the current time, and the next from time.\nIn practice what this looks like, when writing a simple query that looks for the eventName\nexample your query would look something like this.\nselect * from logs\nwhere eventName='example'\nand receivedAt BETWEEN {from:DateTime} and {to:DateTime}\nIt's important not to use eventTime because many log sources deliver eventTimes in a delayed\nmanner, and eventTime is the timestamp provided by the log source. If you instead window\nusing eventTime it can cause messages to be missed in situations where log sources deliver\ndelayed data.By querying receivedAt you ensure data is searched as it's received by the RunReveal platform\nregardless of data delays from sources.","creating-a-new-detection#Creating a new detection":"If you'd like to save yourself from writing this yaml, the CLI can help you create the yaml\nstructure needed to create a new detection.The runreveal detections create command will walk you through a wizard that will output\na yaml containing the structure you need to upload a detection to our api.\n:) runreveal detections create\nDetection name: my-detection\nDescription: example for the docs.\nCategories (comma separated): aws, signal\nSeverity: Low\nRisk Score (number): 10\nATT&CK classification: collection\nEnabled: true\nQuery Type: sql\nDetection Created:\n        my-detection.yaml\n        my-detection.sql\n:)"}},"/transforms":{"title":"Log Transforms","data":{"":"Transforms allow you to convert your raw log data into RunReveal's normalized schema. By creating a pipeline of processors, you can extract, modify, and map your log fields to a standardized format.","transform-pipeline#Transform Pipeline":"A transform pipeline consists of a sequence of processors that manipulate your log data. Each processor performs a specific operation, and the output of one processor becomes the input for the next. The final result is mapped to RunReveal's normalized schema fields.","building-your-pipeline#Building Your Pipeline":"Start with your raw log data\nAdd processors to extract and transform the data\nUse the Output processor to map the result to a normalized field\nTest your transform with sample data","available-processors#Available Processors":"","text-extraction-processors#Text Extraction Processors":"","extract-json-field#Extract JSON Field":"Extracts values from JSON data using GJSON path syntax\nExample: Extract nested fields like data.user.id","extract-delimited-field#Extract Delimited Field":"Pulls data from delimited text (CSV, TSV, etc.)\nConfigurable delimiter and quote handling\nSpecify field index to extract","extract-with-regex#Extract with Regex":"Uses regular expressions to capture specific parts of text\nSupports named capture groups\nUseful for semi-structured log formats","text-manipulation-processors#Text Manipulation Processors":"","add-prefixsuffix#Add Prefix/Suffix":"Append text before or after your data\nUseful for standardizing field formats","convert-case#Convert Case":"Transform text to uppercase or lowercase\nEnsures consistent casing in your schema","regex-findreplace#Regex Find/Replace":"Find and replace text patterns\nClean up or standardize text formats","strip-characters#Strip Characters":"Remove specific characters from text\nClean up unwanted characters or whitespace","trim-whitespace#Trim Whitespace":"Remove leading/trailing spaces\nStandardize field values","extract-substring#Extract Substring":"Pull specific portions of text by position\nSupport for both positive and negative indices","split-string#Split String":"Divide text by a delimiter and select a specific part\nUseful for breaking down combined fields","datetime-processor#Date/Time Processor":"","format-datetime#Format Date/Time":"Convert between different date/time formats\nSupports common formats:\nRFC3339\nUnix timestamps (seconds/milliseconds)\nRFC822\nANSIC\nCustom formats","output-processor#Output Processor":"","output-to-field#Output to Field":"Maps transformed data to normalized schema fields\nAvailable normalized fields include:\nEvent fields (id, eventName, eventTime)\nActor fields (id, email, username)\nNetwork fields (src.ip, src.port, dst.ip, dst.port)\nService fields (name)\nResource and tag fields","building-an-effective-pipeline#Building an Effective Pipeline":"Start with Data Extraction\nUse JSON, Regex, or Delimited processors to pull out raw values\nExample: Extract timestamp from a log line using regex\nClean and Format\nApply text manipulation processors to standardize the data\nExample: Convert extracted hostname to lowercase\nTransform Dates\nConvert timestamps to the required format\nExample: Convert Unix timestamp to RFC3339\nMap to Schema\nUse Output processor to map to normalized fields\nExample: Map processed IP address to src.ip","testing-your-transform#Testing Your Transform":"","testing-processors#Testing Processors":"Click \"Test Rule\" to run the sample data through the current rule\nSee how each processor affects the data\nView the final normalized output\nAny errors will be highlighted in red","normalized-schema-preview#Normalized Schema Preview":"Click \"View Normalized Event\" to see the entire normalized event that is created when this transform runs\nEnsure your transforms map to the correct fields\nVerify the data types match the schema requirements","best-practices#Best Practices":"Build Incrementally\nAdd and test one processor at a time\nVerify each step with sample data\nTest Edge Cases\nTry different log formats\nInclude error cases in testing\nReview Final Output\nVerify all required fields are mapped\nCheck data types match schema","troubleshooting#Troubleshooting":"Common transform issues and solutions:\nExtraction Failed\nVerify regex patterns against sample data\nCheck JSON paths exist in the data\nConfirm delimiter settings match the input\nIncorrect Output\nReview processor order\nCheck field names are exact matches\nVerify date formats\nMissing Fields\nEnsure all required fields are mapped\nCheck for typos in field names\nVerify the transform handles all log variations"}},"/sources/source-types/teleport":{"title":"Teleport Cloud Audit Logs","data":{"":"These logs are emitted by Teleport Cloud to an S3 bucket and it's worth noting that the setup process\non the teleport cloud side is slightly unique compared to other providers. They will ask you to run\nseveral commands in AWS Cloud Shell to instantiate a collection of teleport buckets.","ingest-methods#Ingest Methods":"Setup the ingestion of this source using one of the following guides.\nIf using the AWS External Audit Storage method for sending Teleport logs to AWS perform the necessary setup first before finishing the RunReveal ingestion steps.\nAWS S3 Bucket\nAWS S3 Bucket with Custom SQS\nAzure Blob Storage\nGoogle Cloud Storage\nIf using an AWS S3 bucket, use the following SNS topic ARN to send your bucket notifications.\narn:aws:sns:<REGION>:253602268883:runreveal_teleport","setup#Setup":"Teleport offers first party support for sending events to AWS. Otherwise you will need to setup your Teleport account to forward events to\nyour cloud storage account most likely using their fluentd guide.","aws-external-audit-storage#AWS External Audit Storage":"At the end of the process you should be have a new bucket called something like:\nxxxxxxxx-longterm-zzzzzzzzz-zzzz-zzzz-zzzz-zzzzzzzzzzzz\nThis is the bucket you'll provide RunReveal with access to.Their docs are available online, but under Enroll New Integration, you'll need to set up a next\nAWS External Audit Storage integration.\nAs part of the setup process, you'll provide teleport with a name for your integration, the iam role\nname you'd like for teleport to create, the bucket name you'd like teleport to create, and the bucket\nprefix.\nOnce you provide these bits of information, you'll be given a Amazon CloudShell command to run. This will create\nthe role and the bucket within your AWS account. Once you provide teleport with your role's name, including the\nAWS account ID.Once you provide teleport with this information they will ask you to continue with the integration by running\nan Amazon CloudShell command one final time to provision your buckets, and finally test the connection."}},"/how-to-guides/kubernetes-logs":{"title":"Kubernetes Logs","data":{"":"To get kubernetes logs into runreveal, we've built an agent called reveald that\ncan be run as a daemonset that consumes the pod logs from the host, according to\nthe kubernetes log interface.  For more information on the kubernetes log\ninterface, see the official\nguide.","setup#Setup":"Every kubernetes cluster is different.  However, the guide linked above\nrecommends that pods log to /var/log/pods for compatibility with the widest\nnumber of logging agents.  That said, it's still possible for the runtime to\nintercept the pods' logs and send them to a different location.This guide assumes the logs are being written to /var/log/pods from the\ncontainer runtime on the host.From there, we can create a daemonset that runs the reveald agent on every node.\n# daemonset config\nThe reveald config will look something like this:\n{\n  \"sources\": {\n    // The keys here are the identifiers for the sources, and are used to\n    // refernece them when logging or in metrics.\n    \"kubernetes\": {\n      \"type\": \"file\",\n      \"path\": \"/var/log/pods/\",\n      \"extension\": \".log\",\n    },\n  },\n  \"destinations\": {\n    \"runreveal\": {\n      \"type\": \"runreveal\",\n      // Replace this webhook URL with your own, created on https://app.runreveal.com\n      // as a \"Reveald\" type source\n      \"webhookURL\": \"https://example.api.runreveal.com/sources/kawa/webhook/0123456789\",\n      // You can also use environment variables by referencing them with a\n      // dollar sign. The value must be quoted, start with a dollar sign and be\n      // a valid environment variable name\n      // \"webhookURL\": \"$WEBHOOK_URL\",\n    },\n    // \"lumbermill\": {\n    //  \"type\": \"s3\",\n    //  \"bucketName\": \"the-lumber-mill\",\n    //  \"bucketRegion\": \"us-west-2\",\n    // },\n  },\n}\nAfter you've deployed this daemonset to your cluster, you should begin to see\nlogs arriving in runreveal, assuming the network has been configured to allow\noutbound internet connections from the reveald pods and kubernetes cluster."}},"/release-notes/release-v2025-7-9":{"title":"Release Notes for runreveal v2025.7.9","data":{"-highlights#🚀 Highlights":"We're excited to introduce a new chat sharing feature in this release! 💬","chat-sharing-feature#Chat Sharing Feature":"Create and share chats with other users\nImproved rendering for shared chats\nFixed sharing issues for a smoother experience","other-changes#Other Changes":"Added missing runtime exports\nVarious formatting and code cleanup\nThank you for using runreveal! We hope you enjoy the new chat sharing capabilities."}}}